[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Categories\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nGraph 101 (작성중)\n\n\nGraph Representation Learning (2020, William L. Hamilton)\n\n\n4 min\n\n\n\ngraph\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\nLLMs Are Zero-Shot Time Series Forecasters\n\n\n2310.07820\n\n\n3 min\n\n\n\nLLM\n\n\nZero Shot\n\n\npaper review\n\n\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\nLoRA: Low-Rank Adaptation of Large Language Models\n\n\n2106.09685\n\n\n2 min\n\n\n\nLLM\n\n\npaper review\n\n\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\nMMLAB\n\n\nMMLAB installation guide (non-official)\n\n\n7 min\n\n\n\nComputer Vision\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nskeleton data plotting\n\n\nfor NTU RGB+D\n\n\n1 min\n\n\n\nHAR\n\n\ndataset\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nModalities\n\n\nfor Human Action Recognition\n\n\n20 min\n\n\n\nHAR\n\n\npaper review\n\n\n\n\nOct 10, 2023\n\n\n\n\n\n\n\n\n\n\n\nencoding\n\n\nPositional encoding\n\n\n10 min\n\n\n\nNLP\n\n\nPyTorch\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\np tuning (작성중)\n\n\nPEFT; p tuning\n\n\n1 min\n\n\n\nNLP\n\n\nLLM\n\n\ntechnique\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nLoRA\n\n\nPEFT: Low Rank Adaption\n\n\n2 min\n\n\n\nNLP\n\n\nLLM\n\n\ntechnique\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\ntips\n\n\nhuggingface 알아보기\n\n\n3 min\n\n\n\nNLP\n\n\nhuggingface\n\n\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\nPEFT\n\n\nLLM Parameter Efficient Fine Tuning with huggingface tutorial\n\n\n3 min\n\n\n\nNLP\n\n\nhuggingface\n\n\ntutorial\n\n\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\nModel freezing\n\n\npytorch model layer freezing\n\n\n1 min\n\n\n\nPyTorch\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\nHandling imbalanced Data\n\n\nundersampling, oversampling, loss function, weight sampling\n\n\n3 min\n\n\n\ntechnique\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\nZero-shot Learning\n\n\n배우는 방법을 배우기, Meta learning\n\n\n2 min\n\n\n\nMeta learning\n\n\ntheory\n\n\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\nPyTorch101\n\n\nIntroduction to PyTorch\n\n\n9 min\n\n\n\nPyTorch\n\n\ntutorial\n\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\nHow to read a paper\n\n\nS. Keshav, University of Waterloo\n\n\n9 min\n\n\n\ntutorial\n\n\n\n\nMar 6, 2024\n\n\n\n\n\n\n\n\n\n\n\nfinetuning tutorial\n\n\nTorchVision Object Detection Finetuning Tutorial\n\n\n8 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\ntransfer learning tutorial\n\n\nTransfer Learning for Computer Vision Tutorial\n\n\n1 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\n\n\n\nfgsm tutorial\n\n\nGenerating Adversarial Example\n\n\n2 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\n\n\n\nDCGAN face tutorial\n\n\nGenerating Adversarial Example\n\n\n3 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\nSwin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n\n\n2201.01266\n\n\n7 min\n\n\n\ntutorial\n\n\npaper review\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\nFreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)\n\n\n아무것도 모르는 사람들을 위해 쓴다.\n\n\n9 min\n\n\n\nFreeSurfer\n\n\ntutorial\n\n\n\n\nMar 14, 2024\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n1505.04597\n\n\n4 min\n\n\n\ntutorial\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/NLP/encoding.html",
    "href": "posts/NLP/encoding.html",
    "title": "encoding",
    "section": "",
    "text": "encoding을 왜 할까? 컴퓨터가 자연어를 이해할 수 있을까? 아니다. 따라서 컴퓨터가 이해할 수 있도록, 단어 자체의 정보는 보존한 채 자연어를 숫자로 바꾸는 방법이 encoding이다.\n그렇다면 positional encoding에서 보존하고자 하는 정보는 무엇일까? 자리다. 자리가 왜 중요한가? 자리가 의미를 반영하는 경우가 있기 때문이다. 부정어구가 대표적이다. 아래 두 문장을 보면 알 수 있다. 부정어구 안 이 위치하는 곳에 따라 음식의 재료가 바뀐다.\n위치가 중요함을 알았으니 위치 정보를 컴퓨터가 이해할 수 있는 언어로 바뀐 단어에 주입하는 방법에 대해 알아보자."
  },
  {
    "objectID": "posts/NLP/encoding.html#embedding",
    "href": "posts/NLP/encoding.html#embedding",
    "title": "encoding",
    "section": "Embedding",
    "text": "Embedding\n\nDocstring\nclass Embedding(Module):\n    r\"\"\"A simple lookup table that stores embeddings of a fixed dictionary and size.\n\n    This module is often used to store word embeddings and retrieve them using indices.\n    The input to the module is a list of indices, and the output is the corresponding\n    word embeddings.\npytorch의 모든 신경망은 nn 을 상속받음으로써 시작된다. Module은 nn.Module로 공식문서에서 별도로 import 했다. docstring에 의하면 본 모듈은 word embedding 또는 index들을 사용하여 word embedding을 검색하는데 사용된다. 이 모듈을 향한 입력 은 index들로 구성된 list 이고, 이 모듈의 결과값은 word embedding에 해당한다.\n\n\nArguments\n\ninstance를 생성할 때 입력할 params\n\n    Args:\n        num_embeddings (int): size of the dictionary of embeddings\n        embedding_dim (int): the size of each embedding vector\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                     i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                     the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                     but can be updated to another value to be used as the padding vector.\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                                 See Notes for more details regarding sparse gradients.\n\nnum_embeddings : int 값으로, embedding 될 단어쌍 (dictionary) 의 최대 값을 말한다.\nembedding_dim : int 값으로, 각 embedding 벡터의 길이를 말한다.\n\n\n\n\n\n\n\n왜 num_embeddings 에는 임베딩 될 값의 크기보다 더 큰 값을 넣어야 하는가?\n\n\n\n\n\ninstance인 embedding 을 어떻게 쓸 지 생각해보면 좋다. 우리는 어떤 값을 숫자로 표현할 것이고 input 값에 9가 들어가든 len(‘가나다라마바사’) 가 들어가든 아무런 상관이 없다. 단지 숫자로 변환 할 때 모듈이 nn.Embedding 이고 instance를 생성할 때 미리 parameter를 준비해 놔야 하는 점만 중요하게 여기면 된다.\ninput = torch.LongTensor([[1,2,4,5],[4,3,2,len('가나다라마바사')]])\nembedding = nn.Embedding(10, 3)\n\nembedding(input[1][-1])\n# tensor([ 0.2074,  0.0673, -0.1462], grad_fn=&lt;EmbeddingBackward0&gt;)\nnn.Embedding 을 통해 인스턴스를 생성할 때 (num_embeddings * embedding_eim) 모양의 파라미터가 생긴다. embedding(input) 을 통해 input 값을 임베딩 하면 미리 만들어둔 파라미터에 임베딩 된 값이 걸리게 되는 셈이다.\n\nreference\n\nhttps://discuss.pytorch.kr/t/embedding/942\n\n\n\n\n\n\n\nAttributes\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                         initialized from :math:`\\mathcal{N}(0, 1)`\n이전에 instance를 생성할 때 num_embeddings, embedding_dim 을 할당하면 그 형태의 파라미터가 생성된다고 적은 바 있다. 이 형태에 따라 랜덤하게 가중치를 할당하는 역할을 한다. 최초에는 0부터 1 사이의 값으로 할당되며 이 Tensor는 직접적으로 학습되는 값이다.\n\n\nShape\n    Shape:\n        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n    .. note::\n        Keep in mind that only a limited number of optimizers support\n        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n    .. note::\n        When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n        :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n        modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n        calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n        :attr:`max_norm` is not ``None``. For example::\n\n            n, d, m = 3, 5, 7\n            embedding = nn.Embedding(n, d, max_norm=True)\n            W = torch.randn((m, d), requires_grad=True)\n            idx = torch.tensor([1, 2])\n            a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n            b = embedding(idx) @ W.t()  # modifies weight in-place\n            out = (a.unsqueeze(0) + b.unsqueeze(1))\n            loss = out.sigmoid().prod()\n            loss.backward()\n\n\nExamples\n    Examples::\n\n        &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n        &gt;&gt;&gt; embedding = nn.Embedding(10, 3)\n        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n        &gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        &gt;&gt;&gt; embedding(input)\n        tensor([[[-0.0251, -1.6902,  0.7172],\n                 [-0.6431,  0.0748,  0.6969],\n                 [ 1.4970,  1.3448, -0.9685],\n                 [-0.3677, -2.7265, -0.1685]],\n\n                [[ 1.4970,  1.3448, -0.9685],\n                 [ 0.4362, -0.4004,  0.9400],\n                 [-0.6431,  0.0748,  0.6969],\n                 [ 0.9124, -2.3616,  1.1151]]])\n\n\n        &gt;&gt;&gt; # example with padding_idx\n        &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)\n        &gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])\n        &gt;&gt;&gt; embedding(input)\n        tensor([[[ 0.0000,  0.0000,  0.0000],\n                 [ 0.1535, -2.0309,  0.9315],\n                 [ 0.0000,  0.0000,  0.0000],\n                 [-0.1655,  0.9897,  0.0635]]])\n\n        &gt;&gt;&gt; # example of changing `pad` vector\n        &gt;&gt;&gt; padding_idx = 0\n        &gt;&gt;&gt; embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n        &gt;&gt;&gt; embedding.weight\n        Parameter containing:\n        tensor([[ 0.0000,  0.0000,  0.0000],\n                [-0.7895, -0.7089, -0.0364],\n                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n        &gt;&gt;&gt; with torch.no_grad():\n        ...     embedding.weight[padding_idx] = torch.ones(3)\n        &gt;&gt;&gt; embedding.weight\n        Parameter containing:\n        tensor([[ 1.0000,  1.0000,  1.0000],\n                [-0.7895, -0.7089, -0.0364],\n                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    \"\"\"\n    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n                     'norm_type', 'scale_grad_by_freq', 'sparse']"
  },
  {
    "objectID": "posts/NLP/ptuning.html",
    "href": "posts/NLP/ptuning.html",
    "title": "p tuning (작성중)",
    "section": "",
    "text": "p tuning\n“P-tuning is a method for automatically searching and optimizing for better prompts in a continuous space.”\n\nPLM의 일부 가중치만 미세조정하여 continuous prompt emobedding만 tuninng한다.\n\n\n\nReference\n\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "posts/NLP/huggingface/tips.html",
    "href": "posts/NLP/huggingface/tips.html",
    "title": "tips",
    "section": "",
    "text": "다운로드 받은 모델 삭제하는 방법 (cache cli에서 지우기)\n➜ huggingface-cli scan-cache\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS                LOCAL PATH\n--------------------------- --------- ------------ -------- ------------- ------------- ------------------- -------------------------------------------------------------------------\nglue                        dataset         116.3K       15 4 days ago    4 days ago    2.4.0, main, 1.17.0 /home/wauplin/.cache/huggingface/hub/datasets--glue\ngoogle/fleurs               dataset          64.9M        6 1 week ago    1 week ago    refs/pr/1, main     /home/wauplin/.cache/huggingface/hub/datasets--google--fleurs\nJean-Baptiste/camembert-ner model           441.0M        7 2 weeks ago   16 hours ago  main                /home/wauplin/.cache/huggingface/hub/models--Jean-Baptiste--camembert-ner\nbert-base-cased             model             1.9G       13 1 week ago    2 years ago                       /home/wauplin/.cache/huggingface/hub/models--bert-base-cased\nt5-base                     model            10.1K        3 3 months ago  3 months ago  main                /home/wauplin/.cache/huggingface/hub/models--t5-base\nt5-small                    model           970.7M       11 3 days ago    3 days ago    refs/pr/1, main     /home/wauplin/.cache/huggingface/hub/models--t5-small\n\nDone in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\nGot 1 warning(s) while scanning. Use -vvv to print details.\n삭제 : rm -r {LOCAL PATH}\nref : https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
  },
  {
    "objectID": "posts/HAR/ntu-skl-plotting.html",
    "href": "posts/HAR/ntu-skl-plotting.html",
    "title": "skeleton data plotting",
    "section": "",
    "text": "!pwd\n!ls\n!cd ntu_utils/parser_repo; ls; git remote -v\n\n/home/devin/wdir/datasets/utils\nmmskeleton  ntu_utils  plotting  plotting.ipynb  working.ipynb\nconf.py  __pycache__  README.md  read_skeleton.py  utils.py\norigin  https://github.com/Ugenteraan/NTU-RGB-Skeleton-Python.git (fetch)\norigin  https://github.com/Ugenteraan/NTU-RGB-Skeleton-Python.git (push)\n\n\n\n# basic tools\nimport os\nimport random\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\n\n# basic data handling tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# image processing\nimport cv2\nimport imageio\nfrom PIL import Image\nimport matplotlib.image as mpimg\n\n# Modules\nfrom ntu_utils.parser_repo.read_skeleton import read_skeleton\n# 원래 용법대로 사용하면 가져와서 쓸 수가 없어서 import 해서 쓸 수 있도록 변경\n# !cd parser_repo; ls; python3 read_skeleton.py --skel=\"/home/devin/wdir/datasets/NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/S001C001P001R001A001.skeleton\" --save\n\n\n'''\nGLOBAL VARIABLE\n'''\n\n\n@dataclass\nclass args:\n    # wdir/datasets/NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/S001C001P001R001A001.skeleton\n    file_name = \"S001C001P001R001A002\"\n    skel_raw = f\"../NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/{file_name}.skeleton\"\n    skel_processed_sub = \"../processed/st-gcn-processed-data/NTU-RGB-D/xsub/train_data.npy\"\n    skel_processed_view = \"../processed/st-gcn-processed-data/NTU-RGB-D/xview/train_data.npy\"\n    subject_ID = None\n    video = None\n    save = None\n\nSAMPLE = read_skeleton(args)  # args : class, global variable\nSAMPLE_NUMBER = int(args.file_name.split(\"A\")[-1])\n\nrandom_seed = 1\nrandom.seed(random_seed)\n\n# {color_name(str) : (color code B,G,R)}\ncolors = {\n    'blue' : (255, 0, 0,),\n    'green' : (0, 255, 0),\n    'red' : (0, 0, 255),\n    'white' : (255, 255, 255),\n    'black' : (0, 0, 0,),\n    'cyan' : (0,255,255),\n    'magenta' : (255,0,255),\n    'yellow' :  (255,255,0)\n    }\n\n# {joint_name(str) : int}\njoint_annotations = {\n    \"base of spine\" : 1,\n    \"middle of spine\" : 2,\n    \"neck\" : 3,\n    \"head\" : 4,\n    \"left shoulder\" : 5,\n    \"left elbow\" : 6,\n    \"left wrist\" : 7,\n    \"left hand\" : 8,\n    \"right shoulder\" : 9,\n    \"right elbow\" : 10,\n    \"right wrist\" : 11,\n    \"right hand\" : 12,\n    \"left hip\" : 13,\n    \"left knee\" : 14,\n    \"left ankle\" : 15,\n    \"left foot\" : 16,\n    \"right hip\" : 17,\n    \"right knee\" : 18,\n    \"right ankle\" : 19,\n    \"right foot\" : 20,\n    \"spine\" : 21,\n    \"tip of left hand\" : 22,\n    \"left thumb\" : 23,\n    \"tip of right hand\" : 24,\n    \"right thumb\": 25\n}\n\n# {int : labels(str)}\nntu_label_annotations = {\n    1   :  \"drink water\",\n    2   :  \"eat meal/snack\",\n    3   :  \"brushing teeth\",\n    4   :  \"brushing hair\",\n    5   :  \"drop\",\n    6   :  \"pickup\",\n    7   :  \"throw\",\n    8   :  \"sitting down\",\n    9   :  \"standing up (from sitting position)\",\n    10  :  \"clapping\",\n    11  :  \"reading\",\n    12  :  \"writing\",\n    13  :  \"tear up paper\",\n    14  :  \"wear jacket\",\n    15  :  \"take off jacket\",\n    16  :  \"wear a shoe\",\n    17  :  \"take off a shoe\",\n    18  :  \"wear on glasses\",\n    19  :  \"take off glasses\",\n    20  :  \"put on a hat/cap\",\n    21  :  \"take off a hat/cap\",\n    22  :  \"cheer up\",\n    23  :  \"hand waving\",\n    24  :  \"kicking something\",\n    25  :  \"reach into pocket\",\n    26  :  \"hopping (one foot jumping)\",\n    27  :  \"jump up\",\n    28  :  \"make a phone call/answer phone\",\n    29  :  \"playing with phone/tablet\",\n    30  :  \"typing on a keyboard\",\n    31  :  \"pointing to something with finger\",\n    32  :  \"taking a selfie\",\n    33  :  \"check time (from watch)\",\n    34  :  \"rub two hands together\",\n    35  :  \"nod head/bow\",\n    36  :  \"shake head\",\n    37  :  \"wipe face\",\n    38  :  \"salute\",\n    39  :  \"put the palms together\",\n    40  :  \"cross hands in front (say stop)\",\n    41  :  \"sneeze/cough\",\n    42  :  \"staggering\",\n    43  :  \"falling\",\n    44  :  \"touch head (headache)\",\n    45  :  \"touch chest (stomachache/heart pain)\",\n    46  :  \"touch back (backache)\",\n    47  :  \"touch neck (neckache)\",\n    48  :  \"nausea or vomiting condition\",\n    49  :  \"use a fan (with hand or paper)/feeling warm\",\n    50  :  \"punching/slapping other person\",\n    51  :  \"kicking other person\",\n    52  :  \"pushing other person\",\n    53  :  \"pat on back of other person\",\n    54  :  \"point finger at the other person\",\n    55  :  \"hugging other person\",\n    56  :  \"giving something to other person\",\n    57  :  \"touch other person's pocket\",\n    58  :  \"handshaking\",\n    59  :  \"walking towards each other\",\n    60  :  \"walking apart from each other\",\n    61  :  \"put on headphone\"\n}\n\n\nprint(f\"Sample Data has {SAMPLE.keys().__len__()} keys\")        # frame이 각각 다른 것 같음 \njoint_number = 1\nif SAMPLE[str(joint_number)].__len__() == SAMPLE[str(joint_number+1)].__len__():\n    print(f\"Each key has {SAMPLE['1'][0].__len__()} values : joints\")\n\nSample Data has 158 keys\nEach key has 25 values : joints\n\n\n\ndef extract_coordinates(data:dict, target_joint:str, joint_annotations:dict=joint_annotations):\n    \"\"\"\n    Args : \n        - data : dictionary type, skeleton extension raw NTU120 data\n            - keys : total SAMPLE.keys().__len__() length, MUST be string   / length is not fixed\n            - each key has 25 indexes which represents 25 joints\n            - each key has 3 data which represents coordinate dimension: X, Y, (Z)\n        - target_joint : configuration joint of ntu set (exact name)\n        - joint_annotation : dictionary, match string 'target_joint' into its number\n    \"\"\"\n    \n    # Arguments\n    X, Y, Z = 0, 1, 2\n    dontknow = 0    # 이중리스트임 [[값]]\n    start_frame, end_frame = 0, len(data.keys())\n    joint_number = joint_annotations[target_joint] - 1      # 0부터 시작하므로\n\n    # print\n    print(\"=\"*60)\n    print(f\"[{target_joint}]\")\n\n    for next_frame in range(1, end_frame, 10):\n        print(f\"frame no. {start_frame+next_frame}\\t\", end=\" \")\n        print(f\"X : {data[str(start_frame+next_frame)][dontknow][joint_number][X]:.3f}\", end=\"\\t\")\n        print(f\"Y : {data[str(start_frame+next_frame)][dontknow][joint_number][Y]:.3f}\", end=\"\\t\")\n        print(f\"Z : {data[str(start_frame+next_frame)][dontknow][joint_number][Z]:.3f}\")\n\n# test\nfor joint in ['base of spine', 'middle of spine', 'head', 'right foot', 'left foot']:\n    extract_coordinates(SAMPLE, joint)\n    break\n\n============================================================\n[base of spine]\nframe no. 1  X : 1047.710   Y : 513.642 Z : -0.191\nframe no. 11     X : 1048.290   Y : 513.821 Z : -0.183\nframe no. 21     X : 1048.281   Y : 511.923 Z : -0.216\nframe no. 31     X : 1049.081   Y : 509.494 Z : -0.219\nframe no. 41     X : 1048.423   Y : 512.998 Z : -0.216\nframe no. 51     X : 1048.136   Y : 513.229 Z : -0.224\nframe no. 61     X : 1048.910   Y : 511.708 Z : -0.197\nframe no. 71     X : 1049.379   Y : 512.329 Z : -0.202\nframe no. 81     X : 1049.375   Y : 512.324 Z : -0.237\nframe no. 91     X : 1049.288   Y : 511.088 Z : -0.226\nframe no. 101    X : 1048.591   Y : 510.243 Z : -0.277\nframe no. 111    X : 1048.618   Y : 510.258 Z : -0.298\nframe no. 121    X : 1048.800   Y : 510.808 Z : -0.230\nframe no. 131    X : 1048.711   Y : 511.964 Z : -0.236\nframe no. 141    X : 1049.196   Y : 511.940 Z : -0.268\nframe no. 151    X : 1050.422   Y : 511.508 Z : -0.296\n\n\n\ndef check_label(sample_number:int=SAMPLE_NUMBER, ntu_label_annotations=ntu_label_annotations) -&gt; str:\n    '''\n    Args:\n        - label: ntu_train_label, (0~59까지의 값:int, skeleton_file_name)\n\n    # Note: actions labelled from A1 to A60 are contained in \"NTU RGB+D\" (https://rose1.ntu.edu.sg/dataset/actionRecognition/)\n    '''\n    label_name:str = ntu_label_annotations[SAMPLE_NUMBER]\n    label_name_no_blank:str = label_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n    return label_name_no_blank\n\n# test \ncheck_label(sample_number=SAMPLE_NUMBER)   \n\n'eat_meal_snack'\n\n\n\n# PARAMS\n# target_joints = ['base of spine', 'middle of spine', 'spine', 'head','right elbow', 'left elbow', 'tip of left hand', 'tip of right hand','right foot', 'left foot']     # temp selection\ntarget_joints = ['base of spine', 'middle of spine', 'neck', 'head', \n                'left shoulder', 'left elbow', 'left wrist', 'left hand', \n                'right shoulder', 'right elbow', 'right wrist', 'right hand', \n                'left hip', 'left knee', 'left ankle', 'left foot', \n                'right hip', 'right knee', 'right ankle', 'right foot', \n                'spine',\n                'tip of left hand', 'left thumb', 'tip of right hand', 'right thumb']     # full joints (total 25)\nstart_frame, dontknow = 1, 0\nX, Y, Z = 0, 1, 2\n\njoint_numbers = [joint_annotations[target_joint] - 1 for target_joint in target_joints]\nXs = [SAMPLE[str(start_frame)][dontknow][joint_number][X] for joint_number in joint_numbers]\nYs = [SAMPLE[str(start_frame)][dontknow][joint_number][Y] for joint_number in joint_numbers]\nZs = [SAMPLE[str(start_frame)][dontknow][joint_number][Z] for joint_number in joint_numbers]\n\n\n# Canvas (CV2) initial setting\n# canvas : [ref](https://bkshin.tistory.com/entry/OpenCV-5-%EC%B0%BD-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EC%B2%98%EB%A6%AC)\ncanvas_shape = (1080, 1920, 3)    # 임의 지정 가능, video 가 1920 * 1080 이므로 그 기준으로 설정\ncation_classes = 120            # ref: 3.1.2, 82 daily actions, 12 health-related actions, 26 mutual actions\nsubjects = 106                  # ref: 3.1.3 106 distinct subject\n\nimg = np.zeros((canvas_shape), np.uint8)    # uint : 부호 없는 정수 or 0을 포함하는 양수로 uint 또는 int 뒤에 오는 숫자는 bit의 개수를 의미한다. (uint8 : 2^8개의 정수 표현 가능, 0~255)\n\n\n# '''\n# 'base of spine', 'middle of spine', 'spine', 'head',\n# 'right elbow', 'left elbow', 'tip of left hand', 'tip of right hand',\n# 'right foot', 'left foot'\n# '''\n\n# base_of_spine     = (int(Xs[0]), int(Ys[0]))  # green\n# middle_of_spine   = (int(Xs[1]), int(Ys[1]))  # blue\n# spine             = (int(Xs[2]), int(Ys[2]))  # green\n# head              = (int(Xs[3]), int(Ys[3]))  # red\n# right_elbow       = (int(Xs[4]), int(Ys[4]))  # yellow\n# left_elbow        = (int(Xs[5]), int(Ys[5]))  # yellow\n# tip_of_left_hand  = (int(Xs[6]), int(Ys[6]))  # magenta\n# tip_of_right_hand = (int(Xs[7]), int(Ys[7]))  # magenta\n# right_foot        = (int(Xs[8]), int(Ys[8]))  # cyan\n# left_foot         = (int(Xs[9]), int(Ys[9]))  # cyan\n\n\n# draw line (temp selection / for 10 joints)\ndef draw_line_10(img, Xs, Ys):\n    ## Params\n    base_of_spine     = (int(Xs[0]), int(Ys[0]))  # green\n    middle_of_spine   = (int(Xs[1]), int(Ys[1]))  # blue\n    spine             = (int(Xs[2]), int(Ys[2]))  # green\n    head              = (int(Xs[3]), int(Ys[3]))  # red\n    right_elbow       = (int(Xs[4]), int(Ys[4]))  # yellow\n    left_elbow        = (int(Xs[5]), int(Ys[5]))  # yellow\n    tip_of_left_hand  = (int(Xs[6]), int(Ys[6]))  # magenta\n    tip_of_right_hand = (int(Xs[7]), int(Ys[7]))  # magenta\n    right_foot        = (int(Xs[8]), int(Ys[8]))  # cyan\n    left_foot         = (int(Xs[9]), int(Ys[9]))  # cyan\n    \n    \n    ## line\n    custom_thick = 3\n\n    result = cv2.line(img, pt1=base_of_spine,    pt2=middle_of_spine,    color=colors['green'],      thickness=custom_thick)  \n    result = cv2.line(img, pt1=middle_of_spine,  pt2=spine,              color=colors['blue'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=head,               color=colors['red'],        thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=right_elbow,        color=colors['cyan'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=left_elbow,         color=colors['cyan'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=right_elbow,      pt2=tip_of_right_hand,  color=colors['yellow'],     thickness=custom_thick)  \n    result = cv2.line(img, pt1=left_elbow,       pt2=tip_of_left_hand,   color=colors['yellow'],     thickness=custom_thick)  \n    result = cv2.line(img, pt1=base_of_spine,    pt2=left_foot,          color=colors['magenta'],    thickness=custom_thick)  \n    result = cv2.line(img, pt1=base_of_spine,    pt2=right_foot,         color=colors['magenta'],    thickness=custom_thick)  \n\n    return result\n\n\n\n# draw line (full joints: 25)\ndef draw_line(img, Xs, Ys):\n    ## Args\n    connects:List[Tuple[int]] = [(1, 2), (2, 21), (21, 3), (3, 4),\\\n                                (21, 9), (9, 10), (10, 11), (11, 12), (12, 24), (12, 25),\\\n                                (21, 5), (5,6), (6,7), (7,8), (8,22), (8,23),\\\n                                (1, 17), (17, 18), (18, 19), (19, 20),\\\n                                (1, 13), (13, 14), (14, 15), (15, 16)]  # 논문 기준 (1 ~ 25)\n    \n    rgbcodes = [colors['green'], colors['blue'], colors['red'], colors['cyan'], colors['yellow'], colors['magenta']] * 4\n    custom_thick = 3\n    for i, (idx1, idx2) in enumerate(connects):\n        # args\n        former:tuple = (int(Xs[idx1-1]), int(Ys[idx1-1]))   # list index에 맞게 idx1 -1\n        latter:tuple = (int(Xs[idx2-1]), int(Ys[idx2-1]))   # list index에 맞게 idx2 -1\n        \n        # draw\n        result = cv2.line(img, pt1=former, pt2=latter, color=rgbcodes[i], thickness=custom_thick)\n    return result\n\n# draw_line(img, Xs, Ys)\n\n\n## Local Environment ##\n# to prevent kernel crash\n\n# cv2.imshow('plotting canvas', img)      # plotting canvas 창에 이미지 표시\n# cv2.waitKey(0)                          # 아무 키나 누르면\n# cv2.destroyAllWindows()                 # 모든 창 닫기\n\n\n# GIF으로 만들 이미지 저장\nPATH_images = f\"./plotting/images/{args.file_name}/\"\nPATH_gifs = f\"./plotting/gifs/\"\n\nos.makedirs(PATH_images, exist_ok=True)\nos.makedirs(PATH_gifs, exist_ok=True)\n\n# 프레임 생성\nfor next_frame in range(0, SAMPLE.keys().__len__()):\n    # [NOTE] Global Var\n    # start frame = 1, dontknow = 0\n    # target_joints = ['base of spine', 'middle of spine', 'spine', 'head','right elbow', 'left elbow', 'tip of left hand', 'tip of right hand','right foot', 'left foot']     # temp selection\n    \n    Xs = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][X] for joint_number in joint_numbers] \n    Ys = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][Y] for joint_number in joint_numbers]\n    Zs = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][Z] for joint_number in joint_numbers]\n    \n    # draw line\n    img = np.zeros((canvas_shape), np.uint8)    # 매번 초기화\n    result = draw_line(img, Xs, Ys)\n    rgb_result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n\n    try:\n        cv2.imwrite(f'{PATH_images}/{start_frame+next_frame}.jpg', result)\n    except: break\n\n\npath = [f\"{PATH_images}/{i}\" for i in os.listdir(PATH_images)]\npaths = [ Image.open(i) for i in path]\n\nimageio.mimsave(f'{PATH_gifs}/{args.file_name}_{check_label(sample_number=SAMPLE_NUMBER)}_{len(Xs)}.gif', paths, fps=5)\n\n\nNTU RGB+D\n\n\n\n\n\n\n\n임의의 joint 10개 추출\n25개 joint 전부 Plotting\n\n\n\n\nS001C001P001R001A001\ndrink water\n\n\n\n\n\n\nS001C001P001R001A002\neat_meal/snack\n\n\n\n\n\n\nS001C001P001R001A003\nbrushing_teeth"
  },
  {
    "objectID": "posts/papers/2106.09685.html",
    "href": "posts/papers/2106.09685.html",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "section": "",
    "text": "github\n\n\nAbstract\nMS사에서 진행한 연구. 자연어 처리에서의 중요한 패러다임 중 하나는 일반적인 도메인에서 학습된 대규모 사전학습 모델을 특정 과업이나 도메인에 적용시키는 것이다. 이 작업을 fine tuning이라고 하는데 더 큰 사전학습 모델을 사용할수록 모델의 매개변수를 유지하는 것은 더욱 어려워진다. 이는 더 많은 컴퓨팅 파워를 요구하고, 더 많은 비용을 야기하므로 현재 패러다임에서 큰 모델을 효과적으로 fine tuning 하는 방법을 찾는 것은 중요하다.\n이에 연구진은 Low Rank Adaption (LoRA)를 제안한다. LoRA는 사전학습 모델의 가중치를 freeze하고 학습 가능한 rank decomposition matrices를 각 레이어에 끼워넣는 방식이다. 이는 downstream 작업에서 학습 가능한 매개변수의 개수를 크게 줄일 수 있다.\n연구진이 예시로 든 GPT-3 175B 모델은 매개변수가 175 Billion 개라는 의미로 수많은 매개변수가 모델 안에 내재되어있음을 이름으로 알 수 있다. 해당 모델을 비롯하여 현재의 LLM 모델들은 billion 단위의 매개변수를 기본적으로 탑재하고 있고 이는 자연히 막대한 컴퓨팅 자원을 필요로 하게 된다.\n예시 모델을 LoRA와 Adam으로 fine tuning한 경우 학습 가능한 매개변수는 10,000배 줄어들고 GPU memory 크기는 3배 줄어들게 된다. LoRA는 fine tuning된 RoBERTa, DeBERTa, GPT-2, GPT-3의 능력 동급 혹은 그 이상으로 기능한다. 더 적은 개수의 학습 가능 매개변수를 가졌음에도 불구하고 더 많은 학습 처리량을 보이며, adapter와는 다르게 추론 지연(inference latency)이 일어나지 않는다는 장점이 있다.\n연구진은 또한 경험적 조사를 통해 language model adaptation의 낮은 계수(rank of matrix)가 모델의 효과를 조명함을 알아냈다.\n이에 연구진은 RoBERTa, DeBERTa, GPT-2의 checkpoints와 LoRA를 PyTorch로 구현한 통합 구현체를 배포한다. : https://github.com/microsoft/LoRA\n\n\n\n\n\n\nfine tuning에 최적화 함수가 필요한 이유\n\n\n\n\n\n\nfine tuning은 새로운 데이터로 모델을 학습하여 일반적인 과제를 수행하는 대형 모델을 특수한 도메인에 정착시키는 과정이다. 이 과정은 단순 학습인데 왜 최적화 함수가 필요한가? 이전에 사용했던 최적화 함수를 다시 사용하면 되지 않나?\nfine tuning도 결과적으로 ’학습’을 하는 과정이다. 학습 과정에서 optimizer는 최적화 함수로 loss function의 값이 가장 작은 값으로 수렴하도록 돕는 역할을 한다. 즉, loss function을 최적화하는 것이 optimizer의 목표다. optimizer 함수는 모델의 매개변수(parameter)를 매번 조절하여 손실함수의 값을 최소화하는데, 이를 통해 fine tuning에서도 tuning이 제대로 되고 있는지 확인할 수 있다. 최적화는 모델 성능에도 영향을 미치므로 단순히 이전에 사용했던 최적화 함수를 재사용하기보다는 새로운 전략을 모색하는 것이 효과적이다. 최적화 전략은 데이터셋의 크기, 사용 가능한 계산 리소스를 포함한 여러가지 요소를 고려하기 때문이다.\n\n\n\n\n\n\n\n\n\n\n\n\noptimization process"
  },
  {
    "objectID": "posts/techniques/zero_shot.html",
    "href": "posts/techniques/zero_shot.html",
    "title": "Zero-shot Learning",
    "section": "",
    "text": "딥러닝의 성능은 데이터의 질, 양과 정비례하는데 데이터 수집의 어려움, 레이블링의 까다로움, 레이블링의 시간 및 금전적 비용 부담으로 더 이상 데이터에 의존하기엔 어려운 단계에 이르렀다. 이러한 수렴단계에서 데이터의 절대량에 구애받지 않는 모델의 중요성이 대두되었고 데이터의 context를 읽어내는 이른바 meta learning이 발전하게 되었다. Zero-shot Learning은 그 중 한 방법론으로 한 번도 본 적 없는 레이블을 구분해 낼 수 있는 모델이다.\n\nZero-shot learning 이란: label이 지정된 소수의 클래스 집합 데이터와 클래스에 대한 ‘추가 정보만을’ 사용하여 한 번도 본 적 없는 많은 클래스까지 잘 예측하도록 학습한 모델.\n\n그렇다면 어떤 추가정보를 사용해 학습했기에 본 적 없는 데이터까지 추론할 수 있을까? 얼룩말을 학습한 모델에게 호랑이 이미지를 주고 어떤 동물인지 알아내라는 질문을 했다고 하자.\n\n\n \n\n\n얼룩말을 학습한 모델이 호랑이를 본다면 먼저 얼룩말에서 학습한 특성을 살필 것이다. 꼬리가 있는가, 줄무늬가 있는가, 갈기가 있는가. 이러한 정보를 살핀 모델은 호랑이를 ‘꼬리가 있고 검은 줄무늬가 있으나 갈기는 없는 주황색 가죽을 가진 동물’ 로 설명할 수 있을 것이다. 기존처럼 이미지 모델을 이용해 분류 문제를 풀면 이 동물이 얼룩말이 아니라는 결론만 얻을 수 있지만 Zero-shot learning은 이미지를 묘사한 context를 얻을 수 있음에 주목한다. 이렇게 얻어낸 호랑이를 설명한 묘사를 언어 모델에 넣는다면 어떨까? 언어모델은 위키피디아를 포함한 다량의 언어를 학습한 모델이므로, 이미지 모델에서 동물의 종류를 찾기엔 부족했던 ‘갈기가 없고 주황색 가죽을 가진’ 특징을 찾아낼 수 있을 것이다. 주황색 바탕에 검은 줄무늬를 가진 얼룩말 크기의 동물은 무엇일까? 호랑이다.\n이렇게 이미지 모델에서 이미지의 특성을 찾아내고, 그 특성을 언어모델에 물어 학습하지 않았던 label을 분류하는 모델이 Zero-shot model이다.\n그런데 이미지 모델에서 이미지의 특성은 어떻게 찾아낼까?\n\n\n\n\n\n이미지 모델은 이미지를 이미지가 아닌 행렬로 받아들인다. 이미지의 특성은 숫자로 표현되어 행렬이 되는데, 이렇게 특성을 반영하는 방식을 ’(semantic) embedding’이라고 한다. 임베딩 된 두 이미지를 비교하면 공통된 특성은 유사한 숫자로 표현되어 있을 것이다. 이렇게 학습되지 않은 특성도 임베딩 벡터 값으로 보존할 수 있다.\n\n\n\n\n\n이렇게 Zero-shot learning의 직관을 얻을 수 있었다. 추후 모델이 작용하는 구체적인 기작과 Meta Learning에 대해 알아보도록 하겠다.\n\n\n\n\n\n\nFew-shot learning task with meta-learning\n\n\n\n\n\nDMQM 연구실의 세미나 기록은 청자들의 후기에서도 사고를 확장할 수 있는데 이번에 눈에 들어온 부분은 아래와 같다.\n\nFew-shot learning task with meta-learning Meta learning은 Meta training(경험을 쌓고)과 Meta testing(관심 대상의 소수데이터로 다수데이터를 잘 예측)으로 이루어진다. Meta training을 위해 확보된 데이터 셋에서 여러 개의 과업을 나눈다. 즉, 각 과업은 예측하고자 하는 클래스, 소수데이터, 다수데이터가 서로 다르다. 이렇게 여러 과업으로 나누어 학습하는 방법을 에피소딕 학습(episodic training)으로 말한다. 이렇게 얻어낸 “경험”으로 부터 관심 대상이 되는 과업(task new)을 잘 수행해야 하는데 어떤 “경험”을 반영할 지 선택하는 게 중요하다.\n\n메타러닝은 배우는 방법을 배우는 방법이라고 간략하게 알고 있다. 조금 더 사람의 이해에 가까운 학습 방법으로 느껴져 흥미가 간다."
  },
  {
    "objectID": "posts/techniques/handling_imbalanced_data.html",
    "href": "posts/techniques/handling_imbalanced_data.html",
    "title": "Handling imbalanced Data",
    "section": "",
    "text": "under sampling : 너무 많은 양의 데이터를 잘라내는 방식\nover sampling : 적은 양의 데이터를 증강하는 방식\nweight sampling : 학습할 배치에 데이터가 들어갈 확률을 지정하는 방식\nloss function : 적은 양의 데이터 학습과정에 가중치를 주는 방식\n\n데이터의 절대량을 조정하는 방법과 (1, 2) 학습할 때 데이터의 균형을 맞추는 방법이 (3, 4) 있다.\n이들 중 weight sampling 방법과 imbalanced data task에 적합한 loss function를 알아보겠다.\n\n\n배치(batch)크기는 하이퍼파라미터(hyperparameter)의 한 종류로 한 번 기울기를 갱신할 때(step) 사용하는 데이터의 개수를 말한다. 배치는 미니배치(mini batch)라고도 불리며 \\(2^n\\) 개로 구성된다. 이 때 배치를 구성하는 방식을 샘플링(Sampling)이라고 하는데 Weight Sampling은 배치를 구성하는 데이터를 각각 다른 확률에 따라 추출하는 샘플링 방식이다. 따라서 데이터의 절대적인 개수가 작아 뽑힐 확률이 적은 데이터에게 가중치를 주어 더 자주 뽑힐 수 있게 조정하는 과정을 거칠 수 있다.\n예를 들어, 아래 표와 같은 데이터가 있을 때, c가 뽑힐 확률은 0.1, d가 뽑힐 확률은 0.4로 d가 뽑힐 확률이 네배 더 크다. 불균형 데이터(imbalanced data) 에서는 치명적으로, 한 배치에 뽑힌 데이터가 모두 한 label로 구성될 가능성이 있기 때문이다. 아래 표에서 배치가 32라고 할 때, 배치를 구성하는 label이 모두 d라면 모델은 균형있는 학습을 하지 못하게 되거나 d에 과적합 될 수 있다. 그러므로 전체 데이터가 불균형하더라도 배치 안에서는 균형있는 학습을 진행하기 위해 torch.utils.data.WeightedRandomSampler 메서드를 사용한다.\n\n\n\nlabel\ncount\n\n\n\n\na\n30\n\n\nb\n20\n\n\nc\n10\n\n\nd\n40\n\n\n\nWeightedRandomSampler 를 사용하면 코드 셀과 같은 결과를 확인할 수 있다. 첫번째 예제에서는 index 1의 가중치가 0.9로 가장 크며 복원추출(replacement=True)한 결과 역시 1이 세번으로 가장 많이 추출된 것을 확인 할 수 있다.\n\nfrom torch.utils.data import WeightedRandomSampler\nprint(\"replacement = True\\t-&gt; \", list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)))\nprint(\"replacement = False\\t-&gt; \", list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)))\n\nreplacement = True  -&gt;  [4, 1, 1, 4, 1]\nreplacement = False -&gt;  [0, 1, 2, 4, 5]\n\n\n\n\n\n다른 방법으로는 학습 과정에서 가중치를 주는 방법이 있다. 모델이 문제를 풀 때 해당 문제가 쉬운지 어려운지는 어떻게 판별할까? 분류 문제에서는 최종 확률값으로 문제의 난이도를 판별한다. 이것 같기도 하고, 저것 같기도 해서 헷갈리니 각 label이 답일 확률이 비슷비슷하게 높은 것이다. 따라서 \\(logit\\) \\(^1)\\) 값의 평균은 낮을 수밖에 없다. 답을 결정하는 최종 확률은 그 중 가장 높은 값을 고른 것이니 최종 확률이 낮을수록 어려운 문제다.\n\n\\(^1)\\) \\(logit\\) : 어떤 사건이 벌어질 확률 \\(p\\)가 \\([0,1]\\) 사이의 값일때 이를 \\([-\\infty, +\\infty]\\) 사이 실수값으로 변환하는 과정을 로짓(logit) 변환이라고 한다.\n\n\n\n문제가 어려운 문제인지 아닌지 알아야 하는 이유는 여기에 있다. focal loss는 불균형 데이터 문제를 해결할 때 대표적으로 쓰이는 손실함수로, 쉬운 문제를 틀렸을 때엔 작은 loss 값을, 어려운 문제를 틀렸을 때엔 큰 loss 값을 반환한다. 데이터가 적어 상대적으로 잘 학습하지 못한 label은 틀렸을 때 모델의 성능에 크게 영향을 미치게 되므로 학습 과정에서 가중치를 준다고 생각할 수 있다. 그렇다면 focal loss의 최대값은 어떻게 될까? focal loss는 기본적으로 연산한 loss에서 난이도만큼 값을 ‘깎는’ 원리이므로 focal loss의 최대값은 기본 손실값과 같을 것이다.\n\ntorchvision에서 제공하는 focal loss : source code\n\n이 외에도 기존 손실함수에 가중치를 줄 수 있는데 f1, cross entropy, fbeta, accuracy 등의 함수가 그러하다. 해당 함수들의 ‘average’ 인자값에 ’weighted’를 주면 가중된 손실이 누적된다. 이렇게 가중된 손실함수를 여러개 사용하면 모델의 성능이 개선될 수 있다."
  },
  {
    "objectID": "posts/techniques/handling_imbalanced_data.html#summary",
    "href": "posts/techniques/handling_imbalanced_data.html#summary",
    "title": "Handling imbalanced Data",
    "section": "",
    "text": "under sampling : 너무 많은 양의 데이터를 잘라내는 방식\nover sampling : 적은 양의 데이터를 증강하는 방식\nweight sampling : 학습할 배치에 데이터가 들어갈 확률을 지정하는 방식\nloss function : 적은 양의 데이터 학습과정에 가중치를 주는 방식\n\n데이터의 절대량을 조정하는 방법과 (1, 2) 학습할 때 데이터의 균형을 맞추는 방법이 (3, 4) 있다.\n이들 중 weight sampling 방법과 imbalanced data task에 적합한 loss function를 알아보겠다.\n\n\n배치(batch)크기는 하이퍼파라미터(hyperparameter)의 한 종류로 한 번 기울기를 갱신할 때(step) 사용하는 데이터의 개수를 말한다. 배치는 미니배치(mini batch)라고도 불리며 \\(2^n\\) 개로 구성된다. 이 때 배치를 구성하는 방식을 샘플링(Sampling)이라고 하는데 Weight Sampling은 배치를 구성하는 데이터를 각각 다른 확률에 따라 추출하는 샘플링 방식이다. 따라서 데이터의 절대적인 개수가 작아 뽑힐 확률이 적은 데이터에게 가중치를 주어 더 자주 뽑힐 수 있게 조정하는 과정을 거칠 수 있다.\n예를 들어, 아래 표와 같은 데이터가 있을 때, c가 뽑힐 확률은 0.1, d가 뽑힐 확률은 0.4로 d가 뽑힐 확률이 네배 더 크다. 불균형 데이터(imbalanced data) 에서는 치명적으로, 한 배치에 뽑힌 데이터가 모두 한 label로 구성될 가능성이 있기 때문이다. 아래 표에서 배치가 32라고 할 때, 배치를 구성하는 label이 모두 d라면 모델은 균형있는 학습을 하지 못하게 되거나 d에 과적합 될 수 있다. 그러므로 전체 데이터가 불균형하더라도 배치 안에서는 균형있는 학습을 진행하기 위해 torch.utils.data.WeightedRandomSampler 메서드를 사용한다.\n\n\n\nlabel\ncount\n\n\n\n\na\n30\n\n\nb\n20\n\n\nc\n10\n\n\nd\n40\n\n\n\nWeightedRandomSampler 를 사용하면 코드 셀과 같은 결과를 확인할 수 있다. 첫번째 예제에서는 index 1의 가중치가 0.9로 가장 크며 복원추출(replacement=True)한 결과 역시 1이 세번으로 가장 많이 추출된 것을 확인 할 수 있다.\n\nfrom torch.utils.data import WeightedRandomSampler\nprint(\"replacement = True\\t-&gt; \", list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)))\nprint(\"replacement = False\\t-&gt; \", list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)))\n\nreplacement = True  -&gt;  [4, 1, 1, 4, 1]\nreplacement = False -&gt;  [0, 1, 2, 4, 5]\n\n\n\n\n\n다른 방법으로는 학습 과정에서 가중치를 주는 방법이 있다. 모델이 문제를 풀 때 해당 문제가 쉬운지 어려운지는 어떻게 판별할까? 분류 문제에서는 최종 확률값으로 문제의 난이도를 판별한다. 이것 같기도 하고, 저것 같기도 해서 헷갈리니 각 label이 답일 확률이 비슷비슷하게 높은 것이다. 따라서 \\(logit\\) \\(^1)\\) 값의 평균은 낮을 수밖에 없다. 답을 결정하는 최종 확률은 그 중 가장 높은 값을 고른 것이니 최종 확률이 낮을수록 어려운 문제다.\n\n\\(^1)\\) \\(logit\\) : 어떤 사건이 벌어질 확률 \\(p\\)가 \\([0,1]\\) 사이의 값일때 이를 \\([-\\infty, +\\infty]\\) 사이 실수값으로 변환하는 과정을 로짓(logit) 변환이라고 한다.\n\n\n\n문제가 어려운 문제인지 아닌지 알아야 하는 이유는 여기에 있다. focal loss는 불균형 데이터 문제를 해결할 때 대표적으로 쓰이는 손실함수로, 쉬운 문제를 틀렸을 때엔 작은 loss 값을, 어려운 문제를 틀렸을 때엔 큰 loss 값을 반환한다. 데이터가 적어 상대적으로 잘 학습하지 못한 label은 틀렸을 때 모델의 성능에 크게 영향을 미치게 되므로 학습 과정에서 가중치를 준다고 생각할 수 있다. 그렇다면 focal loss의 최대값은 어떻게 될까? focal loss는 기본적으로 연산한 loss에서 난이도만큼 값을 ‘깎는’ 원리이므로 focal loss의 최대값은 기본 손실값과 같을 것이다.\n\ntorchvision에서 제공하는 focal loss : source code\n\n이 외에도 기존 손실함수에 가중치를 줄 수 있는데 f1, cross entropy, fbeta, accuracy 등의 함수가 그러하다. 해당 함수들의 ‘average’ 인자값에 ’weighted’를 주면 가중된 손실이 누적된다. 이렇게 가중된 손실함수를 여러개 사용하면 모델의 성능이 개선될 수 있다."
  },
  {
    "objectID": "posts/LabHAI/papers/2201.01266.html",
    "href": "posts/LabHAI/papers/2201.01266.html",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "section": "",
    "text": "목표\n1. 이해할 수 있는데까지 이해하기\n2. 코드를 이용한 재구현"
  },
  {
    "objectID": "posts/LabHAI/papers/2201.01266.html#abstract",
    "href": "posts/LabHAI/papers/2201.01266.html#abstract",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "section": "Abstract",
    "text": "Abstract\n\n뇌종양의 semantic segmentation1은 근본적으로 multiple MRI imaging을 이용한 의료이미지 분석 과제다.\n3D 의료 영상 세그멘테이션에서는 FCNNs 2 방식이 사실상의 표준 3으로 자리잡았다.\n\nU-net4: U shape Convolution Network of Biomedical domain for Image segmentation\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation 에서 발표되었으며 네트워크의 크기가 U모양이다.5\nU-net은 다음 과제에서 SOTA를 달성했다.\n\n2D semantic segmentation\n3D semantic segmentation\n\n\nacross various imaging modalities\n\nU-net 의 한계는 이러하다: long range dependency\n\nconvolution layers의 제한된 kernel 사이즈6로 인한 장거리 예측에 어려움이 있다.7\n\\(\\therefore\\) 다양한 크기의 종양을 분할하는데 어려움이 있다.8\n\n반면 transformer 계열 모델에서는 long range 문제가 희박하게 일어나고, 이것을 medical domain에 적용한 것이 Swin UNEt TRansformers다.\n\nUNETR 에서는 FCNNs이 아닌 tranformer를 적용했다.\n\nUNETR: 3D brain tumor semantic segmentation transformer model\n\n\nUNETR의 특징 또는 기여점\n\n3D brain tumor semantic segmentation task를 seq2seq 과제로 재구성했다\nmulti model 입력 데이터를 1D sequence에 embedding 했다.\n해당 데이터를 계층적 UNETR의 인코더에 입력으로 사용했다.\nencoder: 5개의 다른 해상도의 특징 추출9 &gt;&gt; decoder: FCNN기반, 해상도 특징이 각각 연결된다. (via skip connection)\n\n\n\n2 Fully Convolutional Neural Networks3 de facto standard4 U-shaped net5 the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture6 이후 맥락에 따라 kernel크기에 따라 이미지 크기가 작아지는 문제로 보인다.7 long-range information is sub-optimal8 멀리 떨어질수록 관계가 줄어드는 지역적 특성을 살린 것이 CNN이다. 이게 다양한 크기의 종양 판별과 무슨 관계가 있는지 확인 필요.9 각각 다른 해상도가 시사하는 바는?\n\n\n\n\n\nRPN에서의 FCN 언급\n\n\n\n\n\nRegion Proposal Network에서 언급하는 ’CNN으로 공간 정보를 보존한다’는 말이 이 말이다. 같은 segmentation task이니 모두 공간 정보를 보존하는게 중요할거라고 생각했는데 이 아이디어를 object detection에서 모두 쓰고 있었다. 초기 FCN의 segmentation이 얼마나 거칠었는지 생각해보면 큰 발전을 이뤘다고 할 수 있다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFCN (2015)\nmask r-cnn (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfaster r-cnn (2016)\nmask r-cnn (2018)\n\n\n\n\n\n\n\n\n\n\n\n\n간단한 FCN 정리\n\n\n\n\n\n\n발상: 이미지의 정보가 압축된 Feature map이 있다면 역으로 사용/복원할 수 있지 않을까?\n\n\n\n\n\n\n\n\n\n\n\n\nFully Convolution Network\n\n\n\nCNN을 생각해보자. CNN은 이미지의 픽셀들에 Convolution 연산을 해서 특징들을 찾아낸다. 그걸 Feature map 이라고 했다. 그렇다면 어떤 모델이든 마지막 Convolution 레이어의 결과물, 다시 말해 마지막 Feature map 은 이미지의 특징을 가장 압축적으로 가지고 있는 행렬 아니겠나? 이걸 천천히 되돌려 생각하면 레이어마다 산출되는 Feature map 은 이미지의 정보를 \\(n\\) 배 압축한 행렬이 된다. 우리는 이렇게 이미지의 정보를 압축하는 과정을 down sampling 이라고 하기로 했다.\n우리에겐 학습 과정이 있으니 down sampling 을 역으로 되짚어 갈 수도 있다. 이를 deconvolution 또는 up sampling의 한 종류라고 한다. 물론 다른 방법으로도 이미지를 복원할 수 있다. 그 방법을 논문에선 bilinear interpolation(이중선형보간)이라고 하는데, 선형 보간을 2차원으로 확장했다고 생각하면 될 것 같다. 픽셀간의 거리비를 이용해 빈 공간을 채우는 방식이라고 한다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinear interpolation\nbilinear interpolation\n\n\n\n그러면 convolution은 어떻게 역으로 되짚어 가야할까? feature map의 크기를 키우면 된다. 우리는 feature map의 특정 픽셀에 해당하는 이미지의 픽셀들을 알고 있다. 그 이미지의 픽셀들을 convolution 한 결과가 feature map 이니 당연하다. 그리고 특징을 뽑아내는 과정에서 버렸던 정보들은 feature map에 kernel을 곱하면 된다. 나누기의 반대는 곱하기라는 개념과 크게 다르지 않다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nconvolution\ndeconvolution\n\n\n\nFCN에서는 이 두가지 개념에 skip architecture라는 고유한 구조를 적용해 모든 레이어를 컨볼루션화 한다. 참고로 논문에서 사용한 커널의 크기는 \\(14 \\times 14\\)다.\n\n가장 먼저 마지막 계층에 21채널의 \\(1 \\times 1\\) convolution 레이어를 붙이는데 이는 PASCAL 이라는 객체 검출 데이터셋과 형태를 맞추기 위함이었다.\n\n32x10 upsample로 시작한다. \\(1 \\times 1\\) 형태의 pool5 layer를 32배 하는데 이때 upsample이 deconvolution 과정을 말한다. 이렇게 upsample 한 예측결과는 그대로 둔다.\n16x upsample을 만든다.\n\n마지막 layer(pool5)의 마지막 conv layer를 2배 upsample 하고 이를 커널(\\(14 \\times 14\\))과 곱한다\n직전 layer(pool4)을 1에 더하여 semantic 정보를 구체화 한다.\n\n3에서 얻은 행렬을 16배 upsample 하여 pool4만 upsample 하는 것보다 세밀한 결과를 얻는다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFCN 16s\nFCN 8s\nDAG nets learn to combine coarse\n\n\n\n비슷한 과정을 8배에서도 진행해서 더 선명한 이미지를 얻고, 그렇게 복원된 이미지의 각 픽셀을 분류 하는 것이 segmentation의 전체 과정이다. 이때 해당 논문만의 특징인 skip architecture는 간단하게 각 feature map이 어떤 정보를 담고 있었을지 추론하는 과정을 생각해보면 된다. Convolution을 진행할수록 구체적인 정보보다는 대략적인 정보를 담게 되는데 이것에 착안해 후반의 layer에서는 대략적인 특성을, 초반의 layer에서는 구체적인 특성에 집중하도록 했다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFCN for segmentation\nFCN layers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskip architecture\n\n\n\n\n출처\n\nFCN: https://medium.com/@msmapark2\nbilinear interpolation: https://dambaekday.tistory.com/3\ndeconvolution: https://realblack0.github.io/2020/05/11/transpose-convolution.html\n\n\n\n\n\n10 stride 값이 32다. 16, 8 도 마찬가지다."
  },
  {
    "objectID": "posts/LabHAI/papers/2201.01266.html#introduction",
    "href": "posts/LabHAI/papers/2201.01266.html#introduction",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "section": "Introduction",
    "text": "Introduction\n\n알아 둘 내용만 기록\n\n\n인간 뇌에 영향을 미치는 brain tumors 는 120종류가 넘는다.\n뇌종양은 두가지로 분류되는데11 원발성 뇌종양은 뇌에서 직접 발현하며12 이차성/전이성 뇌종양은 다른 장기로부터 전이된다.13\n\nprimary tumor: 대부분 신경아교세포14에서 비롯된 신경교종15\n\n종양의 병리학적 단계에 따른 하위 분류: low grade gliomas(LGG)16, High grade gliomas(HGG)17\n\nHGG: 악성18으로 빠르게 자라며 수술 또는 방사선 치료가 필요한 종양, 예후가 좋지 않다.\n참고: 분당서울대병원\n\nMagnetic Resonance Imaging (MRI)는 진단분석 도구로 쓰이며 모니터링, 분석, 수술계획 수립 등에서 사용된다.\n딥러닝 기반 이미지 분할(image segmentation) 기법은 (1) 정확하고 (2) 재현 가능한 방법을 제공하는데 두각을 드러낸다.\n\n재현 가능한 방법이 중요한 이유는 수술 때문인가?\n\nCNN의 limited kernel size로 인한 long range dependencies 문제는 정확한 종양 segmentation에서 치명적이다; 종양이 다양한 크기나 모양으로 나타나기 때문\n본 논문에서 주목한 ViT의 장점\n\npairwise interaction between token embedding and its global contextual representation\n\nself-attention을 기반으로 한 transformer이므로 당연함\n\neffective learning of pretext task for self-supervised pre-training\n\npretext task?\n\nSSL을 할때 연구자가 미리 설정해두는 과제로 모델은 해당 과제(task)를 목표로 라벨이 없는 데이터를 학습한다.\n한 파트 안에서 다른 파트를 예측하는 학습인 self prediction과 배치 데이터 샘플 간의 유사성을 찾는 contrastive learning19으로 나뉜다. (출처)\n자기지도학습에서 데이터의 표현을 학습하기 위해 구성한 문제를 pretext task라고 지칭하고, 실제로 풀고 싶은 문제를 downstream task라고 한다. ‘의료 데이터의 자기지도학습 적용을 위한 pretext task 분석’(공희산 외, 2021)은 rotation task와 jigsaw task를 수행했을때 이미지 전체를 보고 학습하는 rotation task의 효과가 더 좋았다고 발표했다.\n\nself-supervised(자기지도학습)이 논문의 과제에서 주요한 영향을 미치는 이유?\n\n\n의료 영상 분야에서 UNETR은 ViT를 encoder에 적용한 최초의 모델이다.\nTransformer와 Swin Transformer의 차이점?\n\n우선 ‘Swin transformers are suitable for various down- stream tasks wherein the extracted multi-scale features can be leveraged for further processing.’ 때문에 사용했다고 한다.\n\n종합적으로 Swin UNETR은 다음 세가지의 조합이 되겠다.\n\nSwin Transformer as an encoder\nutilized U-shape network\nCNN based decoder\n\n\n11 Brain tumors are categorized into primary and secondary tumor types12 primary tumor type originate from brain cells13 secondary tumors metastasize into the brain from other organs14 glial cell15 gliomas16 WHO grade II17 WHO grade III-IV18 malignant19 siamese network를 기반으로 발전함"
  },
  {
    "objectID": "posts/LabHAI/papers/2201.01266.html#related-work",
    "href": "posts/LabHAI/papers/2201.01266.html#related-work",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "section": "Related work",
    "text": "Related work\n\n필요할때 돌아와서 읽고 찾아보는 형식으로 진행"
  },
  {
    "objectID": "posts/LabHAI/papers/2201.01266.html#swin-unetr",
    "href": "posts/LabHAI/papers/2201.01266.html#swin-unetr",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "section": "Swin UNETR",
    "text": "Swin UNETR\n\nEncoder"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html",
    "href": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html",
    "title": "fgsm tutorial",
    "section": "",
    "text": "FGSM1 으로 MNIST를 속여보자.\n여기서 FGSM은 오분류를 목표로 하는 화이트박스 공격이다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html#footnotes",
    "href": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html#footnotes",
    "title": "fgsm tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFast Gradient Sign Attack↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html",
    "href": "posts/LabHAI/tutorial/pytorch101.html",
    "title": "PyTorch101",
    "section": "",
    "text": "이전에 왜 텐서냐, 부터 시작해야 한다.\n왜 텐서일까?\n\n텐서란 무엇일까?\n\n텐서는 배열이나 행렬과 같은 특수 자료 구조. GPU에서 사용할 수 있도록 NumPy의 ndarray를 개량했다. 개념적으로는 배열(array)와 다를게 없다.\n정말로 다를게 없나?\n\n그렇다! 텐서와 넘파이배열은 종종 내부 메모리를 공유하며 서로 형태를 전환할 수 있기까지 하다.\n\n\n\n그러므로 다른 자료형과 구분되는 텐서의 특징은 GPU 사용이 가능하다는 점이다. 즉, 병렬연산에 최적화 되어있다는 것과 같다.\n그 빠른 병렬연산으로 하는 일이 automatic differentiation 즉, 자동 미분이다.\n\n\nTensor is optimized at automatic differentiation"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#왜-파이토치냐",
    "href": "posts/LabHAI/tutorial/pytorch101.html#왜-파이토치냐",
    "title": "PyTorch101",
    "section": "",
    "text": "이전에 왜 텐서냐, 부터 시작해야 한다.\n왜 텐서일까?\n\n텐서란 무엇일까?\n\n텐서는 배열이나 행렬과 같은 특수 자료 구조. GPU에서 사용할 수 있도록 NumPy의 ndarray를 개량했다. 개념적으로는 배열(array)와 다를게 없다.\n정말로 다를게 없나?\n\n그렇다! 텐서와 넘파이배열은 종종 내부 메모리를 공유하며 서로 형태를 전환할 수 있기까지 하다.\n\n\n\n그러므로 다른 자료형과 구분되는 텐서의 특징은 GPU 사용이 가능하다는 점이다. 즉, 병렬연산에 최적화 되어있다는 것과 같다.\n그 빠른 병렬연산으로 하는 일이 automatic differentiation 즉, 자동 미분이다.\n\n\nTensor is optimized at automatic differentiation"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#주요-내용",
    "href": "posts/LabHAI/tutorial/pytorch101.html#주요-내용",
    "title": "PyTorch101",
    "section": "2. 주요 내용",
    "text": "2. 주요 내용\n\n데이터셋 구축 코드와 학습 코드를 분리하는 것이 가독성 및 유지보수면에서 좋다.\n\n\n2.1. 데이터는 어떻게?\nPyTorch의 데이터셋 관리 방식은 독특하지만 편리하다. 데이터 작업을 위한 기본 요소 두가지가 존재하는데 이는 각각 DataLoader 와 Dataset이다. 1 데이터를 받아오는건 샘플(feature), 정답(label)으로 구성된 Dataset이고 각각의 값을 iterable한 객체로 감싸 접근하게 쉽게 만든 객체가 DataLoader다.\n1 모두 torch.utils.data 하위의 모듈이다.\nDataset: sample, label; 정답이 매칭된 데이터\n\nDataset을 직접 생성하는 경우도 있는데, 이후에 DataLoader로 사용하기 위해 다음 세가지 magic method를 구현해야 한다: __init__, __len__, __getitem__\n\nDataLoader iterable data; data를 minibatch에 전달하는 역할을 하며, 에폭마다 섞는 shuffle을 수행한다. PyTorch의 장점인 multiprocessing으로 속도 향상을 꾀할 수 있다.\n\n\n\n\n\n\n\n__init__\n\n\n\n\n\n\nDataset 객체가 구축될 때 한 번 실행되는 초기화 함수.\n예를 들어, 이미지 파일과 주석 파일이 포함된 디렉토리와 변형방법 (transform, target_transform) 을 초기화한다.\n\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None) -&gt; None:\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform\n\n\n\n\n\n\n\n\n\n__getitem__\n\n\n\n\n\n\nlen 으로 총 개수를 알았으니 인덱스로 값을 불러올 수 있다.\n계속해서 이미지를 예로 들었을 때, pd.readcsv 로 self.img_labels를 불러왔으니 DataFrame형태다.\n\n\n디스크에서 이미지 위치를 식별한다.\ntorchvision2 의 read_image method 를 이용해 이미지를 텐서로 변환 한다.\n__init__에서 정의한 self.img_labels 로 텐서로 변환된 이미지의 라벨 값을 호출한다.\n\n(선택) 필요시 transform 절차를 거친다. 3\n\n텐서 이미지와 라벨을 최종 형태인 dictionary 로 변환한다.\n\ndef __getitem__(self, idx) -&gt; dict:\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = read_image(img_path)\n    label = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        label = self.target_transform(label)\n    sample = {\"image\": image, \"label\": label}\n    return sample\n\n\n\n3 왜 하필 이 단계에서 진행하는지는 target_transform을 확인할 것2 torchvision.io\n2.1.1. 활용은 이렇게 한다.\n\nfrom torch.utils.data import DataLoader로 데이터로더를 받아오고, 인자로 데이터셋과 배치사이즈를 전달한다.\nDataset마다 loader가 있어야 하니 train, test 모두 DataLoader로 받아와야한다.\n배치 자동화, 샘플링, 섞기 등 다양한 기능을 내부에서 제공한다.\n배치사이즈에 맞는 개수의 feature와, label을 묶은 객체의 요소 (batch) 를 반환한다.\n\n\niterable한 객체이므로 for문으로 간단하게 테스트 할 수 있다.\n\nfrom torch.utils.data import Dataset  # 이전 단계에서 정의한 학습용, 테스트용 데이터셋\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data:Dataset, batch_size:int=64, shuffle:bool=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n\n\n2.2. 모델은 어떻게?\nPyTorch의 모델들은 nn.Module 을 상속받는 클래스를 생성해서 정의한다. 모델을 구성하는 기본 요소가 이미 세팅되어있어 __init__에서 세팅만 하면 되니 편리하다.\n\n__init__ 함수에서 계층들을 정의하고\nforward 메서드에서 데이터를 전달하는 방식을 정한다.\n어떤 하드웨어(cpu, gpu, mps)를 사용할지 결정하는 것도 이 단계다.\n\n\n2.2.1. 예시 모델 확인\n\n\ntutorials.pytorch.py\n\n# 학습에 사용할 CPU나 GPU, MPS 장치를 얻습니다.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\n# 모델을 정의합니다.\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\n\n\n\n\n\n\nlogits?\n\n\n\n\n\n선형 딥러닝 모델의 최종 값이다.\n\nlog + odds\n정규화 되지 않은 로그 확률로, 모든 실수가 될 수 있다.\nlog-odds function은 \\(0~1\\) 사이의 값을 계산하는 시그모이드의 역함수다.\n\n@haje01\n\n\nclassification을 진행할 때 중간 레이어에 무엇을 넣든 마지막에 FC layer로 마무리하는데 이때 노드를 분류하는 클래스의 수만큼 만든다. 즉, 이 FC layer를 통과한 결과가 특정 클래스에 해당할 가능성을 의미한다고 볼 수 있다. FC layer에 들어가기 전단계인 활성화 함수로는 Sigmoid, Softmax, ReLU 등을 사용한다. 활성화 함수에 넣기 전의 로그 확률을 logits라고 말하며 PyTorch에서는 실질적으로 이 값을 다룬다.\n\n역순으로 생각하면 쉽다.\n1. 분류 문제에서 각 클래스에 해당할 확률을 알아낸다.\n2. 그 확률은 활성화 함수라는 값을 통해 나온 값이다.4\n3. 위의 활성화 함수에 넣는 값이 logit 이다\n\n왜 최종 확률이 아니라 logits을 남겨두는가?\n활성화 함수를 통한 값, 확률을 보면 직관적인 이해도가 높아지지만 잠재적으로 값이 누수되거나 연산 과정에서 값이 누락될 위험이 잔존한다. 활성화 함수를 통해 확률을 구하는 과정은 어렵지 않으므로 그 원형인 logits을 보존하는게 나은 선택이다. 그뿐 아니라 logit은 entropy 연산에서도 사용되므로 남겨두는 편이 활용도가 좋다.\n\n@KFrank\n\n\n즉, logit을 남겨두는 이유를 아래로 정리할 수 있다.\n1. 정보의 손실을 막기 위해\n2. cross entropy loss 등의 loss 계열에 사용하기 위해\n\n\n\n\n4 활성화 함수는 확률을 연산하는 함수다.\n\n\n\n\n\nbackward는?\n\n\n\n\n\n순전파와 역전파는 역할이 분리된 함수 각각에 포함되어 있다.\n\n순전파는 모델을 구성하는 방식이고\n역전파는 학습에서의 파라미터 ‘최적화’ 과정이다.\n\n.backward()로 구현되어 있다. 다음 단계인 train 함수에서 사용한다.\n\n\n\n\n\n\n\n\n\n\n\n굳이 model().to(device) 를 해줘야 하는 이유?\n\n\n\n\n\n\n기본적으로 텐서의 생성 위치는 CPU인데 tensor.to(‘cuda’) 를 통해 GPU로 텐서를 이동 할 수 있다.\n모든 텐서의 위치가 동일해야 연산을 할 수 있으니 코드 설계에 유의하자.\n\n\n\n\n\n\n\n2.3. 학습은 어떻게?\n텐서플로우에서 처럼 fit으로 끝나는게 아니라 train함수를 따로 정의해야했다. DataLoader는 학습에 필요한 데이터니 이때 모델이 학습할 수 있게 데이터를 넘겨주고 위에서 선언한 모델과 손실함수, 최적화 함수를 함께 전달한다. 실질적인 학습이 진행되는 곳이라 위에서 정의한 값들을 다 여기에 전달해주는게 맞다.\n\n최적화 단계: 하이퍼파라미터를 정의하고 학습하며 파라미터를 조정한다. (train_loop)\n\nepoch, batch size5, learning rate\n각 epoch마다 어떤 단계를 거칠 것인가, 모델 설계는 이쪽에 들어간다\n\n\n5 size에 맞추어 batch를 넘겨줄 수 있는것도 DataLoader가 배치를 만들어주는 역할을 하기 때문이다.\n\n\n\n\n\n배치 정규화\n\n\n\n\n\n\n학습 데이터의 분포를 정하는 방식, 학습 단계에서만 사용한다.\n\n학습을 하면서 분포를 정하는 방식인 배치 정규화는 과적합 외에도 기울기 소실 및 폭주를 완화하여 학습을 안정적으로 할 수 있게 돕는데 가중치 \\(w\\) 가 커질 경우 다음 층에서 학습해야 하는 범위가 커진다. 따라서 학습 중 레이어 단위의 가중치 조절이 필요한데 이 역할이 배치 정규화다.\n\n\n\n\n검증 단계 (test_loop)\n\n손실함수: 모델 출력인 logit이 여기서 사용된다.\neval() 로 평가모드 전환 잊지 말 것\n\n\n\n\n\n\n\n\neval이 무엇인가\n\n\n\n\n\n\nevaluation의 약자로 모델을 평가하는 과정이다.\n모델을 평가모드로만 전환하는 단계다.\n\ndropout 비활성화\n배치정규화(의 이동평균, 이동분산) 업데이트 정지\n\n\n일관성 있는 결과를 얻을 수 있다. (모델 자체의 성능에 집중할 수 있다.)\n\n\n\n\n\n\n\ntutorials.pytorch.py\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # 예측(prediction)과 손실(loss) 계산\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # 역전파\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n\n\n2.4. 모델 관리는 어떻게?\n\n저장하고 불러오기\n모델의 형태를 포함하여 저장하고 불러오는 것을 목표로 한다.\n\n모델 전체를 불러오는 torch.save()\n\n\ntutorials.pytorch.py\n\n# 경로 지정\nPATH = \"entire_model.pt\"\n\n# 저장하기\ntorch.save(net, PATH)\n\n# 불러오기\nmodel = torch.load(PATH)\nmodel.eval()\n\n[권장] 매개변수만 저장하는 torch.save_dict()\n\n문법이 직관적이다.\n하지만 모델 저장시 사용한 클래스 밑 디렉토리 구조에 종속된다. (매개변수이니 어쩔 수 없다.)\n\n\n\ntutorials.pytorch.py\n\n# 경로 지정\nPATH = \"state_dict_model.pt\"\n\n# 저장하기\ntorch.save(net.state_dict(), PATH)\n\n# 불러오기\nmodel = Net()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\n\n\n\n불러올 때에도 .eval() 과정을 거친단걸 잊지 말자!"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#전체-흐름",
    "href": "posts/LabHAI/tutorial/pytorch101.html#전체-흐름",
    "title": "PyTorch101",
    "section": "3. 전체 흐름",
    "text": "3. 전체 흐름\n\n데이터셋 구축\n\nDataset? DataLoader?\n값들이 모두 가속장치로 옮겨가 있는가?\n\n모델 구축\n\n__init__ 에 모델을 선언하고\n__forward__ 에 학습 과정을 정의했는가?\n\n하이퍼파라미터 정의 (학습)\n\n학습은 얼마나?\n배치 사이즈는 얼마나?\n학습률은 얼마나?\n\n최적화 단계는 어떻게? (학습)\n\n손실 함수는?\n최적화 함수는?\n\n모델 정의 방법?\n\n리팩토링 등을 예정하지 않고 있다면 torch.save_dict()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "pkg 파일 버전에 맞게 설치\n설치 과정에서 system setting 의 보안 예외 설정\n환경변수 설정\n$ export FREESURFER_HOME=/Applications/freesurfer/7.1.1\n$ export SUBJECTS_DIR=$FREESURFER_HOME/subjects\n단축어 설정\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\n터미널 설정\n# .bash_profile 또는 .zshrc 도 무관\nvi ~/.bashrc\n\n# vim 사용방법 확인하여 아래 내용 추가\n# freesurfer\nexport FREESURFER_HOME=\"/Applications/freesurfer/7.4.1\"\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\n테스트\n# 터미널 재실행 또는 `source ~/.{터미널 설정 파일}`\n$ source ~/.bashrc\n\n# 실행 확인\n$ freesurfer        # freesurfer 안내사항 출력\n$ freeview          # freeview 실행\n\n터미널 종료 후 재실행 해도 명령어가 작동되면 된다.\n이후 테스트는 튜토리얼에서 확인. 같은 방식으로 export TUTORIAL_DATA='실행경로' 추가해주면 된다.\n\n\n\n\n\nFreeSurfer Download and Install\n\nFreeSurfer Release 7 System Requirements\n\nSummary of Requirements:\nSee list of supported operating systems for each release below.\n    Intel processor supporting AVX instructions\n    RAM: 8GB for recon, 16GB suggested for viewing graphics\n    Graphics card: 3D graphics card with its own graphics memory & accelerated OpenGL drivers\n    Size of installed image: 16GB\n    Typical size of a processed subject: 300MB\n    Other requirements: Matlab (only needed to run FS-FAST, the fMRI analysis stream)\n아래 환경에서 설정했다. Matlab 이외의 프로그램은 필요하지 않다.1 - MATLAB\n1 튜토리얼 진행 중 문제 발견시 추가 예정 (24.03.14)\n\n\nlocal environment\n\n\n\n\n\n\n공식 가이드 (step by step installation demo) 를 따라하면 웬만한 과정은 다 해결 된다.\n\n최신 버전의 패키징 파일을 다운로드 한다.\n\n\n\nMacOS Install & Setup 에 자세한 스크린샷이 첨부되어 있다.\n\n\n\n\nimage\n\n\n\n최신인 버전 7.4.1을 다운로드 한다. 맥은 설치 전과정을 담은 pkg 파일을 제공한다.\nVirtual Box도 제공하지만 설치시점인 2024년에는 m1을 지원하지 않는다.\n수동 설치(manual install)를 위해 .tar 파일을 다운로드 했을 시 패키징 프로그램 .pkg의 기본 경로와 겹치지 않도록 압축 해제해야 한다.2\n\npkg 파일은 /Applications/freesurfer 아래에 각 버전을 명시한 폴더를 만들고 그 안에 freesurfer의 실제 파일을 설치한다.\ne.g. /Applications/freesurfer/7.4.1/\n\n\n2 “Please do not expand any tar archive such that the path to FREESURFER_HOME is /usr/local/freesurfer on linux or /Applications/freesurfer on MacOS. Those paths are overwritten by the installer packages so manual installs there may not be preserved.”\n\n\npkg파일을 열면 중간쯤 설치되다가 멈추거나 보안 오류를 발생시킨다. 윈도우에서도 그렇듯 알 수 없는 개발자가 만든 프로그램은 OS에서 설치되지 않게 막아뒀는데 애플의 경우 그 과정이 조금 까다롭다. 이 프로그램도 앱스토어에 등록되지 않은 인증서 없는 프로그램이라 설치가 막힌 것다. 보안상 문제 없는 프로그램이므로 시스템 설정에 가서 예외 처리를 해주면 된다.\n\n시스템 설정\n\n\n\nsystem settings\n\n\nprivacy & security 이동\nallow applications downloaded from ~ 하단에 이슈가 발생한 프로그램이 보이는데 open anyway 누르고 진행하면 된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nprivacy & security\nopen anyway\n\n\n\n\n\n\n\n\npkg 파일이 마저 열리면 continue 계속해서 눌러주면 된다. 따로 설정할 건 없다.\n\n설치 디스크 설정을 할 수 있는 부분이 나오는데 특별한 경우가 아닌 이상 본체에 설치한다.\n\n다 설치되면 pkg 파일을 지울건지 말건지 물어보는데 쓸 일 없으므로 지워줘도 된다.\n\n\n\n\n만약 제대로 설치하지 못했거나 지우고 싶을 경우 아래 명령어를 사용하면 된다.\n$ sudo /Applications/freesurfer/7.1.4/uninstall.sh\n\n$ 는 제외하고 입력\n버전이 다를 경우 /7.1.4/ 자리에 해당하는 버전을 넣으면 된다. (e.g. /7.1.5/)\n관리자 권한3으로 freesurfer가 설치 된 위치4에 있는 제거 파일5을 실행하겠다는 뜻이다.\n\n3 sudo4 /Applications/freesurfer/7.1.4/5 uninstall.sh\n\n\n\n\n구글링 해보면 이부분에서 걸리는 경우가 많던데 환경변수 설정 문제인 경우가 많았다.\n가장 앞의 $ 는 CLI (터미널 환경) 의 명령어를 입력 받는 줄임을 알리는 표기이므로 무시하면 된다.\n\n\n\n단계는 단순하다.\n\n우리는 설치한 freesurfer를 터미널에서 사용하고 싶다.\n그러려면 단축어 설정(alias)이 필요한데\n단축어 설정을 하려면 정확히 어떤 파일을 실행해야 하는지 명시해야 한다.\n그리고 이 과정을 매번 반복할 수 없으니 터미널을 실행할 때마다 자동으로 세팅되게 해줘야 한다.\n\n\n\n\n아래 내용을 알면 입력만 하고 넘어가도 된다.\n\n환경변수\nexport\nsource\necho\n\n\n\n# After the installer program exits, use the terminal window to setup the environment \n# with the same commands listed above except that the path \n# for FREESURFER_HOME is now /Applications/freesurfer/7.1.1.\n\n$ export FREESURFER_HOME=/Applications/freesurfer/7.1.1\n$ export SUBJECTS_DIR=$FREESURFER_HOME/subjects\n$ source $FREESURFER_HOME/SetUpFreeSurfer.sh\n-------- freesurfer-darwin-macOS-7.1.1-20200429-3a03ebd --------\nSetting up environment for FreeSurfer/FS-FAST (and FSL)\nWARNING: /Users/synpro/freesurfer/fsfast does not exist\nFREESURFER_HOME   /Applications/freesurfer/7.1.1\nFSFAST_HOME       /Users/synpro/freesurfer/fsfast\nFSF_OUTPUT_FORMAT nii.gz\nSUBJECTS_DIR      /Applications/freesurfer/7.1.1/subjects\nMNI_DIR           /Users/synpro/freesurfer/mni\n\n$ which freeview\n/Applications/freesurfer/7.1.1/bin/freeview\n\n\n\n터미널에서 아래 명령어들을 입력해주면 된다.\n# `export` 는 환경변수를 명시해주는 명령어다.\n# 7.1.4 가 아니라 다른 버전을 설치했을 시 해당 부분 변경\n\nexport FREESURFER_HOME=/Applications/freesurfer/7.1.4\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\n\n\n\nFREESURFER_HOME\n\nexport FREESURFER_HOME=/Applications/freesurfer/7.1.1\n\n실행하고자 하는 파일이 어디에 있는가? == 설치 위치 명시\n설치 경로를 매번 적어줄 수 없으니 FREESURFER_HOME 에 저장해두고 쓴다.\n\ne.g. 하단의 $FREESURFER_HOME\n\n확인하고 싶으면 터미널에 echo $FREESURFER_HOME 을 입력해본다./Applications/freesurfer/7.1.1 가 출력되면 넘어간다.\n\n\nSUBJECTS_DIR\n\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\n\nfreesurfer는 데이터를 subjects에 보관한다. 해당 위치가 아니라 다른 위치의 데이터를 사용하고 싶으면 = 이후에 원하는 경로를 입력하면 된다.\n위 명령어에서 $FREESURFER_HOME 는 변수로 저장한 설치 위치를 가져오겠다는 의미다.\n터미널을 열때마다 export해서 바꿔줄 수 있으니 웬만하면 기본 경로로 해주고 그때그때 바꿔쓰자.\n\n\nSetUpFreeSurfer.sh\n\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\n실제로 freesurfer를 실행하는 파일이다. 사전 설정 없이 /Applications/freesurfer/7.1.1/SetUpFreeSurfer.sh만 실행해도 해당 터미널에서 freesurfer는 실행된다. 모두 번거로우니 위 과정을 거치는 것이다.\n공식 문서의 ------- 아래는 freesurfer를 실행하는 환경을 말한다. 앞에서 세팅한 FREESURFER_HOME, SUBJECTS_DIR 를 확인할 수 있다.\n\n다른 경로 FSFAST_HOME, FSF_OUTPUT_FORMAT, MNI_DIR 모두 같은 방법으로 설정하면 된다.\n\n\n\n\n\n\n\n\n\n공식문서는 이렇게 설명하고 있다.\n터미널에서 단어를 입력할 경우 파일을 실행하게 하는 일종의 ’바로가기’를 alias 라고 한다.\n위에서 경로와 파일을 명시해줬으니 어떤 이름으로 실행하게 할지만 알려주면 된다.\n\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\n\nalias freesurfer =\n\nfreesurfer 라는 명령어를 입력할 경우\n\n$FREESURFER_HOME/SetUpFreeSurfer.sh\n\n실제로 freesurfer를 여는 파일을\n\nsource\n\n실행하겠다\n\n\n위와 같은 뜻이다. 명령어를 실행하고 freesurfer 를 입력하면 되는데 너무 길다면 alias freesurfer 의 freesurfer 대신 다른 명칭을 적으면 된다.\n\n\n\n\n\n자동실행을 위한 과정이다. 2번 과정을 다하고 터미널을 끄면 모든게 초기화된다. 매번 세팅을 새로하는 사람이라면 좋겠지만 터미널을 닫고 여는 일이 잦은 사람이라면 불편하지 않을 수 없다.\n그렇다고 모르는 명령어를 입력하자니 걱정될 사람들을 위해 위와 같이 자세히 적어두었다. 2번을 다 이해했다면 터미널 세팅을 열고 바꾸기만 하면 되니 편하게 복사해서 붙여넣자.\n\n\nbash를 기본으로 하고, vim을 사용할 줄 모른다는 전제로 적는다.\n\n\n\n\nvi ~/.bash_profile 명령어로 터미널 설정 파일을 연다.\n\n~/.bash_profile : 터미널 세팅\nbash_profile 대신 bashrc 써도 상관 없다! 둘의 차이점을 안다면 편한대로 작성하자!\n\n참고: [17] 리눅스 - Bash 쉘 스크립트 작성 (bash shell, alias, history, 명령 재실행)\n\n~/.bash_profile : 각 사용자에게 적용되는 환경 설정과 시작 프로그램을 지정하는 파일\n~/.bashrc : 각 사용자의 별명과 함수들을 정의하는 파일\n\n\nzsh 을 기본 터미널로 써도 괜찮다. zsh을 열때 bash 설정을 참조하도록 설정되어 있을 것이다.\n터미널에 cat ~/.zshrc | grep bash 를 입력했을 때 출력6이 뭐라도 보인다면 문제 없다.7\n\n\n6 if [ -f ~/.bash_profile ]; then . ~/.bash_profile; fi 등7 터미널 설정 파일에 bash쉘 설정파일을 참조하는게 있는지 확인하는 명령어다.\n\n\nvi/vim을 사용할 줄 모른다면 아무것도 누르지말고 아래를 따라한다. 안다면 2.5 내용을 붙여넣기만 하면 된다.\n\n영문인지 키보드 상태 확인한다.8\nPress ENTER or type command to continue 보고 엔터\n최하단의 Normal 을 보고 a 를 눌러 입력모드로 진입한다.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n막 켜졌을 때\na눌렀을 때`\n\n\n\n화살표 눌러 최하단으로 이동\n아래 내용 붙여넣기10\n# freesurfer\nexport FREESURFER_HOME=\"/Applications/freesurfer/7.4.1\"\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\nesc 누르고 :wq 입력한 후에 엔터\n\n\n\nbash_profile\n\n\n터미널로 돌아갔는지 확인\n\n8 한글이면 영문으로 변경9 Insert 상태가 되지 않았을 때에는 뭘 눌러도 입력되지 않는다. 주의할 것.10 경로나 버전이 다르다면 당연히 바꿔줘야 한다.\n\n\n\n터미널을 닫았다가 다시 열거나 source ~/.bash_profile 또는 source ~/.zshrc 입력하여 설정을 적용한다.\n설정한 단축어인 freesurfer를 입력했을 때 /Applications/freesurfer/7.1.1/SetUpFreeSurfer.sh 를 실행했던 결과와 같은 결과11가 나오면 성공\n\nfreeview는 freesurfer가 실행되어 있으면 함께 사용할 수 있다.\n\n\n11 freesurfer 정보가 나온다.\n\n\n뭐 이렇게까지 적어놨나 싶지만 공식문서에서 여러 경우의 수를 고려했던 모양인지 단순한 문제를 복잡하게 적어두기도 했고, 이런 문제나 또 이런 문제도 발견해서 세팅하는 김에 가볍게 적었다. 다 아는 내용이라면 넘어가자. 연구실에서 WSL 환경도 세팅 중이니 끝나면 따로 가이드를 적어보겠다.\n\n\n\n\n\n문제들은 확인되는대로 보충한다. (2024.03.14)\n\n\n\n\nIS APP SILICON READY?\n\n\n이슈라기보다는 확인 중인 내용인데, application의 get info에서 보면 kind에 intel이라고 적혀있다. 그래도 아직까진 문제없이 돌아가니 튜토리얼 진행하면서 안되는 부분을 발견하면 적겠다.\n\n위에도 적었지만 우선 MATLAB은 M1 칩과 호환된다: Is MATLAB Apple silicon ready?: Yes, Native Apple Silicon support (R2023b)\n\n의견들\n\nm1 관련 내용은 반년 전까지 누군가 업데이트 해둔게 있다: https://github.com/neurolabusc/AppleSiliconForNeuroimaging M1을 권장하지는 않지만 이미 쓰고 있는데 어쩌겠나.\n(v 7.1.1) Apple Silicon M1 compatibility - FreeSurfer YES; everything else no\n\n\n\n\n\n\n라이센스 추가했는지 확인\n\nhttps://neurostars.org/t/freeview-quit-unexpectedly/26134\n\n\n\n\n\n/Applications/freesurfer/7.2.0/bin/freeview: line 2: 2150 Segmentation fault: 11 \n$FREESURFER_HOME/Freeview.app/Contents/MacOS/freeview “$@”\n\nhttps://neurostars.org/t/problem-installing-freesurfer-in-mac/20552/18\n\n놀랍게도 보조 모니터 이슈다. freeview까지 본체로 이동할 필요는 없고 터미널만 원래 모니터로 이동시키면 해결 된다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#준비",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#준비",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "FreeSurfer Download and Install\n\nFreeSurfer Release 7 System Requirements\n\nSummary of Requirements:\nSee list of supported operating systems for each release below.\n    Intel processor supporting AVX instructions\n    RAM: 8GB for recon, 16GB suggested for viewing graphics\n    Graphics card: 3D graphics card with its own graphics memory & accelerated OpenGL drivers\n    Size of installed image: 16GB\n    Typical size of a processed subject: 300MB\n    Other requirements: Matlab (only needed to run FS-FAST, the fMRI analysis stream)\n아래 환경에서 설정했다. Matlab 이외의 프로그램은 필요하지 않다.1 - MATLAB\n1 튜토리얼 진행 중 문제 발견시 추가 예정 (24.03.14)\n\n\nlocal environment"
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#설치",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#설치",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "공식 가이드 (step by step installation demo) 를 따라하면 웬만한 과정은 다 해결 된다.\n\n최신 버전의 패키징 파일을 다운로드 한다.\n\n\n\nMacOS Install & Setup 에 자세한 스크린샷이 첨부되어 있다.\n\n\n\n\nimage\n\n\n\n최신인 버전 7.4.1을 다운로드 한다. 맥은 설치 전과정을 담은 pkg 파일을 제공한다.\nVirtual Box도 제공하지만 설치시점인 2024년에는 m1을 지원하지 않는다.\n수동 설치(manual install)를 위해 .tar 파일을 다운로드 했을 시 패키징 프로그램 .pkg의 기본 경로와 겹치지 않도록 압축 해제해야 한다.2\n\npkg 파일은 /Applications/freesurfer 아래에 각 버전을 명시한 폴더를 만들고 그 안에 freesurfer의 실제 파일을 설치한다.\ne.g. /Applications/freesurfer/7.4.1/\n\n\n2 “Please do not expand any tar archive such that the path to FREESURFER_HOME is /usr/local/freesurfer on linux or /Applications/freesurfer on MacOS. Those paths are overwritten by the installer packages so manual installs there may not be preserved.”\n\n\npkg파일을 열면 중간쯤 설치되다가 멈추거나 보안 오류를 발생시킨다. 윈도우에서도 그렇듯 알 수 없는 개발자가 만든 프로그램은 OS에서 설치되지 않게 막아뒀는데 애플의 경우 그 과정이 조금 까다롭다. 이 프로그램도 앱스토어에 등록되지 않은 인증서 없는 프로그램이라 설치가 막힌 것다. 보안상 문제 없는 프로그램이므로 시스템 설정에 가서 예외 처리를 해주면 된다.\n\n시스템 설정\n\n\n\nsystem settings\n\n\nprivacy & security 이동\nallow applications downloaded from ~ 하단에 이슈가 발생한 프로그램이 보이는데 open anyway 누르고 진행하면 된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nprivacy & security\nopen anyway\n\n\n\n\n\n\n\n\npkg 파일이 마저 열리면 continue 계속해서 눌러주면 된다. 따로 설정할 건 없다.\n\n설치 디스크 설정을 할 수 있는 부분이 나오는데 특별한 경우가 아닌 이상 본체에 설치한다.\n\n다 설치되면 pkg 파일을 지울건지 말건지 물어보는데 쓸 일 없으므로 지워줘도 된다.\n\n\n\n\n만약 제대로 설치하지 못했거나 지우고 싶을 경우 아래 명령어를 사용하면 된다.\n$ sudo /Applications/freesurfer/7.1.4/uninstall.sh\n\n$ 는 제외하고 입력\n버전이 다를 경우 /7.1.4/ 자리에 해당하는 버전을 넣으면 된다. (e.g. /7.1.5/)\n관리자 권한3으로 freesurfer가 설치 된 위치4에 있는 제거 파일5을 실행하겠다는 뜻이다.\n\n3 sudo4 /Applications/freesurfer/7.1.4/5 uninstall.sh"
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#환경변수-설정",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#환경변수-설정",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "구글링 해보면 이부분에서 걸리는 경우가 많던데 환경변수 설정 문제인 경우가 많았다.\n가장 앞의 $ 는 CLI (터미널 환경) 의 명령어를 입력 받는 줄임을 알리는 표기이므로 무시하면 된다.\n\n\n\n단계는 단순하다.\n\n우리는 설치한 freesurfer를 터미널에서 사용하고 싶다.\n그러려면 단축어 설정(alias)이 필요한데\n단축어 설정을 하려면 정확히 어떤 파일을 실행해야 하는지 명시해야 한다.\n그리고 이 과정을 매번 반복할 수 없으니 터미널을 실행할 때마다 자동으로 세팅되게 해줘야 한다.\n\n\n\n\n아래 내용을 알면 입력만 하고 넘어가도 된다.\n\n환경변수\nexport\nsource\necho\n\n\n\n# After the installer program exits, use the terminal window to setup the environment \n# with the same commands listed above except that the path \n# for FREESURFER_HOME is now /Applications/freesurfer/7.1.1.\n\n$ export FREESURFER_HOME=/Applications/freesurfer/7.1.1\n$ export SUBJECTS_DIR=$FREESURFER_HOME/subjects\n$ source $FREESURFER_HOME/SetUpFreeSurfer.sh\n-------- freesurfer-darwin-macOS-7.1.1-20200429-3a03ebd --------\nSetting up environment for FreeSurfer/FS-FAST (and FSL)\nWARNING: /Users/synpro/freesurfer/fsfast does not exist\nFREESURFER_HOME   /Applications/freesurfer/7.1.1\nFSFAST_HOME       /Users/synpro/freesurfer/fsfast\nFSF_OUTPUT_FORMAT nii.gz\nSUBJECTS_DIR      /Applications/freesurfer/7.1.1/subjects\nMNI_DIR           /Users/synpro/freesurfer/mni\n\n$ which freeview\n/Applications/freesurfer/7.1.1/bin/freeview\n\n\n\n터미널에서 아래 명령어들을 입력해주면 된다.\n# `export` 는 환경변수를 명시해주는 명령어다.\n# 7.1.4 가 아니라 다른 버전을 설치했을 시 해당 부분 변경\n\nexport FREESURFER_HOME=/Applications/freesurfer/7.1.4\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\n\n\n\nFREESURFER_HOME\n\nexport FREESURFER_HOME=/Applications/freesurfer/7.1.1\n\n실행하고자 하는 파일이 어디에 있는가? == 설치 위치 명시\n설치 경로를 매번 적어줄 수 없으니 FREESURFER_HOME 에 저장해두고 쓴다.\n\ne.g. 하단의 $FREESURFER_HOME\n\n확인하고 싶으면 터미널에 echo $FREESURFER_HOME 을 입력해본다./Applications/freesurfer/7.1.1 가 출력되면 넘어간다.\n\n\nSUBJECTS_DIR\n\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\n\nfreesurfer는 데이터를 subjects에 보관한다. 해당 위치가 아니라 다른 위치의 데이터를 사용하고 싶으면 = 이후에 원하는 경로를 입력하면 된다.\n위 명령어에서 $FREESURFER_HOME 는 변수로 저장한 설치 위치를 가져오겠다는 의미다.\n터미널을 열때마다 export해서 바꿔줄 수 있으니 웬만하면 기본 경로로 해주고 그때그때 바꿔쓰자.\n\n\nSetUpFreeSurfer.sh\n\nsource $FREESURFER_HOME/SetUpFreeSurfer.sh\n\n실제로 freesurfer를 실행하는 파일이다. 사전 설정 없이 /Applications/freesurfer/7.1.1/SetUpFreeSurfer.sh만 실행해도 해당 터미널에서 freesurfer는 실행된다. 모두 번거로우니 위 과정을 거치는 것이다.\n공식 문서의 ------- 아래는 freesurfer를 실행하는 환경을 말한다. 앞에서 세팅한 FREESURFER_HOME, SUBJECTS_DIR 를 확인할 수 있다.\n\n다른 경로 FSFAST_HOME, FSF_OUTPUT_FORMAT, MNI_DIR 모두 같은 방법으로 설정하면 된다.\n\n\n\n\n\n\n\n\n\n공식문서는 이렇게 설명하고 있다.\n터미널에서 단어를 입력할 경우 파일을 실행하게 하는 일종의 ’바로가기’를 alias 라고 한다.\n위에서 경로와 파일을 명시해줬으니 어떤 이름으로 실행하게 할지만 알려주면 된다.\n\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\n\nalias freesurfer =\n\nfreesurfer 라는 명령어를 입력할 경우\n\n$FREESURFER_HOME/SetUpFreeSurfer.sh\n\n실제로 freesurfer를 여는 파일을\n\nsource\n\n실행하겠다\n\n\n위와 같은 뜻이다. 명령어를 실행하고 freesurfer 를 입력하면 되는데 너무 길다면 alias freesurfer 의 freesurfer 대신 다른 명칭을 적으면 된다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#터미널-설정",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#터미널-설정",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "자동실행을 위한 과정이다. 2번 과정을 다하고 터미널을 끄면 모든게 초기화된다. 매번 세팅을 새로하는 사람이라면 좋겠지만 터미널을 닫고 여는 일이 잦은 사람이라면 불편하지 않을 수 없다.\n그렇다고 모르는 명령어를 입력하자니 걱정될 사람들을 위해 위와 같이 자세히 적어두었다. 2번을 다 이해했다면 터미널 세팅을 열고 바꾸기만 하면 되니 편하게 복사해서 붙여넣자.\n\n\nbash를 기본으로 하고, vim을 사용할 줄 모른다는 전제로 적는다.\n\n\n\n\nvi ~/.bash_profile 명령어로 터미널 설정 파일을 연다.\n\n~/.bash_profile : 터미널 세팅\nbash_profile 대신 bashrc 써도 상관 없다! 둘의 차이점을 안다면 편한대로 작성하자!\n\n참고: [17] 리눅스 - Bash 쉘 스크립트 작성 (bash shell, alias, history, 명령 재실행)\n\n~/.bash_profile : 각 사용자에게 적용되는 환경 설정과 시작 프로그램을 지정하는 파일\n~/.bashrc : 각 사용자의 별명과 함수들을 정의하는 파일\n\n\nzsh 을 기본 터미널로 써도 괜찮다. zsh을 열때 bash 설정을 참조하도록 설정되어 있을 것이다.\n터미널에 cat ~/.zshrc | grep bash 를 입력했을 때 출력6이 뭐라도 보인다면 문제 없다.7\n\n\n6 if [ -f ~/.bash_profile ]; then . ~/.bash_profile; fi 등7 터미널 설정 파일에 bash쉘 설정파일을 참조하는게 있는지 확인하는 명령어다.\n\n\nvi/vim을 사용할 줄 모른다면 아무것도 누르지말고 아래를 따라한다. 안다면 2.5 내용을 붙여넣기만 하면 된다.\n\n영문인지 키보드 상태 확인한다.8\nPress ENTER or type command to continue 보고 엔터\n최하단의 Normal 을 보고 a 를 눌러 입력모드로 진입한다.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n막 켜졌을 때\na눌렀을 때`\n\n\n\n화살표 눌러 최하단으로 이동\n아래 내용 붙여넣기10\n# freesurfer\nexport FREESURFER_HOME=\"/Applications/freesurfer/7.4.1\"\nexport SUBJECTS_DIR=$FREESURFER_HOME/subjects\nalias freesurfer=\"source $FREESURFER_HOME/SetUpFreeSurfer.sh\"\nesc 누르고 :wq 입력한 후에 엔터\n\n\n\nbash_profile\n\n\n터미널로 돌아갔는지 확인\n\n8 한글이면 영문으로 변경9 Insert 상태가 되지 않았을 때에는 뭘 눌러도 입력되지 않는다. 주의할 것.10 경로나 버전이 다르다면 당연히 바꿔줘야 한다.\n\n\n\n터미널을 닫았다가 다시 열거나 source ~/.bash_profile 또는 source ~/.zshrc 입력하여 설정을 적용한다.\n설정한 단축어인 freesurfer를 입력했을 때 /Applications/freesurfer/7.1.1/SetUpFreeSurfer.sh 를 실행했던 결과와 같은 결과11가 나오면 성공\n\nfreeview는 freesurfer가 실행되어 있으면 함께 사용할 수 있다.\n\n\n11 freesurfer 정보가 나온다.\n\n\n뭐 이렇게까지 적어놨나 싶지만 공식문서에서 여러 경우의 수를 고려했던 모양인지 단순한 문제를 복잡하게 적어두기도 했고, 이런 문제나 또 이런 문제도 발견해서 세팅하는 김에 가볍게 적었다. 다 아는 내용이라면 넘어가자. 연구실에서 WSL 환경도 세팅 중이니 끝나면 따로 가이드를 적어보겠다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#이슈들",
    "href": "posts/LabHAI/tutorial/freesurfer/installation_macos.html#이슈들",
    "title": "FreeSurfer v7 installation A-Z (MacOS) for apple silicon M1 (Senoma 14.4)",
    "section": "",
    "text": "문제들은 확인되는대로 보충한다. (2024.03.14)\n\n\n\n\nIS APP SILICON READY?\n\n\n이슈라기보다는 확인 중인 내용인데, application의 get info에서 보면 kind에 intel이라고 적혀있다. 그래도 아직까진 문제없이 돌아가니 튜토리얼 진행하면서 안되는 부분을 발견하면 적겠다.\n\n위에도 적었지만 우선 MATLAB은 M1 칩과 호환된다: Is MATLAB Apple silicon ready?: Yes, Native Apple Silicon support (R2023b)\n\n의견들\n\nm1 관련 내용은 반년 전까지 누군가 업데이트 해둔게 있다: https://github.com/neurolabusc/AppleSiliconForNeuroimaging M1을 권장하지는 않지만 이미 쓰고 있는데 어쩌겠나.\n(v 7.1.1) Apple Silicon M1 compatibility - FreeSurfer YES; everything else no\n\n\n\n\n\n\n라이센스 추가했는지 확인\n\nhttps://neurostars.org/t/freeview-quit-unexpectedly/26134\n\n\n\n\n\n/Applications/freesurfer/7.2.0/bin/freeview: line 2: 2150 Segmentation fault: 11 \n$FREESURFER_HOME/Freeview.app/Contents/MacOS/freeview “$@”\n\nhttps://neurostars.org/t/problem-installing-freesurfer-in-mac/20552/18\n\n놀랍게도 보조 모니터 이슈다. freeview까지 본체로 이동할 필요는 없고 터미널만 원래 모니터로 이동시키면 해결 된다."
  },
  {
    "objectID": "posts/Graph/GNN101.html",
    "href": "posts/Graph/GNN101.html",
    "title": "Graph 101 (작성중)",
    "section": "",
    "text": "denote\nmeaning\nnote\n\n\n\n\n\\(\\cal{G} = (\\cal{V, E})\\)\nformal definition of graph\ncalligraphic font used\n\n\n\\(\\cal{V}\\)\na set of nodes\nnode = vertex\n\n\n\\(\\cal{E}\\)\na set of edges, edges between nodes\n\\(\\therefore\\) edge needs coordinate-likely information to denote each one\n\n\n\\(u \\in \\cal{V}\\)\na node of node set \\(\\cal{V}\\)\nnormally node denoted by \\(\\cal{u}\\) or \\(\\cal{v}\\)\n\n\n\\((u, v) \\in \\cal{E}\\)\nwhen u, v in \\(\\cal{V}\\), it means an edge from u to v vice versa\ncheck it is direct graph or not"
  },
  {
    "objectID": "posts/Graph/GNN101.html#general-notation",
    "href": "posts/Graph/GNN101.html#general-notation",
    "title": "Graph 101 (작성중)",
    "section": "",
    "text": "denote\nmeaning\nnote\n\n\n\n\n\\(\\cal{G} = (\\cal{V, E})\\)\nformal definition of graph\ncalligraphic font used\n\n\n\\(\\cal{V}\\)\na set of nodes\nnode = vertex\n\n\n\\(\\cal{E}\\)\na set of edges, edges between nodes\n\\(\\therefore\\) edge needs coordinate-likely information to denote each one\n\n\n\\(u \\in \\cal{V}\\)\na node of node set \\(\\cal{V}\\)\nnormally node denoted by \\(\\cal{u}\\) or \\(\\cal{v}\\)\n\n\n\\((u, v) \\in \\cal{E}\\)\nwhen u, v in \\(\\cal{V}\\), it means an edge from u to v vice versa\ncheck it is direct graph or not"
  },
  {
    "objectID": "posts/Graph/GNN101.html#what-is-a-graph",
    "href": "posts/Graph/GNN101.html#what-is-a-graph",
    "title": "Graph 101 (작성중)",
    "section": "1.1 What is a graph",
    "text": "1.1 What is a graph\n그래프에서의 머신러닝을 논하기 이전에 ’그래프 데이터’가 정확히 무엇을 의미하는지 나타내는 공적인 표현에 대해 조금 알아둘 필요가 있다. 공식적으로, 그래프 \\(\\cal{G} = (\\cal{V, E})\\) 는 정점의 집합인 \\(\\cal{V}\\) 와 정점 사이의 간선의 집합인 \\(\\cal{E}\\) 로 정의된다. 정점 \\(u \\in \\cal{V}\\) 와 \\(v \\in \\cal{V}\\) 로 이루어진 간선은 다음과 같이 정의한다: \\((u, v) \\in \\cal{E}\\). 많은 경우에 (우리는) 단순 그래프 (simple graph) 만을 고려하는데, 단순 그래프는 대부분 하나의 간선이 각 정점의 쌍 사이에 존재하는 집합으로 어떤 간선도 정점 하나에 존재하지는 않는 그래프이다.\n그래프를 편리하게 표현하는 방법은 adjacency matrix (인접행렬) \\(A \\in \\mathbb{R}^{|\\cal{V}|*|\\cal{V}|}\\) 를 사용하는 방법이다. 1 \\(A\\) 로 그래프를 표현하기 위해서는 그래프의 정점을 순서대로 배열함으로써 모든 정점의 색인들(indexes)이 인접행렬 \\(A\\)의 각각의 행과 열이 되게 해야 한다. 그렇게 하면 다음 조건의 행렬에서의 모든 간선의 존재를 표현할 수 있다: \\(A[u, v] = 1\\) if \\((u,v) \\in \\cal(E)\\) and \\(A[u, v] = 0\\) otherwise 2 3 만약 그래프가 방향이 없는 간선 (undirected edge)으로만 구성된 경우 인접행렬 \\(A\\)는 대칭행렬이 된다. 하지만 간선들이 방향성이 있다면 (edge direction matters) \\(A\\)는 대칭이지 않아도 된다. 몇몇 그래프들은 가중치를 가질 수도 있는데 (weighted edges) 그 경우 그래프에 기재되는 값이 {0, 1}이 아닌 임의의 실수가 된다. 예를 들어 가중 그래프 중 단백질간 상호작용 그래프는 두 단백질 사이의 연관된 힘을 나타내는 그래프로 쓰일 수 있다.\n\n1.1.1 Multi-relational Graph\nmulti-relational graph는 방향이 있는 간선, 없는 간선, 가중치가 있는 간선을 넘어 다양한 종류의 간선이 있는 그래프를 고려한다. 예를 들어, 약물과 약물의 상호작용 그래프에서 각 간선이 두 약물을 동시에 복용할 때 발생할 수 있는 부작용에 대하여 서로 두가지의 간선이 필요할 수 있다. 이 예에서 간선 표기법을 확장하여 다음을 표현할 수 있다:\n간선 또는 관계 유형 \\(\\tau\\), (\\(u\\), \\(\\tau\\), \\(v\\)) \\(\\in\\) \\(\\cal{E}\\), 그리고 하나의 인접행렬 \\(A_{\\tau}\\) 를 간선 종류마다 정의할 수 있다. 이러한 그래프를 multi-relational 하다고 말하며 전체의 그래프는 인접 텐서 \\(\\cal{A} \\in \\mathbb{R}^{\\cal{|V| * |R| * |V|}}\\) 로 정의된다. 4 multi-relational graph의 두가지 중요한 부분집합은 1. heterogenous, 2. multiplex 그래프로 나뉜다. tau (\\(\\tau\\))는 간선의 타입을 의미하며, 간선의 종류에는 위에 기술한 바와 같이 방향이 있는 것, 없는 것, 가중치가 있는 것 등이 포함된다. 이렇게 간선의 종류가 달라지면 Adjacency Matrix \\(A\\)도 따로 정의해야 한다.\n\nHeterogeneous graph\nHeterogeneous graph에서, 정점들은 type에 물들어있다. 다시 말해, 정점 집합의 일부는 다음과 같이 해체될 수 있다: \\(\\cal{V = V_1} \\cup \\cal{V_2} \\cup \\dots \\cup \\cal{V_k}\\) where \\(\\cal{V_i} \\cap \\cal{V_j} = \\emptyset, \\forall_i \\neq j\\). heterogeneous graph의 간선은 일반적으로 제한이 걸려있는데 이는 집합 \\(\\cal{V}\\)가 서로 겹치지 않는 부분집합 \\(\\cal{V_1}\\) 부터 \\(\\cal{V_k}\\) 5 의 합집합이다."
  },
  {
    "objectID": "posts/Graph/GNN101.html#footnotes",
    "href": "posts/Graph/GNN101.html#footnotes",
    "title": "Graph 101 (작성중)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\mathbb{R}\\) : Real number set ($\\mathbb{R}$ in TeX)↩︎\n\\(A[u, v] = 1\\): \\(u\\)와 \\(v\\) 사이에 간선이 존재할 때 (\\(A[u, v] = 0\\): 존재하지 않을 때)↩︎\n\\((u, v)\\) 가 그래프 안에 있고 간선 집합 \\(\\cal{E}\\) 안에 존재할 때 \\(A\\)의 위치 \\((u, v)\\)에 올 수 있는 값은 간선이 존재하거나 (1) 존재하지 않는 (0) 두 가지 경우의 수 뿐이다. ↩︎\n행렬 A가 실수 집합 V, R의 계량수(cardinality, cardinal number)로 결정되는 차원으로 구성된다.↩︎\n\\(\\forall\\) : forall↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html",
    "title": "How to read a paper",
    "section": "",
    "text": "처음부터 정독하며 필요한 부분을 발췌 혹은 보충한다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#abstract",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#abstract",
    "title": "How to read a paper",
    "section": "Abstract",
    "text": "Abstract\n연구자들이 논문을 읽는데 사용하는 시간에 비하여 그것을 읽는 방법은 충분히 가르쳐지지 않고 있으며 이는 상당한 손실을 야기한다. 본 논문은 논문을 읽을 때에 실용적이고 효과적인 three-pass method 개요를 서술할 것이며 포괄적 문헌 조사 (literature survey)에서 이를 어떻게 사용하는지 보이려 한다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#introduction",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#introduction",
    "title": "How to read a paper",
    "section": "Introduction",
    "text": "Introduction\n연구자들은 논문을 여러가지 이유로 읽는다.\n\n최신 동향을 살피기 위해\n새로운 분야로의 진출을 위해\n컨퍼런스나 수업의 리뷰를 위함\n\n리뷰란 정확히 무엇을 말하는가?: 전반적으로 검토하는 행위 혹은 리뷰 논문을 읽는 행위 등을 말할 수 있으나 본 맥락에서는 피어리뷰(peer review)1를 말하는 것 같다.\n\n\n1 동료 평가로, 타 연구자의 연구를 객관적으로 분석하고 저널에 실리기 적합한지 판정하는 과정이다. 저널의 영역, 목적, 독자층을 살피는 과정이 수반되며 논문의 가치를 판정하기 위해 몇 가지 항목을 검토하게 된다.\n검토되는 항목은 다음과 같다.\n\n보존 가치\n참신성\n연구 윤리/직업적 무결성\n글의 완성도\n데이터의 질과 분석의 타당성\n결론의 타당성\n\n\n\n\n\n\n\n\nReviewer Guidelines for Journal Selections\n\n\n\n\n\n\n논문 작성 시 유념할 것\n\n\n보존가치 Archival: 저널에 싣는다는 것은 어떤 형태로든 해당 연구가 기록되고 외부에 퍼진다는 의미다. 따라서 저널에 실릴 논문은 결과나 해석의 가치가 있는 결과를 포함해야 한다.\n\n논문의 내용이 과거의 연구와 관계있거나 향후 연구의 초석이 될 가능성이 있는가?\n결과나 해석이 지속 가능한 과학적 가치를 가지고 있는가?\n해당 주제가 연구 분야에서 중요한가?\nSOTA (State Of The Art) 를 넘어서는 성능 혹은 강점을 가지고 있는가?\n\n참신성 Innovative: 기술적으로 새롭거나, 혁신적이거나, 구조적인 리뷰여야 한다. 연구의 가치를 판정함에 있어 가장 중요한 지표로 보인다.\n\n주제가 오늘날의 청중에게 흥미로운 내용이거나 중요한 내용인가?\n아이디어/정보/방법이 가치있거나 새롭거나 창의적인가?\n저자가 제시한 정보가 완전히 새로운 것인가?\n분석적이거나, 수치적이거나, 실험적인 결과 혹은 해석이 독창적인가? (Are ~ original 에서 original을 독창적으로 해석)\n결과가 미칠 영향이 선명하게 기술되어 있는가?\n\n직업적 무결성 Professional Integrity: 기본적인 윤리에 대한 문제다. 연구자로서 개인의 사적 이익과 견해를 최대한 배제하고 객관적이고 공정하게 연구에 임했는지 확인한다.\n\n특정 자본과 밀접한 관계를 맺고 있지 않은가?\n개인적이거나 편향된 관점을 가지지는 않았는가?\n선행연구가 적절히 인용되어 있는가?\n(만약 그렇게 주장할 경우) 실제로 최초로 사용된 기술인가?\n반대되는 주장이나 경쟁 구도에 있는 연구 등을 폄하하지 않는가?\n선행연구를 공정하고 건설적인 방식으로 참조하고 있는가?\n연구자로서 적절한 규정 및 윤리 지침을 따랐는가?\n\n발표 Presentation: 논문도 글이다. 읽히기에 적합한 글인지, 연구의 내용을 명료하고 병확하게 기술했는지 판정한다.\n\n서론에서 동기를 설명하고 독자의 방향을 제시하는가?\n논문에 수행 작업, 방법, 주된 결과가 설명되어 있는가?\n주제에서 벗어나지 않았는가?\n표와 그림이 명확하고 정확한가?\n논문에서 사용한 개념이 명확히 제시되어 있는가?\n제목과 주요 단어가 오용되지 않았는가?\n논문의 길이가 논문의 범위에 적합한지까지 살피는 줄 몰랐는데 그렇다고 한다.\n기본적인 작문 기술을 갖추었는가?; 단어 선택, 문장 구조, 단락 구성, 참고 문헌 인용 등\n\n질 Quality: 데이터의 질과 분석과 기술의 타당성을 확인한다. 아무리 좋은 데이터라도 적합한 분석 과정을 거치지 않는다면 무의미하며, 아무리 좋은 기술이라도 적절한 데이터를 사용하지 않는다면 적합한 결과를 확인할 수 없다.\n\n기술적으로 건전한가?\n강점 뿐 아니라 그 한계까지 평가하고 있는가?\n평가 지표가 명시되어 있는가?\n선행 연구를 충분히 검토하였는가?\n선행 연구에서 이어지는 연구인가? 즉, 이전에 입증된 연구에서 참조된 가정으로부터 비롯된 연구인가?\n\n결론의 타당성 Soundness of conclusions: 글과 주장을 올바르게 맺었는지 확인한다.\n\n주장이 확고한가?\n결론이 이론적, 실험적으로 타당한가?\n제시한 사실이 충분히 결론을 뒷받침 할 수 있는가?\n\n\n\n출처: SAE International\n\n\n\n\n이와 같은 이유로 연구자들은 매년 수백시간을 논문을 읽는데 사용한다. 당연히 논문을 읽는 방법이 중요할 수 밖에 없는데, 적절하게 논문을 읽는 방법이 가르쳐지지 않은 실정이다. 대학원생들은 스스로 시행착오를 겪으며 ’논문 읽기’를 습득해야 하는데 이는 노력을 낭비하는 일이 아닐 수 없다.\n여러해 동안 저자는 쉽고 효율적으로 논문에 접근하는 방법을 사용해왔는데, 이를 이 논문에서 제시하겠다고 한다.\n여기까지가 도입부의 내용이다. callout으로 정리한 내용에 따르면 일반적으로 introduction 에는 ‘동기’ 및 ’연구의 방향’을 제시하는 과정이 필요하다. 이번 논문의 내용을 살펴보자. 서론은 세줄로 요약이 가능하다. (1) 연구자들은 논문을 많이 읽는다/읽어야 한다. (2) 그러나 도움을 받을 방법이 없기에 그 접근 장벽은 상당히 높아졌다. (3) 저자는 이를 쉽게 해결할 방법(three-pass approach)을 알고 있다.\n짧은 도입부지만 여기에는 동기 (1~2) 와 연구의 방향 (3) 이 모두 제시되어 있다. 도입부로써 적절하다고 판단할 수 있겠다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#the-three-pass-approach",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#the-three-pass-approach",
    "title": "How to read a paper",
    "section": "The Three-pass approach",
    "text": "The Three-pass approach\n주요 아이디어는: 처음부터 끝까지 냅다 읽어내리지 말고, 세번정도에 나누어 읽으라는 것이다. 각각의 ’pass’에는 성취해야 할 목표들이 있는데 이를 나열하면 다음과 같다.\n\n논문의 전반적인 핵심 논제를 파악할 것\n실제 논문의 내용을 이해할 것, 그러나 모든 세부 내용까지 알 필요는 없다.\n깊이 읽으며 앞선 pass에서 읽어내지 못한 부분까지 파악한다.\n\n\nThe first pass\n읽어볼만 한지, 그렇다면 얼마나 깊게 읽어야 하는지 전반적으로 판단하는 단계다. 5분에서 10분을 들여 빠르게 스키밍 한다. 단계는 다음과 같다.\n\n제목, 초록, 소개를 읽고\n섹션과 하위섹션의 제목을 읽으며 무슨 내용일지 파악한다.\n결론을 확인한다.\n참고문헌을 훑어보며2 읽은 내용을 전반적으로 검토한다.3\n\n2 glance: 흘낏 보다, 휙휙 훑어보다.3 tick sbd/sth off: (이미 처리했음을 나타내기 위해) ~에 체크 표시(✓)를 하다해당 단계가 제대로 진행 되었다면 다음 항목에 대답할 수 있어야 한다.\n\n범주: 어떤 종류의 논문인가? - 주제, 연구방법 등\n맥락: 다른 논문과의 연관성은 어떠한가?\n정확도: 가정은 타당해 보이는가?\n기여도: 주된 기여도는 무엇인가? - novelty 등을 말하는 것으로 보임\n명확성: 논문은 잘 쓰여졌는가?\n\n위 질문에 대답함으로써 이해도를 파악하고, 그 대답을 통해 논문을 마저 읽을지 말지 결정할 수 있는데 배경 지식이 부족해 충분히 이해하지 못했거나, 논문의 주장이 빈약하거나, 논문의 질 자체가 좋지 않은 경우 더 읽지 않아도 좋다. 다만 연구 분야가 아니더라도 향후 도움이 될 것으로 판단되면 첫번째 단계는 통과한 것으로 본다.\n\n\n\n\n\n\n제목과 소제목들은 이래서 중요하다.\n\n\n\n\n\n첫번째 단계에서 더 읽을지 말지가 결정된다. 작성한 논문이 읽힐지 말지 결정되는건 이 짧은 5~10분 사이의 시간이므로 섹션 제목을 일관성 있게 선택하고 간결하며 포괄적인 초록을 작성하는데 주의를 기울여라.\n\n리뷰 통과 뿐 아니라, 일반 연구자들을 위해서도 그렇게 해야 한다.\n5분이 지나도 논문의 핵심을 이해하지 못하면 논문이 읽히지 않을 가능성이 높다.\n\n\n\n\n\n\nThe second pass\n첫번째 단계를 통과했다면 더 주의깊게 읽을 때가 됐다. 세부사항은 잠시 넘겨두고 논문을 이해하는데 초점을 맞추자. 메모하거나 의견을 정리하는 것도 도움이 된다.\n\n시각자료 검토\n특히 논문의 그림, 도표 등 시각 자료를 시간들여 살피는 것이 좋은데, 이는 논문의 질을 판단하는 주요한 요소가 되기 때문이다. 예를 들어; 축에 레이블이 제대로 붙어있는지, 결과가 오차 막대와 함께 포함되어 있는지4 결론이 통계적으로 유효한지 확인할 수 있는 방법이다.] 등을 확인하는 것이 좋다.\n참고문헌 검토\n또한, 참고문헌의 경우 나중에 읽을 것을 분류하는게 좋은데 이는 논문의 배경에 대해 알 수 있는 좋은 방법이기 때문이다. 읽지 않은 참고 문헌을 나중에 읽을 수 있게 표시하라고 명시해 두었으니, 참고문헌을 찾아 읽는 습관도 함께 들여두자.\n\n그렇다면 두번째 단계를 통과했는지 알아보는 척도는 무엇일까. 아마도 내용의 이해일 것이다.\n\nAfter this pass, you should be able to grasp the content of the paper. You should be able to summarize the main thrust of the paper, with supporting evidence, to someone else. This level of detail is appropriate for a paper in which you are interested, but does not lie in your research speciality.\n\n한시간 안에 이 단계를 끝마치는게 좋다.5 만약 한시간이 지나도 논문을 이해하지 못한다면 우리에겐 세가지 선택지가 남는다.\n5 전문 분야는 한시간 이내에 끝내야 한다는 의미일까? 아니면 더 오랜 시간을 들여서 검토해야 한다는 의미일까? 전자인것 같긴 하다.\n포기한다.\n미뤄둔다.\n계속한다.\n\n포기한다고 해서 세상이 끝나는건 아니다. 미룬다고 해서 큰일이 당장 벌어지는 것도 아닐 것이다. 특히 두번째 선택지는, 전진을 위한 후퇴에 가깝다. 배경지식을 습득하고 보는 것이기 때문이다. 하지만 언제나 준비만 할 수는 없는 것 같다.\n대개 나는 두번째 방법을 선택하는데, 지난한 시간 끝에 때로 3번을 강행하는게 좋다는 결론을 내리게 되었다. 모르면 모르는대로 실험해보자. 그 과정에서 얻은 것이 배경지식보다 더 빠른 길을 제시할 수 있다.\n\n\nThe third pass\n이제 논문을 완전히 이해할 때가 됐다. 완전히 이해했다는 건 어떤 의미일까? 전 단계에서 다른 사람에게 설명할 수 있을 정도로 충분히 이해했다면, 다시말해 이해의 수준이 ’다른 사람에게 내보일만큼’에 도달했다면 마지막은 나에게 떳떳할만큼의 수준을 갖추는 것이 순서일 것이다.\n\nThe key to the third pass is to attempt to virtually re-implement the paper: that is, making the same assumptions as the authors, re-create the work.\n\n재현이다. 이 과정과 실제 논문을 비교하면 첫번째 단계에서 가늠해보았던 논문의 혁신적인 부분을 직접 체험할 수 있을 뿐 아니라 두번째 단계에서 찾아내지 못했던 실험의 미비함이나 가정의 불완전함 또한 발견할 수 있다. 그 과정에서 새로운 연구 주제를 찾을 수 있게 된다.\n이 과정은 까다롭게 진행해야 한다. 모든 가정을 파악하고, 모든 요소에 이의를 제기하며, 본인이라면 어떻게 증명했을지 생각한 후에 논문과 비교하는 과정을 거쳐야한다. 이 과정은 초심자에겐 네다섯시간, 숙련자에겐 한시간정도 소요된다.\n세번째 단계를 거쳤다면 다음을 해낼 수 있어야 한다. 거의 해당 논문을 리뷰하는 수준까지 가야 하는 것 같다.\n\nAt the end of this pass, you should be able to reconstruct the entire structure of the paper from memory, as well as be able to identify its strong and weak points.\n\n특히 논문에 직접적으로 드러나지 않은 암시적인 가정6, 인용의 누락, 분석 기법의 잠재적 문제점까지 찾아내는 것이 목표다.\n6 implicit assumptions"
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#doing-a-literature-survey",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#doing-a-literature-survey",
    "title": "How to read a paper",
    "section": "Doing A Literature survey",
    "text": "Doing A Literature survey\n실제로 논문을 찾아보는 단계다. 간단히 방법만 기록하겠다.\n\nGoogle Scholar, CiteSeer 등의 학술 검색 엔진에서 키워드를 잘 선택하여 최신 논문 세개에서 다섯개를 찾을 것\n논문을 훑어보며 내용을 파악하고 관련 내용을 검토할 것\n\n이때 서베이 논문을 찾을 수도 있는데, 그렇다면 기뻐하며 끝내자.\n\n참고문헌에서 반복되는 저자 이름을 찾을 것.\n\n이때 반복되는 논문과 주요 저자의 최근 연구 이력을 잘 살피자.\n\n영향력이 큰 컨퍼런스의 최근 발표 내용을 살펴보고, 수준 높은 연구를 익힐 것.\n3번에서 모아둔 주요 연구자료와 4번에서 찾은 연구들을 함께 살피고, 반복해서 읽는다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#experience-related-work",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#experience-related-work",
    "title": "How to read a paper",
    "section": "4~5 experience, related work",
    "text": "4~5 experience, related work\n경험에 근거하여 숲을 보기 전에 나무부터 살피고 있지 말라는 조언을 전한다.\n또한\n\n리뷰 논문을 작성하는 중이라면 아래 논문을 참고하라\n\nS. Peyton Jones, “Research Skills,”\n\nhttp://research.microsoft.com/ simonpj/Papers/giving- a-talk/giving-a-talk.html\n\n\n기술 논문을 작성하는 중이라면 아래 웹 사이트와 프로세스 개요를 탐독하라\n\nH. Schulzrinne, “Writing Technical Articles,”\n\nhttp://www.cs.columbia.edu/ hgs/etc/writing- style.html\n\nG.M. Whitesides, “Whitesides’ Group: Writing a Paper,”\n\nhttp://www.che.iitm.ac.in/misc/dd/writepaper.pdf\n\n\n이 모든 연구 기술의 스펙트럼을 다루는 웹 사이트도 하나 추천한다.\n\nACM SIGCOMM Computer Communication Review Online\n\nhttp://www.sigcomm.org/ccr/drupal/"
  },
  {
    "objectID": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html",
    "href": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html",
    "title": "finetuning tutorial",
    "section": "",
    "text": "pytorch                               2.2.1-py3.10_0\n  torchaudio                            2.2.1-py310_cpu\n  torchvision                           0.17.1-py310_cpu\n\n\n\n\n\n\n\n\n\n\n\nPenn-Fudan Database sample (image, mask)\n\n\n\n\nDataset: Penn-Fudan Database for Pedestrian Detection and Segmentation\n\nnumber\n\n(total) 170 images\n(total) 345 instances of pedestrians1\n\nusually used benchmark as segmentation\nhttps://www.cis.upenn.edu/~jshi/ped_html/\n\nModel: pretrained Mask R-CNN\n\n\n\n보행자↩︎\n\n\n\nimport torchvision\n\n# FastRCNN: https://arxiv.org/abs/1506.01497\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\n배경지식\n\n\n\n\n\n\nObject detection history\n\n\n\n\n\nSelective Search for Object Recognition\n\nSliding window2: 목표는 이미지의 전체 영역을 다양한 크기의 window로 sliding 시키고 각 window에 목표 object가 포함되는지 탐색한다. 컴퓨팅 리소스를 비효율적으로 사용한다.\nRegion Proposal3: 직관적으로 생각했을 때 전체 이미지를 확인하는건 당연히 비효율적이다. 물체가 있을만한 구역(RoI; Region of Interest)을 살피는게 중요하다.\n\nselective search: 논문에서 밝힌 바에 따르면 해당 알고리즘은 다음 목표를 가지고 설계되었다.\n\n\n\n\n\n\n\n\n\n\n\nselective search example\n\n\n\n\nCapture All Scale: 객체는 어떤 형태로든 나타날 수 있다. 따라서 위의 이미지처럼 다양한 크기의 bbox를 만들어 사용한다.\n\nhierarchical grouping algorithm; segmentation에서 일반적으로 사용하는 bottom up grouping이 비슷한 특징을 가진 것끼리 묶는 방식이라면 selective search는 이미 비슷한 특징을 가진 픽셀을 분류해두었다. 따라서 같은 객체에 해당하는 것끼리 묶기만 하면 된다.\n\nDiversification: 특정 구역을 지정하지 않는다. 구역은 인접한 곳의 색깔, 질감 등을 고려하여 설정되며 밝기까지 이 구역 생성 방법에 영향을 미친다.4\nFast to Compute: 이렇게 다양한 조건을 고려하면 컴퓨팅 리소스를 많이 사용하지 않을까? 아니다. 이 알고리즘의 개발 목표부터 효율 추구였다.5 위와 같은 bbox 생성 과정6은 컴퓨터 병목현상7 8에 걸리지 않을 것을 목표로 한다. 결국 2.1.에서 언급한 hierarchical algorithm(or hypotheses)가 제대로 작동했기에 효율적인 컴퓨팅 리소스 사용이 가능해졌다.\n\n\n\n\n\n\n\n\nConvolution에서 kernel이 특징을 추출하는 과정과 비슷해보이기도 한다. 실제로 개념은 같다.↩︎\ne.g. selective search, edge boxes↩︎\n이 다양성이 다양한 bbox를 생성하는데 영향을 미치는 것 같다.↩︎\n비효율적이라면 sliding window를 쓰면 됐다.↩︎\n‘the creation’?↩︎\nCPU 혹은 GPU 중 하나의 리소스가 부족한 경우 느려지는 현상.↩︎\n2012년에 발표된 논문에서 언급한 병목현상이니 해당 알고리즘은 CPU에서 동작할 것으로 보인다.↩︎\n\n\n\n\n\n\n\n\n1 stage detector vs 2 stage detector\n\n\n\n\n\n\nobject detection을 한번에 진행하느냐, 두단계에 나눠서 진행하느냐\n\n\n\n\n\n\n\n\n\n\n\n\nZou et al. 2019. Object Detection in 20 Years: A Survey\n\n\n\n\n1-Stage는 RoI가 아니라 Anchor box를 추출한다. 먼저 이미지를 그리드 단위로 나누고, 그리드에서 confidence matrix를 생성한다. 이때문에 이미지의 맥락적 이해도가 낮고 2 stage보다 정확도가 떨어질 수 있지만 속도가 빠르다는 장점이 있어 영상처리 등에서 사용된다.\n\ne.g. YOLO, SSD\n\n2-Stage는 고전적인 방식처럼 1. RoI를 탐지하고 2. Classification 또는 Box Regression을 진행한다. 여기서 bbox를 뽑는 과정은 box regression, classification은 이미지 자체의 답을 분류하는 일이다.\n\ne.g. R-CNN계열: Fast R-CNN, Faster R-CNN\n\n\n\n\n\n\n\n\n\n\n\nFaster R-CNN, Mask R-CNN; Image Segmentation\n\n\n\n\n\n\nMask R-CNN은 Faster R-CNN의 한 종류 중 하나다.9\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n\nwhy?\n\nFast R-CNN이 실시간 탐지가 가능한 속도를 이뤄냈으나 하드웨어의 차이가 분명히 존재한다; GPU, CPU\n따라서 알고리즘을 통해 속도 향상을 이뤄내보려고 한다.\n\nnovelty?\n\nregional proposal에서 selective search가 아닌 CNN을 사용한다. (=RPN)\nconvolution layer를 공유한다는 점10으로 인해 속도가 굉장히 빠르다.\n\nRegion Proposal Network, RPN\n\n앞서 언급한 것과 같이 CNN으로 이루어져 있다.\n하나의 CNN으로 region proposal을 제안하고, object detection도 수행한다. = detection network인 R-CNN과 연산을 공유한다.\n이 과정이 FPN의 핵심이고, Mask R-CNN도 해당 과정을 채택했다고 직접 언급했다.\n과정\n\n특성 추출; 이 과정을 어떻게 진행하느냐에 따라 방법론이 달라진다. (RPN)\n\n이미지를 사전학습 된 Conv Layer에 통과시켜 feature map을 추출한다.\nbackbone network를 얻은 feature map의 각 위치에서 sliding window를 수행한다.11\n\n[발췌] slide a small network over the convolution feature map output\n\nsmall network! takes input as an \\(n \\times n\\) spatial windows\n\n\nregion proposal 생성을 위해 k개의 anchor box를 사용한다.\n최종적으로 bbox를 결정하게 된다.\n\nRoIPool을 통해 bbox cls, bbox reg를 진행하는 과정 (Fast R-CNN identical)\n\nclassification 과정으로, k개의 box에 객체가 있는지 없는지 binary score로 결과를 준다.\nregression layer가 k개 box의 좌표를 출력한다.12\n\n\n\n\nMask R-CNN\n\nwhy?\n\ninstance segmentation: 같은 class여도 다른 객체로 인식하는 task에서 사용하기 위해 개발되었다.\n선행연구: Faster R-CNN은 pixel to pixel 대응이 되지 않는다.13\n\nnovelty?\n\nRoI Align이 추가되어 pixelwise segmentation이 가능하다.\nmask branch 로 segmentation단을 분리했다. FCN에서 진행하던 픽셀당 클래스 분류는 적합하지 않다고 한다.14\n\n“RoIAlign has a large impact: it improves mask accuracy by relative 10% to 50%, showing bigger gains under stricter localization metrics. Second, we found it essential to decouple mask and class prediction: we predict a binary mask for each class independently, without competition among classes, and rely on the network’s RoI classification branch to predict the category. In contrast, FCNs usually perform per-pixel multi-class categorization, which couples segmentation and classification, and based on our experiments works poorly for instance segmentation”\n\n\nArchitecture\n\nRPN에서 RoI를 가져오고, mask branch를 추가한 형태다.\n\nbbox reg branch와 평행으로 추가\nmask branch; 각 RoI에 작은 FCN을 추가한 형태\n\n\n\nRoI: R-CNN에서 사용하는 RoIPool 대신 RoIAlign을 사용한다.\n\nRoIPool: 좌표를 정수로 양자화 한다.15 그리드에 알맞은 위치로 RoI의 윤곽을 맞춰주는 방식이다.\n\nbounding box는 어느정도 오차가 허용되니 좌표가 소수점 단위로 이동하는건 괜찮지만 segmentation에서는 위치정보를 엄밀하게 다뤄야 한다.\n\nRoIAlign: RoIPool에서의 양자화를 제거하고 bilinear interpolation을 통해 추출된 특징에 맞게 align되게 한다. 즉, 위치정보를 픽셀단위로 보존한다.\n\nmask branch: classification을 위한 branch가 아닌 공간 정보가 보존된 RoI map이 있으므로 binary mask를 출력할 수 있다.16\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlign\nmask branch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoI Pooling & RoI Align image\n\n\n\n\n\n출처\n\nhttps://deep-learning-study.tistory.com/464\nhttps://herbwood.tistory.com/10\nhttps://ropiens.tistory.com/76\nhttps://velog.io/@skhim520/Mask-R-CNN-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0\nRoI 이미지 출처\n\n\n\n\n\n\n\nbased on top of Faster R-CNN; a model that predict both bounding boxes and class scores for potential objects in the image↩︎\nBy sharing convolutions at test-time, the marginal cost for computing proposals is small (e.g., 10ms per image).↩︎\n논문에서는 3x3 filter 사용↩︎\nbox중앙, x좌표, y좌표, height, width↩︎\nsegmentation을 수행할때 양자화 등으로 인해 공간 정보를 잃는다.↩︎\n보충 필요↩︎\nfloat -&gt; int↩︎\nsegmentation 보충↩︎\n\n\n\n\n\n\n\n\nBilinear interpolation at RoI Align example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterpolation equation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoI placement\nbilinear interpolation\nmax pooling\n\n\n\n\nRoI 이미지 출처\n\n\n\n\n\n\n라이브러리\n\nimport os\nimport torch\n\nfrom torchvision.io import read_image \nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\n\n\n데이터셋 생성\n\nimport matplotlib.pyplot as plt\nfrom torchvision.io import read_image\n\n\nimage = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\nmask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n\nplt.figure(figsize=(16, 8))\nplt.subplot(121)\nplt.title(\"Image\")\nplt.imshow(image.permute(1, 2, 0))\nplt.subplot(122)\nplt.title(\"Mask\")\nplt.imshow(mask.permute(1, 2, 0))\n\n\n\n\n\n\n\n\n\n\"\"\"데이터를 한번에 불러오기\"\"\"\nclass PennFudanDataset(torch.utils.data.Dataset):\n    # `root dir` 에서의 `root`\n    def __init__(self, root, transforms):\n        self.root = root \n        self.transforms = transforms \n\n        # load all image file\n        # ensure that they are aligned; img, mask 한꺼번에 다루기 위해 \n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n    \n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        # 위에서 import 한 torchvision의 read_image사용\n        img = read_image(img_path)      # [3, 397, 396]\n        mask = read_image(mask_path)    # [1, 397, 396]\n\n        # 고유한 값 추출\n        # torch.unique(input:Tensor) -&gt; return the unique elements of the input tensor\n        obj_ids = torch.unique(mask)    # [0, 1, 2] (shape: [3])\n        obj_ids = obj_ids [1:]          # 이때 첫값은 배경: black\n        num_objs = len(obj_ids)         # 총 채널 개수\n\n        # split the color-encoded mask into a set of binary masks\n        # mask: [2, 1, 1] 형태의 [0, 1]로 이루어진 행렬; binary mask\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # bbox generate\n        # shape: [number of people, 4]\n        boxes = masks_to_boxes(mask)    # [x1, y1, x2, y2], 사람의 수만큼 해당 행렬이 포함되어있다.\n\n        # label이 하나이므로 (1) 속하느냐 (0) 속하지 않느냐 로 구분된다.\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(\n                            boxes, \n                            format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n\n모델 선언\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO; backbone\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nbackbone\n\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\n# AnchorGenerator:\n#   that generates anchors for a set of feature maps and image sizes.\n#   The module support computing anchors at multiple sizes \n#   and aspect ratios per feature map. \nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n\n# --- load a pre-trained model for cls --- #\n# https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py\n# backbone의 결과값은 ordered dict이므로 인덱싱도 가능\nbackbone = torchvision.models.mobilenet_v3_small(weights=\"DEFAULT\").features\n# ``FasterRCNN`` needs to know the number of output channels in a backbone.\n#   \\because backbone의 output channel이 anchor box가 된다.\n# torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features 코드로 확인 가능\n# 설계상 Conv 마지막 레이어를 빠져나오면 last channel이 1280으로 된다.\nbackbone.out_channels = 1280\n\n# --- RPN anchors: 5 x 3 --- #\n# 5 different sizes and 3 different aspect(측면 비율)\n# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문\nanchor_generator = AnchorGenerator(\n    sizes=((32, 64, 128, 256, 512)),     # 5\n    aspect_ratios=((0.5, 1.0, 2.0))     # 3\n)\n\n# --- define feature map --- #\n# backbone이 Tensor를 반환할 때 'feature_names'는 [0]이 될 것으로 가정\n# 일반적으로 백본은 OrderedDict[Tensor] 이르모 이름을 정할 수 있다.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(\n    featmap_names = ['0'],  # List[str]\n    output_size=7,  # 구현된 바 7x7 이었음\n    sampling_ratio=2\n)   \n\n# --- define model --- #\nmodel = FasterRCNN(\n    backbone=backbone,\n    num_classes=2,      # binary segmentation\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=roi_pooler\n)\n\n\n\n데이터셋에 맞추어 객체 검출 (object detection, segmentation)\n\n1 stage 방법론을 채택\n\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\ndef get_model_instance_segmentation(num_classes):\n    # 사전학습된 모델을 불러올 것\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weight=\"DEFAULT\")\n\n    # '분류를 위해' 입력 특징을 받아온다.\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # 미리 학습된 헤더를 새로운 것으로 바꾸는데, 헤더란?\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # mask predictor'를 위한' 입력 특징의 '차원'을 얻는다.\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # mask predictor 변경\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                        hidden_layer,\n                                                        num_classes)\n\n    return model\n\n\n\n\n통합\n\n# #| output: false\n# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\n\n데이터 증강 및 변환\n\n# import transforms as T  # ModuleNotFoundError: No module named 'sgmllib'\nfrom torchvision.transforms import v2 as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ToDtype(torch.float, scale = True))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\n테스트 코드\n\nimport utils\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\ndataset = PennFudanDataset(root='./data/PennFudanPed/', transforms=get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=4, shuffle=True, num_workers=0,\n    collate_fn=utils.collate_fn)\n    # num_workers=0 issue &gt;&gt; RuntimeError: DataLoader worker (pid(s) 75278) exited unexpectedly\n# 학습 시\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# 추론 시\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions\n\n\n\n\n메인 학습, 검증\n\nif torch.cuda.is_available() : device = torch.device('cuda')\nelif torch.backends.mps.is_available() : device = torch.device('mps')\nelse : device=torch.device('cpu')\nprint(f'Using {device}')\n\nUsing mps\n\n\n\nfrom engine import train_one_epoch, evaluate\n\n# GPU: cuda -&gt; mps\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n# device = 'cpu'\nprint(f'Using {device}')\n\n# dataset; 2 classes; background, person\nnum_classes = 2\n# use PennFundanPed dataset, define transform\n# get_transform; 위에서 정의함 - 50% 확률로 좌우 flip\ndataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n\n# split dataset (train, test)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define dataloaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True,\n    # num_workers=4,\n    collate_fn=utils.collate_fn\n)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test,\n    batch_size=1,\n    shuffle=False,\n    # num_workers=4,\n    collate_fn=utils.collate_fn\n)\n\n# get model\nmodel = get_model_instance_segmentation(num_classes=num_classes)\n\n# move model into device; model도 옮겨야 한다.\nmodel.to(device)\n# from torch import nn; model = nn.DataParallel(model)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(\n    params,\n    lr=0.005,\n    momentum=0.9,\n    weight_decay=0.0005\n)\n\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)\n\n# let's train it just for 2 epochs\nnum_epochs = 0\n\nfor epoch in range(num_epochs):\n    \n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\nprint(\"That's it!\")\n\nUsing mps\nThat's it!\n\n\n\n\n\n\n\n\n— 이슈 기록 —\n\n\n\n\n\nLoss is nan, stopping training\n\n확인 결과: OOM - 메모리에 데이터를 올려두는 과정에서 배치를 1로 정하면 문제가 생기지 않는데, 2부터는 문제가 발생함.\n\n# mps 사용시 발생하는 이슈;\n#   mps nightly 배포 당시 사용했을 때 문제없이 딥러닝이 돌아갔었는데 갑자기 안 됨.\n#   v2로 올라가서 생기는 문제일지도 모르겠지만 tv_tensor를 사용해야 해서\n#   버전 다운그레이드는 불가능한 상황\n\nError: command buffer exited with error status.\n    The Metal Performance Shaders operations encoded on it may not have completed.\n    Error: \n    (null)\n    Internal Error (0000000e:Internal Error)\n    &lt;AGXG13XFamilyCommandBuffer: 0x6a0739b40&gt;\n    label = &lt;none&gt; \n    device = &lt;AGXG13XDevice: 0x12e53f000&gt;\n        name = Apple M1 Pro \n    commandQueue = &lt;AGXG13XFamilyCommandQueue: 0x12f22ba00&gt;\n        label = &lt;none&gt; \n        device = &lt;AGXG13XDevice: 0x12e53f000&gt;\n            name = Apple M1 Pro \n    retainedReferences = 1\n\nsource code\n\nofficial pytorch github implementing list\nmps implemented test code\n\npytorch github issue\n\n#119968\n#111173\nSolved!\n\napple forum\n\n#693849\n#711854"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html",
    "title": "DCGAN face tutorial",
    "section": "",
    "text": "학습 데이터들의 분포를 학습해\n새로운 데이터를 생성\n\n\n생성자와 구분자로 구별되는 두 모델을 가지고 있다.\n\n생성자: 실제 이미지와 유사한 정교한 이미지 생성\n구분자: 실제 이미지인지 생성자의 이미지인지 판별\n\n생성자와 구분자의 상호작용을 통해 실제와 같은 이미지를 만들어낸다.\n구분자가 생성자의 이미지 중 50%를 제대로 판별할 때 균형상태에 도달했다고 말한다.\n\n\nDGGAN: GAN에서 파생된 모델로 생성자와 구분자에서 ’합성곱 신경망(Convolution)’과 ’전치 합성곱 신경망(Convolution-transpose)’를 사용했다\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn \nimport torch.nn.parallel\nimport torch.backends.mps as mps \n\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# seed\nmanualSeed = 42\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\nnp.random.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nRandom Seed:  42\n\n\n&lt;torch._C.Generator at 0x11820d070&gt;\n\n\n\n\n\n# dataset dir root path\ndataroot = \"./data/celeba\" \n\n# dataloader에서 사용할 thread 수\nworkers = 2\n\nbatch_size = 256\n\n# 이미지의 크기; 모든 이미지 변환하여 크기 통일함\nimage_size = 64\n\n# number channel; RGB이미지이므로 3\nnc = 3\n\n# latent vector size = 생성자의 입력값 크기\nnz = 100\n\n# 생성자를 통과하는 특징 데이터의 채널 크기\nngf = 64\n\n# 구분자를 통과하는 특징 데이터의 채널 크기\nndf = 64\n\nnum_epochs = 1\n\nlr = 0.0002\n\n# adam optimizer의 beta1 하이퍼파라미터\nbeta1 = 0.5\n\n# 사용가능한 gpu 번호; cpu = 0\nngpu = 0\n\n\n\n\n\ndataset = dset.ImageFolder(root=dataroot,\n                            transform=transforms.Compose([\n                                transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                            ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# 데이터가 너무 커서 샘플로 몇개만 뽑음\nsample_size = batch_size*1000\ndataset, _dataset = torch.utils.data.random_split(dataset, lengths=[sample_size, len(dataset)-sample_size])\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# device\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n\n# real batch check\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Image\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균 0, 분산 0.02인 정규분포\n\nmean = 0 stdev=0.02\n구분자와 생성자 모두 무작위 초기화 진행\n\n\n\n# 매개변수로 '모델'을 받아 합성곱, 전치합성곱, 배치정규화 계층을 초기화\n\ndef weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('ConV') != -1:\n        nn.init.normal_(model.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(model.weight.data, 1.0, 0.02)\n        nn.init.constant_(model.bias.data, 0)\n\n\n\n\n\n잠재공간 \\(z\\)를 데이터공간, 즉 이미지로 변환하는 과정을 거친다 (\\(3 \\times 64 \\times 64\\))\nstride 2를 가진 전치 합성곱 계층을 이어서 구성\n\n각 전치 합성곱 계층 하나당 아래 두개 레이어를 쌍으로 묶어 사용함\n\n2차 배치 정규화 계층\nrelu 활성화 함수\n\n\n마지막 출력 계층에서는 tanh 함수를 사용하는데 이는 출력값을 \\([-1, 1]\\) 사이로 조정하기 위함 이다.\n배치 정규화 계층이 중요한데, 이 계층이 경사 하강법의 흐름에 중요한 역할을 했다고 함 (논문 원문 참조)\n\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # ngf: 생성자를 통과하는 특징 데이터의 채널 크기\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0 ),     # 왜 ngf를 8배 하나?\n            nn.BatchNorm2d( ngf * 8 ),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d( ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)\n\n\nnetG = Generator(ngpu).to(device)\n\n# 모든 가중치의 평균을 0, 분산을 0.02로 초기화\nnetG.apply(weights_init)\n\n# 모델 구조 확인\nnetG\n\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n\n\n\n\n\n\n입력: \\(3 \\times 64 \\times 64\\)\nConv2d, BatchNorm2d, LeakyReLU로 데이터를 가공\n\nstride를 합성곱 계층에서 적용하는데 논문에서는 스스로 pooling을 학습하기 때문에 pooling 계층을 넣지 않아도 효과가 좋았다고 한다.\n\n마지막 계층 Sigmoid로 확률값 반환\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 입력 데이터의 크기는 ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n\nnetD = Discriminator(ngpu).to(device)\nnetD.apply(weights_init)\nnetD\n\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n\n\n\n\n\n손실함수로 BCELoss1 사용\n\\(\\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\\)\n\n여기서 조정해야하는 \\(log(D(x))\\) 와 \\(log(1 - D(G(Z)))\\) 를 확인할 수 있다.\n\n\n\n목표: \\(y\\)를 이용하여 손실함수를 최대화하는 방법 찾기\n\n\ncriterion = nn.BCELoss()\n\n# 입력, 잠재공간 벡터 생성\n# device = device 아니고 device만 넣으면 에러\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# 참 거짓의 라벨\nreal_label = 1\nfake_label = 0 \n\n# optimizer 생성\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\n\n\n\n\n손실함수를 어떻게 최대화 할 것인가?\n\n\n실제 데이터만 가져와 미니배치를 만든 후 훈련한다: \\(D\\) (학습)\n\n1의 결과로 얻은 \\(log(D(x))\\) 의 손실값을 계산한다. (수식 계산)\n역전파 과정에서의 변화도를 계산한다.\n\n\n이들 변화값은 축적(accumulate)시켜야 한다.\n이후 최적화 함수를 사용한다.\n\n\\(log(D(G(z)))\\) 를 최대화 하는 방식으로 학습2\n\n진짜 이미지들에서 G의 손실값을 구하고 가짜이미지에서도 같은 방식을 적용한다.\n이 과정에서 BCELoss의 일부인 \\(log(x)\\)를 일부 사용할 수 있다.\n\n\n\n실제값과 거짓값의 학습에서 나타나는 차이점을 통해 손실함수를 분리한다.\n\n튜토리얼에서는 다음과 같이 사용한다.\n\nLoss_D - 진짜 데이터와 가짜 데이터들 모두에서 구해진 손실값. (\\(log(D(x)) + log(1 - D(G(z)))\\)).\nLoss_G - 생성자의 손실값. \\(log(D(G(z)))\\)\nD(x) - 구분자가 데이터를 판별한 확률값입니다. 처음에는 1에 가까운 값이다가, G가 학습할수록 0.5값에 수렴하게 됩니다.\nD(G(z)) - 가짜데이터들에 대한 구분자의 출력값입니다. 처음에는 0에 가까운 값이다가, G가 학습할수록 0.5에 수렴하게 됩니다\n\n\n# 손실값들을 저장\n\nimg_list, G_losses, D_losses = [], [], []\niters = 0\n\n# `배치` 반복\ndef batch_loop(dataloader, netD, netG, criterion, epoch):\n    for i, data in enumerate(dataloader, 0):\n        ##################################\n        # (1) D 신경망 업데이트\n        # 실제 데이터로 학습\n        # 전체 손실함수 값을 최대화\n        ##################################\n\n        # --- 실제 데이터 학습 --- #\n        netD.zero_grad()\n\n        # 배치의 사이즈나 사용할 디바이스에 맞게 조정하는 이유? \n        # &gt;&gt; 효율성?\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label,\n                            dtype=torch.float, device=device)\n        \n        # 실제 데이터 투입\n        output = netD(real_cpu).view(-1)\n        # 손실값\n        errD_real = criterion(output, label)\n        # print(\"errD_real: \", errD_real)\n        # 역전파 하며 변화도 계산\n        errD_real.backward()\n        D_x = output.mean().item()\n        # print(\"D_x: \", D_x)\n        \n\n        # --- 가짜 데이터 학습 --- #\n        # 생성자에 사용할 잠재공간 벡터\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # 가짜 이미지 생성\n        fake = netG(noise)\n        # fill_ ?\n        label.fill_(fake_label)\n        \n        # D를 이용해 데이터 진위여부 판별\n        output = netD(fake.detach()).view(-1)\n        # D의 손실값 계산\n        errD_fake = criterion(output, label)\n        # 가짜 이미지의 변화도를 계산한 후 변화도에 더한다 (accumulate)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # 손실값들을 모두 더한다.\n        # errD는 학습 상태를 리포팅 할 때 사용한다! &gt; 임의로 생성한 결과니\n        errD = errD_real + errD_fake \n\n        # D업데이트\n        optimizerD.step()\n        # print(\"errD: \", errD)\n\n\n        ##################################\n        # (1) G 신경망 업데이트\n        # 실제 데이터를 생성\n        # log(D(G(z))) 값을 최대화\n        ##################################\n        netG.zero_grad()\n        label.fill_(real_label)     # 생성자의 손실값을 위해\n        # D를 제대로 업데이트했으므로 다시 가짜데이터 투입\n        output = netD(fake).view(-1)\n        # G의 손실값 계산\n        errG = criterion(output, label)\n        # G의 변화도 계산\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # G 업데이트\n        optimizerG.step()\n\n        # ---------------------------------\n        # 훈련 상태를 출력합니다\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                    % (epoch, num_epochs, i, len(dataloader),\n                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # 이후 그래프를 그리기 위해 손실값들을 저장해둡니다\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # fixed_noise를 통과시킨 G의 출력값을 저장해둡니다\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n\n# print(\"Starting Training Loop...\")\n# for epoch in range(num_epochs):\n#     batch_loop(dataloader, netD, netG, criterion, epoch)\n\n#     iters += 1\n\n'''\nStarting Training Loop...\n[0/1][0/792]    Loss_D: 1.3083  Loss_G: 3.2819  D(x): 0.5943    D(G(z)): 0.5336 / 0.0397\n[0/1][50/792]   Loss_D: 0.2891  Loss_G: 15.2257 D(x): 0.8146    D(G(z)): 0.0000 / 0.0000\n[0/1][100/792]  Loss_D: 0.2566  Loss_G: 4.2529  D(x): 0.8360    D(G(z)): 0.0341 / 0.0382\n[0/1][150/792]  Loss_D: 1.1949  Loss_G: 1.6857  D(x): 0.3761    D(G(z)): 0.0447 / 0.2113\n[0/1][200/792]  Loss_D: 0.9831  Loss_G: 2.3191  D(x): 0.4773    D(G(z)): 0.0473 / 0.1316\n[0/1][250/792]  Loss_D: 0.9653  Loss_G: 4.5867  D(x): 0.8587    D(G(z)): 0.5122 / 0.0142\n[0/1][300/792]  Loss_D: 0.7742  Loss_G: 2.0908  D(x): 0.5511    D(G(z)): 0.0705 / 0.1749\n[0/1][350/792]  Loss_D: 0.7144  Loss_G: 5.1598  D(x): 0.8541    D(G(z)): 0.3957 / 0.0082\n[0/1][400/792]  Loss_D: 1.8883  Loss_G: 1.6982  D(x): 0.2203    D(G(z)): 0.0174 / 0.2421\n[0/1][450/792]  Loss_D: 0.7316  Loss_G: 5.0640  D(x): 0.8330    D(G(z)): 0.3798 / 0.0097\n[0/1][500/792]  Loss_D: 0.4904  Loss_G: 3.8918  D(x): 0.8738    D(G(z)): 0.2750 / 0.0271\n[0/1][550/792]  Loss_D: 0.3976  Loss_G: 3.4012  D(x): 0.7631    D(G(z)): 0.0879 / 0.0493\n[0/1][600/792]  Loss_D: 1.3403  Loss_G: 7.2067  D(x): 0.9398    D(G(z)): 0.6828 / 0.0012\n[0/1][650/792]  Loss_D: 0.5217  Loss_G: 3.1440  D(x): 0.8256    D(G(z)): 0.2559 / 0.0536\n[0/1][700/792]  Loss_D: 0.3997  Loss_G: 3.3902  D(x): 0.8364    D(G(z)): 0.1774 / 0.0445\n[0/1][750/792]  Loss_D: 0.4414  Loss_G: 3.4481  D(x): 0.8536    D(G(z)): 0.2275 / 0.0386\n'''\n\n\n\n\n\n\n# dataloader에서 진짜 데이터들을 가져옵니다\nreal_batch = next(iter(dataloader))\n\n# 진짜 이미지들을 화면에 출력합니다\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# 가짜 이미지들을 화면에 출력합니다\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#gan",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#gan",
    "title": "DCGAN face tutorial",
    "section": "",
    "text": "학습 데이터들의 분포를 학습해\n새로운 데이터를 생성\n\n\n생성자와 구분자로 구별되는 두 모델을 가지고 있다.\n\n생성자: 실제 이미지와 유사한 정교한 이미지 생성\n구분자: 실제 이미지인지 생성자의 이미지인지 판별\n\n생성자와 구분자의 상호작용을 통해 실제와 같은 이미지를 만들어낸다.\n구분자가 생성자의 이미지 중 50%를 제대로 판별할 때 균형상태에 도달했다고 말한다.\n\n\nDGGAN: GAN에서 파생된 모델로 생성자와 구분자에서 ’합성곱 신경망(Convolution)’과 ’전치 합성곱 신경망(Convolution-transpose)’를 사용했다\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn \nimport torch.nn.parallel\nimport torch.backends.mps as mps \n\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# seed\nmanualSeed = 42\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\nnp.random.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nRandom Seed:  42\n\n\n&lt;torch._C.Generator at 0x11820d070&gt;\n\n\n\n\n\n# dataset dir root path\ndataroot = \"./data/celeba\" \n\n# dataloader에서 사용할 thread 수\nworkers = 2\n\nbatch_size = 256\n\n# 이미지의 크기; 모든 이미지 변환하여 크기 통일함\nimage_size = 64\n\n# number channel; RGB이미지이므로 3\nnc = 3\n\n# latent vector size = 생성자의 입력값 크기\nnz = 100\n\n# 생성자를 통과하는 특징 데이터의 채널 크기\nngf = 64\n\n# 구분자를 통과하는 특징 데이터의 채널 크기\nndf = 64\n\nnum_epochs = 1\n\nlr = 0.0002\n\n# adam optimizer의 beta1 하이퍼파라미터\nbeta1 = 0.5\n\n# 사용가능한 gpu 번호; cpu = 0\nngpu = 0\n\n\n\n\n\ndataset = dset.ImageFolder(root=dataroot,\n                            transform=transforms.Compose([\n                                transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                            ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# 데이터가 너무 커서 샘플로 몇개만 뽑음\nsample_size = batch_size*1000\ndataset, _dataset = torch.utils.data.random_split(dataset, lengths=[sample_size, len(dataset)-sample_size])\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# device\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n\n# real batch check\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Image\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균 0, 분산 0.02인 정규분포\n\nmean = 0 stdev=0.02\n구분자와 생성자 모두 무작위 초기화 진행\n\n\n\n# 매개변수로 '모델'을 받아 합성곱, 전치합성곱, 배치정규화 계층을 초기화\n\ndef weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('ConV') != -1:\n        nn.init.normal_(model.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(model.weight.data, 1.0, 0.02)\n        nn.init.constant_(model.bias.data, 0)\n\n\n\n\n\n잠재공간 \\(z\\)를 데이터공간, 즉 이미지로 변환하는 과정을 거친다 (\\(3 \\times 64 \\times 64\\))\nstride 2를 가진 전치 합성곱 계층을 이어서 구성\n\n각 전치 합성곱 계층 하나당 아래 두개 레이어를 쌍으로 묶어 사용함\n\n2차 배치 정규화 계층\nrelu 활성화 함수\n\n\n마지막 출력 계층에서는 tanh 함수를 사용하는데 이는 출력값을 \\([-1, 1]\\) 사이로 조정하기 위함 이다.\n배치 정규화 계층이 중요한데, 이 계층이 경사 하강법의 흐름에 중요한 역할을 했다고 함 (논문 원문 참조)\n\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # ngf: 생성자를 통과하는 특징 데이터의 채널 크기\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0 ),     # 왜 ngf를 8배 하나?\n            nn.BatchNorm2d( ngf * 8 ),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d( ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)\n\n\nnetG = Generator(ngpu).to(device)\n\n# 모든 가중치의 평균을 0, 분산을 0.02로 초기화\nnetG.apply(weights_init)\n\n# 모델 구조 확인\nnetG\n\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n\n\n\n\n\n\n입력: \\(3 \\times 64 \\times 64\\)\nConv2d, BatchNorm2d, LeakyReLU로 데이터를 가공\n\nstride를 합성곱 계층에서 적용하는데 논문에서는 스스로 pooling을 학습하기 때문에 pooling 계층을 넣지 않아도 효과가 좋았다고 한다.\n\n마지막 계층 Sigmoid로 확률값 반환\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 입력 데이터의 크기는 ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n\nnetD = Discriminator(ngpu).to(device)\nnetD.apply(weights_init)\nnetD\n\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n\n\n\n\n\n손실함수로 BCELoss1 사용\n\\(\\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\\)\n\n여기서 조정해야하는 \\(log(D(x))\\) 와 \\(log(1 - D(G(Z)))\\) 를 확인할 수 있다.\n\n\n\n목표: \\(y\\)를 이용하여 손실함수를 최대화하는 방법 찾기\n\n\ncriterion = nn.BCELoss()\n\n# 입력, 잠재공간 벡터 생성\n# device = device 아니고 device만 넣으면 에러\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# 참 거짓의 라벨\nreal_label = 1\nfake_label = 0 \n\n# optimizer 생성\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\n\n\n\n\n손실함수를 어떻게 최대화 할 것인가?\n\n\n실제 데이터만 가져와 미니배치를 만든 후 훈련한다: \\(D\\) (학습)\n\n1의 결과로 얻은 \\(log(D(x))\\) 의 손실값을 계산한다. (수식 계산)\n역전파 과정에서의 변화도를 계산한다.\n\n\n이들 변화값은 축적(accumulate)시켜야 한다.\n이후 최적화 함수를 사용한다.\n\n\\(log(D(G(z)))\\) 를 최대화 하는 방식으로 학습2\n\n진짜 이미지들에서 G의 손실값을 구하고 가짜이미지에서도 같은 방식을 적용한다.\n이 과정에서 BCELoss의 일부인 \\(log(x)\\)를 일부 사용할 수 있다.\n\n\n\n실제값과 거짓값의 학습에서 나타나는 차이점을 통해 손실함수를 분리한다.\n\n튜토리얼에서는 다음과 같이 사용한다.\n\nLoss_D - 진짜 데이터와 가짜 데이터들 모두에서 구해진 손실값. (\\(log(D(x)) + log(1 - D(G(z)))\\)).\nLoss_G - 생성자의 손실값. \\(log(D(G(z)))\\)\nD(x) - 구분자가 데이터를 판별한 확률값입니다. 처음에는 1에 가까운 값이다가, G가 학습할수록 0.5값에 수렴하게 됩니다.\nD(G(z)) - 가짜데이터들에 대한 구분자의 출력값입니다. 처음에는 0에 가까운 값이다가, G가 학습할수록 0.5에 수렴하게 됩니다\n\n\n# 손실값들을 저장\n\nimg_list, G_losses, D_losses = [], [], []\niters = 0\n\n# `배치` 반복\ndef batch_loop(dataloader, netD, netG, criterion, epoch):\n    for i, data in enumerate(dataloader, 0):\n        ##################################\n        # (1) D 신경망 업데이트\n        # 실제 데이터로 학습\n        # 전체 손실함수 값을 최대화\n        ##################################\n\n        # --- 실제 데이터 학습 --- #\n        netD.zero_grad()\n\n        # 배치의 사이즈나 사용할 디바이스에 맞게 조정하는 이유? \n        # &gt;&gt; 효율성?\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label,\n                            dtype=torch.float, device=device)\n        \n        # 실제 데이터 투입\n        output = netD(real_cpu).view(-1)\n        # 손실값\n        errD_real = criterion(output, label)\n        # print(\"errD_real: \", errD_real)\n        # 역전파 하며 변화도 계산\n        errD_real.backward()\n        D_x = output.mean().item()\n        # print(\"D_x: \", D_x)\n        \n\n        # --- 가짜 데이터 학습 --- #\n        # 생성자에 사용할 잠재공간 벡터\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # 가짜 이미지 생성\n        fake = netG(noise)\n        # fill_ ?\n        label.fill_(fake_label)\n        \n        # D를 이용해 데이터 진위여부 판별\n        output = netD(fake.detach()).view(-1)\n        # D의 손실값 계산\n        errD_fake = criterion(output, label)\n        # 가짜 이미지의 변화도를 계산한 후 변화도에 더한다 (accumulate)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # 손실값들을 모두 더한다.\n        # errD는 학습 상태를 리포팅 할 때 사용한다! &gt; 임의로 생성한 결과니\n        errD = errD_real + errD_fake \n\n        # D업데이트\n        optimizerD.step()\n        # print(\"errD: \", errD)\n\n\n        ##################################\n        # (1) G 신경망 업데이트\n        # 실제 데이터를 생성\n        # log(D(G(z))) 값을 최대화\n        ##################################\n        netG.zero_grad()\n        label.fill_(real_label)     # 생성자의 손실값을 위해\n        # D를 제대로 업데이트했으므로 다시 가짜데이터 투입\n        output = netD(fake).view(-1)\n        # G의 손실값 계산\n        errG = criterion(output, label)\n        # G의 변화도 계산\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # G 업데이트\n        optimizerG.step()\n\n        # ---------------------------------\n        # 훈련 상태를 출력합니다\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                    % (epoch, num_epochs, i, len(dataloader),\n                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # 이후 그래프를 그리기 위해 손실값들을 저장해둡니다\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # fixed_noise를 통과시킨 G의 출력값을 저장해둡니다\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n\n# print(\"Starting Training Loop...\")\n# for epoch in range(num_epochs):\n#     batch_loop(dataloader, netD, netG, criterion, epoch)\n\n#     iters += 1\n\n'''\nStarting Training Loop...\n[0/1][0/792]    Loss_D: 1.3083  Loss_G: 3.2819  D(x): 0.5943    D(G(z)): 0.5336 / 0.0397\n[0/1][50/792]   Loss_D: 0.2891  Loss_G: 15.2257 D(x): 0.8146    D(G(z)): 0.0000 / 0.0000\n[0/1][100/792]  Loss_D: 0.2566  Loss_G: 4.2529  D(x): 0.8360    D(G(z)): 0.0341 / 0.0382\n[0/1][150/792]  Loss_D: 1.1949  Loss_G: 1.6857  D(x): 0.3761    D(G(z)): 0.0447 / 0.2113\n[0/1][200/792]  Loss_D: 0.9831  Loss_G: 2.3191  D(x): 0.4773    D(G(z)): 0.0473 / 0.1316\n[0/1][250/792]  Loss_D: 0.9653  Loss_G: 4.5867  D(x): 0.8587    D(G(z)): 0.5122 / 0.0142\n[0/1][300/792]  Loss_D: 0.7742  Loss_G: 2.0908  D(x): 0.5511    D(G(z)): 0.0705 / 0.1749\n[0/1][350/792]  Loss_D: 0.7144  Loss_G: 5.1598  D(x): 0.8541    D(G(z)): 0.3957 / 0.0082\n[0/1][400/792]  Loss_D: 1.8883  Loss_G: 1.6982  D(x): 0.2203    D(G(z)): 0.0174 / 0.2421\n[0/1][450/792]  Loss_D: 0.7316  Loss_G: 5.0640  D(x): 0.8330    D(G(z)): 0.3798 / 0.0097\n[0/1][500/792]  Loss_D: 0.4904  Loss_G: 3.8918  D(x): 0.8738    D(G(z)): 0.2750 / 0.0271\n[0/1][550/792]  Loss_D: 0.3976  Loss_G: 3.4012  D(x): 0.7631    D(G(z)): 0.0879 / 0.0493\n[0/1][600/792]  Loss_D: 1.3403  Loss_G: 7.2067  D(x): 0.9398    D(G(z)): 0.6828 / 0.0012\n[0/1][650/792]  Loss_D: 0.5217  Loss_G: 3.1440  D(x): 0.8256    D(G(z)): 0.2559 / 0.0536\n[0/1][700/792]  Loss_D: 0.3997  Loss_G: 3.3902  D(x): 0.8364    D(G(z)): 0.1774 / 0.0445\n[0/1][750/792]  Loss_D: 0.4414  Loss_G: 3.4481  D(x): 0.8536    D(G(z)): 0.2275 / 0.0386\n'''\n\n\n\n\n\n\n# dataloader에서 진짜 데이터들을 가져옵니다\nreal_batch = next(iter(dataloader))\n\n# 진짜 이미지들을 화면에 출력합니다\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# 가짜 이미지들을 화면에 출력합니다\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#footnotes",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#footnotes",
    "title": "DCGAN face tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBinary Cross Entropy loss↩︎\n\\(log(1 - D(G(z)))\\) 최소화는 원문 논문에서 효율적이지 못하다고 언급함↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/transfer_learning/transfer_learning_tutorial.html",
    "href": "posts/LabHAI/tutorial/transfer_learning/transfer_learning_tutorial.html",
    "title": "transfer learning tutorial",
    "section": "",
    "text": "from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n# import torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\n# cudnn.benchmark = True\nplt.ion()   # 대화형 모드\n\n&lt;contextlib.ExitStack at 0x10465a1a0&gt;\n\n\n\n데이터 불러오기\n\n# augmentation\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),   # 왜 224?\n        transforms.RandomHorizontalFlip(),   # default=0.5\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]\n        )\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n\ndata_dir = \"data/hymenoptera_data\"\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                    data_transforms[x])\n                    for x in ['train', 'val']}\n\nbatch_size = 8\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                                shuffle=True, num_workers=0)\n                                                for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes \n\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n이미지 시각화해서 확인\n\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n\n# 학습 데이터의 배치를 얻고, 매번 다른 배치를 보여준다.\ninputs, classes = next(iter(dataloaders['train']))\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n\n\n\n\n\n\n\n\n\n학습\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=0):\n    since = time.time()\n\n    # 여기서 최고 성능의 모델을 판별하므로 함수 안에서 반복을 해야한다.\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    # 학습\n    for epoch in range(num_epochs):\n        if epoch % 10 == 0:\n            print('-' * 10)\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n        for phase in ['train', 'val']:  # epoch당 두번 돈다.\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()    # 평가모드로 전환\n            \n            # 먼저 선언\n            running_loss = 0.0\n            running_corrects = 0\n\n            # 학습/검증 데이터를 가져온다.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # 최적화를 위해 경사도를 0으로 선언\n                optimizer.zero_grad()\n\n                # forward\n                # 학습때만 연산\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # 학습 단계인 경우 역전파 + 최적화\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                # 연산 결과\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            # 이건 또 뭐야\n            if phase == 'train':\n                scheduler.step()\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            # 원래 .double() 로 데이터형태를 맞춰줬었다.\n            # TypeError: Cannot convert a MPS Tensor to float64 dtype \n            # as the MPS framework doesn't support float64. \n            # Please use float32 instead.\n            # default 값이 float() 다. \n            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n\n            if epoch % 10 == 0:\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # 모델을 깊은 복사(deep copy)함\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n\n    time_elapsed = time.time() - since\n    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # 가장 나은 모델 가중치를 불러옴\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n\n시각화\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # 결과값과 예측값\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)\n\n\n\n미세조정\n마지막 계층을 초기화하는 단계다.\n\n# 사전학습모델 resnet\n# FineTuning의 약자로 ft를 사용하는 것 같음\nmodel_ft = models.resnet18(weights='IMAGENET1K_V1')\n# 사전학습모델로부터 fc layer 입력 채널 수를 얻음\nnum_ftrs = model_ft.fc.in_features  \n\n# 사전학습의 FC layer를 nn.Linear~ 로 교체하는 작업\n# 출력 샘플의 크기: ``nn.Linear(num_ftrs, len (class_names))`` 로 일반화\n# Linear(in_features=512, out_features=8, bias=True)\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))\n\nmodel_ft = model_ft.to(device)\n\n# (역전파) 손실함수 지정\ncriterion = nn.CrossEntropyLoss()\n\n# (최적화) 모든 매개변수들이 최적화되었는지 관찰\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# 7 에폭마다 0.1씩 학습률 감소\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n\nmodel_ft.fc\n\nLinear(in_features=512, out_features=2, bias=True)\n\n\n\n# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n#                         num_epochs=25)\n\n----------\nEpoch 0/24\n----------\ntrain Loss: 0.5382 Acc: 0.7172\nval Loss: 0.2261 Acc: 0.9216\n----------\nEpoch 10/24\n----------\ntrain Loss: 0.1790 Acc: 0.9303\nval Loss: 0.1501 Acc: 0.9608\n----------\nEpoch 20/24\n----------\ntrain Loss: 0.1212 Acc: 0.9467\nval Loss: 0.1478 Acc: 0.9477\n\nTraining complete in 1m 36s\nBest val Acc: 0.960784\n\n\n\nvisualize_model(model_ft)"
  },
  {
    "objectID": "posts/LabHAI/papers/1505.04597.html",
    "href": "posts/LabHAI/papers/1505.04597.html",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "목표\nSwin U-Net을 이해하기 위한 기반 지식을 쌓는다."
  },
  {
    "objectID": "posts/LabHAI/papers/1505.04597.html#abstract",
    "href": "posts/LabHAI/papers/1505.04597.html#abstract",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Abstract",
    "text": "Abstract\n제안할 내용: New network, Training strategy, strong Augmentation\n\nNew Network; Architecture that enables precise localization1\n\nsymmetric expanding path\n\n특징 추출을 위한 contracting path\n위치 파악을 위한 expanding path\n\nend to end\nspeed\n\n성능평가\n\nsliding-window convolution network2 능가\nISBI3 에서 2015년 우승\n\n\n1 segmentation에서 중요2 아마도 당시 SOTA 방법론3 segmentation of neuronal structures in electron microscopic stacks\n\n\n\n\n\nimage segmentation의 벤치마크\n\n\n\n\n\nKvasir-SEG (dataset)\n\nMediaEval에서 공개된 데이터셋으로 위장관4 내시경5 이미지를 포함하고 있다.\n8 classes, 1000 images per class, total 8000\n\n검증된 의사들에 의해 annotate 되었다.\n기준:\n\n해부학적 지점 3개: Z line, pylorus 유문, cecum 맹장\n병리학적 소견 3개: esophagitis 식도염, polyps 용종, ulcerative colitis 궤양성 대장염\n용종 제거 과정 2개\n\n\n\n그 외\n\n\n\n\n\n\n\n\nKvasir-SEG\nCVC-ClinicDB\nACDC\n\n\n\n\n\n\n\n\n\n\n\n대체로 U-Net 또는 U-Net based model이 벤치마크의 시작점에 있는 것을 확인할 수 있다.\n많아봐야 40개 내외의 모델이 테스트 되었다.\n\n왜 벤치마크를 사용하는 모델의 수가 적은 편일까?\n\n일단 개인정보라는 점이 주된 문제로 보인다.\n벤치마크가 아니라 각자가 선택한 데이터셋을 사용하는 이유는 대부분 해당 모델이 해결하려는 문제가 domain specific한 문제이기 때문 아닐까?\n도메인 특성상 벤치마크 성능평가가 의료영상에서는 중요하지 않은 것 같다.\n\n\n\n\n5 endoscopy4 GI: Gastrointestinal의 약어, 위장관은 GI tract라고 한다.\n\n\n\n\n\nend-to-end 로 학습한다는 것의 정확한 의미와 효과\n\n\n\n\n\n\n특징 추출 없이 원본 데이터를 그대로 사용하는 모델을 E2E 모델이라고 한다.\n\n\n\n\n\n\n\n이미지 분류의 예\n\n\n\n\n\n\n\n\n\n\n\n원래 인공지능이 이런게 아니었나? 생각해보면 아니다. 인공지능을 처음 배울때 하는 feature engineering 방법이 전통적 방식의 인공지능이다. 나는 머신러닝이라고 분류해서 부르고 있었던 방식이 알고보니 전통적 방식이었고, 지금의 인공지능 학습 방식은 대부분 E2E 방식이라고 할 수 있겠다.6 방대한 양의 데이터를 다루게 되면서 데이터 자체의 특수함까지 학습할 수 있게 되었다.\n\n효율: 기존의 방식은 인간이 개입해야 하는 부분이 많았고 도메인 지식이 많은 영향을 미쳤는데 E2E는 그 과정을 건너뛰어도 된다.\n한계: 일반적인 딥러닝의 한계다. 거대한 데이터셋이 필요하며 내부에서 일어나는 일을 일반적으로는 설명할 수 없다.\n\n출처: https://www.baeldung.com/cs/end-to-end-deep-learning\n\n\n\n6 출처에서도 그렇게 부르는 것 같은데 일반적인 명명 방식인지는 모르겠다."
  },
  {
    "objectID": "posts/LabHAI/papers/1505.04597.html#선행연구",
    "href": "posts/LabHAI/papers/1505.04597.html#선행연구",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "선행연구",
    "text": "선행연구\n\n선행연구 1에서는 레이어를 늘리고 데이터를 불려 심층적인 네트워크를 구축하는 것이었다.\n\nsingle class label을 결과값으로 하는 분류 모델이라는 한계점이 있다.\n그러나 의료영상 분야에서는 지역화7가 필요하고 데이터셋을 수천개씩 구축할 수 없다.\n\n선행연구 2는 각 픽셀의 클래스를 예측하는 방식으로 지역화에 성공했다. 패치를 기준으로 학습하기 때문에 적은양의 데이터로도 학습할 수 있고, 본 연구와 같은 challenge인 ISBI 2012에서 우승했다.\n\n지역화 정확도가 높지 않다.\n패치가 클수록 더 큰 Max Pooling layer가 필요하므로8 정확도가 낮아진다.\n\n선행 연구 3은 FCN으로, pooling layer를 upsampling layers 로 대체하는 방식으로, 출력의 해상도9를 높일 수 있다.\n즉, 레이어와 데이터를 무작정 키우는 것은 의료 영상 분야에서 방법이 되지 못하고, 각 픽셀을 분류함으로써 localization을 할 수 있으니 pooling layer를 upsampling layer로 대체하여 정확도와 출력 해상도를 높이는 방법이 현재 의료영상 분야에서 시행 또는 연구되고 있다.\n\n7 localization8 레이어를 줄일수록 feature map은 줄어드는데 크기는 지켜야 하므로9 왜?"
  },
  {
    "objectID": "posts/LabHAI/papers/1505.04597.html#u-net",
    "href": "posts/LabHAI/papers/1505.04597.html#u-net",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "U-Net",
    "text": "U-Net\n\n\n\n\n\n\n\n\n\n\n\nU-Net\n\n\n\n\nOne important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers\n‘Upsampling’ 과정에서 더 많은 feature channels 부분을 추가하는게 주된 아이디어다. 그 결과: Fully Connected Layer를 빼고 지역화에 필요한 기능을 수행하는 레이어만 사용하게 된다. 픽셀 예측을 위해 대칭된 위치에서10 입력 이미지를 참조한다.11\n구조상 출력 해상도가 더 낮으므로 목표 이미지의 크기보다 더 큰 이미지를 입력한다. (extrapolation)\n손실 함수는 cross-entropy 를 사용하는데, 거기에 가중치 함수를 곱하는 형태다.\n\\[E = \\sum_{x \\in \\Omega} w(x) \\log(p_{l(x)}(x))\\]\n\n왜? 세포를 대상으로 하는 모델이었으므로 세포를 명확히 구분하는 것이 중요하다. 따라서 작은 분리 경계 (small separation border)를 학습해야 하다. 이 작업을 \\(w(x)\\)가 배경 레이블에 높은 가중치를 부여함으로써 진행한다.\n\n\n\nFig.3\n\n\n\n\n10 mirroring11 tiling strategy, GPU 메모리 보존 가능\nAuto Encoder\n\n\nSkip-Connection"
  },
  {
    "objectID": "posts/LabHAI/papers/1505.04597.html#구현",
    "href": "posts/LabHAI/papers/1505.04597.html#구현",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "구현",
    "text": "구현\n\n모델만 구현해보았다.\n상세 구조는 아래를 참고했다.\n\n\n\n\n\n\n\n\n\n\n\n\nbrain-segmentation-pytorch\n\n\n\n\nimport torch\nimport torch.nn as nn \n\nclass Unet(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # convolution + batch normalize + pooling\n        # 3x3 convolution, unppaded convolution\n        def CBR_2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n            layer = []  # := 왜 안되는거지\n            layer = [nn.Conv2d(in_channels, out_channels)]\n            layer += [nn.BatchNorm2d(num_features=out_channels)]\n            layer += [nn.ReLU()]\n            layers = nn.Sequential(*layer)\n            return layers\n            \n        \n        # --- extract --- #\n        self.down_1_1 = CBR_2d(in_channels=1,  out_channels=64)\n        self.down_1_2 = CBR_2d(in_channels=64, out_channels=64)\n        self.pool_1 = nn.MaxPool2d(kernel_size=2)\n\n        self.down_2_1 = CBR_2d(in_channels=64,  out_channels=128)\n        self.down_2_2 = CBR_2d(in_channels=128, out_channels=128)\n        self.pool_2 = nn.MaxPool2d(kernel_size=2)\n\n        self.down_3_1 = CBR_2d(in_channels=256, out_channels=256)\n        self.down_3_2 = CBR_2d(in_channels=256, out_channels=256)\n        self.pool_3 = nn.MaxPool2d(kernel_size=2)\n\n        self.down_4_1 = CBR_2d(in_channels=512, out_channels=512)\n        self.down_4_2 = CBR_2d(in_channels=512, out_channels=512)\n        self.pool_4 = nn.MaxPool2d(kernel_size=2)\n\n        # --- bridge --- #\n        self.bridge_1 = CBR_2d(in_channels=512, out_channels=1024)\n        self.bridge_2 = CBR_2d(in_channels=1024, out_channels=512)\n        \n        # --- expand --- #\n        # 2x2 convolution (“up-convolution”)\n        # halves the number of feature channels\n        self.up_4_1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=2, padding=0)\n        self.up_4_2 = CBR_2d(in_channels=512, out_channels=512)\n        self.up_4_3 = CBR_2d(in_channels=512, out_channels=512)\n\n        self.up_3_1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, padding=0)\n        self.up_3_2 = CBR_2d(in_channels=256, out_channels=256)\n        self.up_3_3 = CBR_2d(in_channels=256, out_channels=256)\n\n        self.up_2_1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, padding=0)\n        self.up_2_2 = CBR_2d(in_channels=128, out_channels=128)\n        self.up_2_3 = CBR_2d(in_channels=128, out_channels=128)\n\n        self.up_1_1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, padding=0)\n        self.up_1_2 = CBR_2d(in_channels=64, out_channels=64)\n        self.up_1_3 = CBR_2d(in_channels=64, out_channels=64)\n\n        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n\n        def forward(self, x):\n            # down\n            down_1_1 = self.down_1_1(x)\n            down_1_2 = self.down_1_2(down_1_1)\n            pool_1 = self.pool_1(down_1_2)\n\n            down_2_1 = self.down_2_1(pool_1)\n            down_2_2 = self.down_2_2(down_2_1)\n            pool_2 = self.pool_2(down_2_2)\n\n            down_3_1 = self.down_3_1(pool_2)\n            down_3_2 = self.down_3_2(down_3_1)\n            pool_3 = self.pool_3(down_3_2)\n\n            down_4_1 = self.down_4_1(pool_3)\n            down_4_2 = self.down_4_2(down_4_1)\n            pool_4 = self.pool_4(down_4_2)\n\n            # bridge\n            bridge_1 = self.bridge(pool_4)\n            bridge_2 = self.bridge(bridge_1)\n\n            # up\n            upconv_4 = self.up_4_1(bridge_2)    # up convolution\n            concat_4 = torch.cat((upconv_4, down_4_2), dim = 1)\n            up_4_2 = self.up_4_2(concat_4)\n            up_4_1 = self.up_4_3(up_4_2)\n\n            upconv_3 = self.up_3_1(up_4_1)\n            concat_3 = torch.cat((upconv_3, down_3_2), dim = 1)\n            up_3_2 = self.up_3_2(concat_3)\n            up_3_1 = self.up_3_3(up_3_2)\n\n            upconv_2 = self.up_2_1(up_3_1)\n            concat_2 = torch.cat((upconv_2, down_2_2), dim = 1)\n            up_2_2 = self.up_2_2(concat_2)\n            up_2_1 = self.up_2_3(up_2_2)\n\n            upconv_1 = self.up_1_1(up_2_1)\n            concat_1 = torch.cat((upconv_1, down_1_2), dim = 1)\n            up_1_2 = self.up_1_2(concat_1)\n            up_1_1 = self.up_1_3(up_1_2)\n\n            # out\n            x = self.fc(up_1_1)\n\n            return x\n\n\n참고\n\n본문\n\n[Pytorch] U-Net 밑바닥부터 구현하기\n동빈나: [꼼꼼한 논문 설명] U-Net: Convolutional Networks for Biomedical Image Segmentation (MICCAI 2015)\n\n코드 구현\n\nhttps://magicode.tistory.com/68\nbrain-segmentation-pytorch"
  },
  {
    "objectID": "posts/papers/2310.07820.html",
    "href": "posts/papers/2310.07820.html",
    "title": "LLMs Are Zero-Shot Time Series Forecasters",
    "section": "",
    "text": "Abstract\n시계열을 ’숫자 문자열’로 인코딩하면, 시계열 예측을 자연어 처리에서의 다음 토큰 예측과 같이 구성할 수 있다는 접근으로 시작한다. 따라서 저자들은 GPT-3, LLaMA-2 모델과 같은 LLM 이 zero-shot 시계열 추정(extrapolate)의 비교가능한 수준에서, 또는 시계열 학습을 목적으로 만들어진 모델을 상회하는 능력을 보임을 발견했다. 이러한 성능을 촉진하기 위해서 본 논문에서는 시계열 데이터를 효과적으로 토큰화 하는 방법과 토큰에 대한 이산적 분포를 매우 유연한 밀도의 연속값으로 전환하는 방법을 제안한다. 저자들은 시계열에 대한 LLM의 성공이 단순성 및 반복에 대한 편향과 함께 multi-modal distributions을 자연스럽게 표현하는 LLM의 능력에 있다고 주장하는데 이는 많은 시계열에서 두드러지는 특징이며 특히 반복되는 계절 추세에서 그러하다는 점에서 본 논문의 주장과 상통한다.\n\n\n\n\n\n\n토큰의 이산적 분포를 연속값의 유연한 밀도로 변환하는 방법?\n\n\n\n\n\n\n이산분포(불연속분포)는 가능한 결과가 뚜렷하고 분리되어있는 확률 분포를 의미한다. 예를 들면 특정 토큰이 시퀀스에서 발생할 확률과 같다.\n연속값은 반대로, 어떤 실수에서 온도나 주가와 같은 특정 범위를 취할 수 있는 값을 말한다.\n이산분포를 연속값으로 전환함으로써 저자는 LLM이 고정된 토큰의 집합에 국한되지 않고 가능한 다양한 값을 나타낼 수 있는 예측을 생성할 수 있음에 초점을 맞춘다.\n\n\n\n\n또한 연구진은 본 논문에서 LLM이 누락된 비-수학적 데이터를 자연스럽게 처리하는 방법과 텍스트 정보를 수용하는 방법을 보이며 QA를 예측 결과를 설명하는데 도움이 되도록 사용한다. 모델의 크기와 시계열 성능은 일반적으로 비례하지만 이를 밝혀내는 과정에서 연구진은 GPT-4가 토큰 개수로 인해 GPT-3보다 더 좋지 않은 성능과 나쁜 불확실성 교정 성능을 보임을 알아낸다. 이는 RLHF와 같은 정렬 중재(alignment intervention method)의 결과일 수 있다; 사용자가 GPT-4를 이용하여 정렬하던 과정에서 오류 또는 보정이 제대로 이루어지지 않았을 가능성을 명시한다.\n\n\n\n\n\n\n‘단순성 및 반복에 대한 편향과 함께’ + ’다중 분포를 자연스럽게 표현’하는 능력?\n\n\n\n\n\n\nWe argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trend\n\n\nMultimodal distribution, 여러개의 peak와 mode가 있는 확률분포로 여러개의 그룹 또는 데이터의 패턴이 존재함을 나타낸다.\n\nmodality란 양식, 양상이라는 뜻으로 어떤 형태로 나타나는 현상 또는 ’그것을 받아들이는 방식’을 말한다. LLM은 텍스트로 된 입력을 받아왔는데 텍스트만이 아닌 다른 양식들(사진, 소리 등)을 학습하거나 표현할 수 있게 발전해왔다. 예를 들어, OpenAI의 DALL-E 2는 대표적인 multi-modal AI인데 사용자가 문장을 입력하면 모델이 이해한 바를 그림 또는 사진으로 표현한다. DALL-E 2는 기존 이미지들을 개체별로 나누고 이름을 부여한 다음, 위치와 색상, 어떤 동작을 하고 있는지를 이해하고 이미지를 설명하는데 이용된 텍스트 간의 관계를 학습한다.\n따라서 LLM에서의 Multimodal distribution은 LLM이 다중양식을 학습했을 때 보일 수 있는 확률분포다.\n단순성 및 반복에 대한 편향이 중요한 이유는 다양한 유형의 분포가 내재되어있는 시계열의 특징 때문이다. 시계열에서는 추세를 단순화하고, 반복을 찾아내어 경향을 파악해야 하기 때문에 해당 편향을 유지해야만 한다.\n\n\n\n\n\n\n\n\n\n\nRLHF, alignment method\n\n\n\n\n\n\nReinforcement Learning by Human Feedback 의 약자로 LLM의 alignment intervention method다.\n정렬 (alignment) 이란 LLM을 사용자에게 적합하게 조율하는 과정으로 사용자와의 상호작용 사이에서 이루어진다. 따라서 말 그대로 RLHF: 사람의 피드백에 의한 강화학습은 사용자가 프롬프트로 하는 미세조정이라고 볼 수 있겠다. (확인 필)\n\\(^*\\) 하지만 Meta AI에서 발표한 LIMA (Less Is More for Alignment) 논문은 LLM의 Pre-training이 중요하다고 하는데 주장을 살펴보기 위해 논문을 읽어볼 필요가 있다."
  },
  {
    "objectID": "posts/HAR/mmlab.html",
    "href": "posts/HAR/mmlab.html",
    "title": "MMLAB",
    "section": "",
    "text": "MMCV-Full with openmim\nbest practice를 따라 mim으로 mmcv-full을 설치함\npip3 install -U openmim\nmim install mmcv-full\n\n\nMMPose with pip (for third party)\n\nOfficial Install Guide\n\n# pip을 사용해도 무관 (Case b)\npip3 install mmpose\n\nverify the installation of mmpose\n\ndownload config and checkpoint\n# 본인이 원하는 폴더 생성\nmkdir verify-mmpose; cd verify-mmpose   # e.g.\n\n# download\nmim download mmpose --config associative_embedding_hrnet_w32_coco_512x512  --dest .\nverify the inference demo\n\npip을 이용해 third party 용 mmpose를 설치했으므로 demo용 python script 생성\n# (optional) option (a)를 따라 /demo 폴더를 생성\nmkdir demo; cd demo\nvi bottom_up_img_demo.py    # open vim\n# bottom_up_img_demo.py\nfrom mmpose.apis import (init_pose_model, inference_bottom_up_pose_model, vis_pose_result)\n\nconfig_file = 'associative_embedding_hrnet_w32_coco_512x512.py'\ncheckpoint_file = 'hrnet_w32_coco_512x512-bcb8c247_20200816.pth'\npose_model = init_pose_model(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n\nimage_name = 'demo/persons.jpg'\n# test a single image\npose_results, _ = inference_bottom_up_pose_model(pose_model, image_name)\n\n# show the results\nvis_pose_result(pose_model, image_name, pose_results, out_file='demo/vis_persons.jpg')\n\ndownload image for test\n\n스크립트를 수정하지 않았다면 verify-mmpose/demo/ 안에 사람이 포함된 persons.jpg 를 추가\n\nrun script\npwd # /home/devin/env/verify-mmpose\npython3 demo/bottom_up_img_demo.py\n결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npersons.jpg(input)\nvis_persons.jpg(result)\n\n\n\n└── verify-mmpose\n    ├── associative_embedding_hrnet_w32_coco_512x512.py # config\n    ├── demo\n    │   ├── bottom_up_img_demo.py\n    │   ├── persons.jpg\n    │   └── vis_persons.jpg\n    └── hrnet_w32_coco_512x512-bcb8c247_20200816.pth\n\n\nIssue (updated 2022.12.28)\n\n[alias expired over ver 1.24] ‘numpy’ has no attribute ‘int’\n# ERROR LOG\nTraceback (most recent call last):   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;   File \"/home/ubuntu/.local/lib/python3.8/site-packages/numpy/__init__.py\", line 284, in __getattr__\n    raise AttributeError(\"module {!r} has no attribute \" AttributeError: module 'numpy' has no attribute 'int'\n\n# Environment\nOS Release              Ubuntu 20.04.4 LTS\nmmcv-full               1.7.0\nmmpose                  0.29.0\nnumpy                   &gt;=1.19.5\n최신버전으로 numpy를 설치했는데 demo를 실행하는 과정에서 'numpy' has no attribute 'int' 에러가 발생했다. 공식문서에서 추가하라고 지시한 코드에는 numpy가 없었고, 로그를 살펴보니 이미지의 크기를 정하는 과정에서 int type이 필요했다. 실행한 데모는 bottom_up_transform 이었지만 np.int가 사용된 내역을 살펴보면 bottom_up 뿐 아니라 gesture에서도 사용되는 것 같았다.\n# ./datasets/pipelines/gesture_transform.py\n# ./datasets/pipelines/bottom_up_transform.py\n\ninput_size = np.array([input_size, input_size], dtype=np.int)\nstackoverflow 에 의하면 numpy1.20 부터 np.float 또는 np.int의 alias사용이 중단 되었다. np.int_ 로 대체하거나 int로 변환하라는 권고가 나왔는데, 소스코드를 전부 수정할 수 없는 상황이므로 requirements 에 따라 1.19 로 downgrade 하면 해결된다.\n\nNumPy requirements of mmpose: numpy&gt;=1.19.5\nnumpy 1.20 relesas note &gt; For np.int a direct replacement with np.int_ or int is also good and will not change behavior, but the precision will continue to depend on the computer and operating system. If you want to be more explicit and review the current use, you have the following alternatives:\nnumpy 1.24 relesas note &gt; The deprecation for the aliases np.object, np.bool, np.float, np.complex, np.str, and np.int is expired (introduces NumPy 1.20). Some of these will now give a FutureWarning in addition to raising an error since they will be mapped to the NumPy scalars in the future.\n\n\n\n\n\nMMDetection\n\nOfficial Install Guide\n\n\n마찬가지로 mim을 통해 mmcv-full을 설치한다.\n\npip3 install -U openmim\nmim install mmcv-full\n\npip으로 mmdet을 설치한다.\n\npip3 install mmdet\n\nVerify installation MMdet\n\ndownload config and checkpoint\n# 본인이 원하는 폴더 생성\nmkdir verify-mmdet; cd verify-mmdet   # e.g.\n\n# download\nmim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .\nverify the inference demo\n\npip을 이용해 third party 용 mmdet를 설치했으므로 demo용 python script 생성\n# (optional) option (a)를 따라 /demo 폴더를 생성\nmkdir demo; cd demo\nvi img_demo.py    # open vim\n# img_demo.py\nfrom mmdet.apis import init_detector, inference_detector\nconfig_file = 'yolov3_mobilenetv2_320_300e_coco.py'\ncheckpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n여기까지가 공식문서의 demo script인데 이렇게 되면 inference_detector된 결과가 CLI환경에 출력되지 않는다. https://greeksharifa.github.io/references/2021/08/30/MMDetection/#high-level-apis-for-inference 위 포스트를 참고하면 결과 이미지를 확인할 수 있다.\nimg = 'demo/demo.jpg'\ninference_detector(model, img)\n\nresult = inference_detector(model, img)\n# visualize the results in a new window\nmodel.show_result(img, result)\n# or save the visualization results to image files\nmodel.show_result(img, result, out_file='demo/demo_result.jpg')\n\n# test a video and show the results\nvideo = mmcv.VideoReader('demo/demo.mp4')\nfor frame in video:\n    result = inference_detector(model, frame)\n    model.show_result(frame, result, wait_time=1)\n\nprepare for demo\n\nmmpose와는 다르게 mmdet에는 demo용 이미지와 동영상이 있으므로 그 파일을 사용하거나 다운로드한다.\n\npwd # /home/devin/env/verify-mmdet\nwget https://github.com/open-mmlab/mmdetection/blob/master/demo/demo.jpg ./demo/demo.jpg\nrun script bash     pwd # /home/devin/env/verify-mmdet     python3 demo/img_demo.py\n결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo.jpg (input)\ndemo_result.jpg (result)\n\n\n\n└── verify-mmdet\n    ├── yolov3_mobilenetv2_320_300e_coco.py # config\n    ├── demo\n    │   ├── img_demo.py\n    │   ├── demo.jpg\n    │   └── demo_result.jpg\n    └── yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth"
  },
  {
    "objectID": "posts/HAR/modalities.html",
    "href": "posts/HAR/modalities.html",
    "title": "Modalities",
    "section": "",
    "text": "mainstream deep learning architectures\n\nsingle modalities\nmultiple modalities for enhanced HAR\n\nDATA : short trimmed video segments\n\none, only action instance\n\nBenchmark Datasets"
  },
  {
    "objectID": "posts/HAR/modalities.html#introduction",
    "href": "posts/HAR/modalities.html#introduction",
    "title": "Modalities",
    "section": "1. introduction",
    "text": "1. introduction\n\n다양한 data modalities들의 장점, 한계를 및 modality간의 연구 흐름 파악\n\n기술의 발전과 방법론의 창안은 선행 연구의 한계와 발전 가능성에 기초하므로 기술 발전의 흐름과 맥락을 숙지하는 과정은 중요하다. 본 논문은 2022 IEEE에 발표된 Review 논문으로 다양한 인간 행동 표현형을 인식하는 HAR 연구의 최신 흐름을 기술하고 있다. 개요에 따르면 인간 행동은 다양한 데이터 양식으로 표현될 수 있다. 데이터 양식은 크게 가시성 visibility 에 따라 visual modalities, non-visual modalities 로 나뉜다.\n\n\n\n\n\n\n\nvisual modalities\nnon-visual modalities\n\n\n\n\nRGB\naudio\n\n\nSkeleton\nacceleration\n\n\ndepth\nradar\n\n\ninfrared\nwifi signal\n\n\npoint cloud1\n\n\n\nevent stream\n\n\n\n\n1 3차원 공간상에 퍼져 있는 여러 포인트(Point)의 집합(set cloud)으로 Lidar 센서와 RGB-D 센서로 수집된 데이터다.\n1.1 visual modalities\n일반적으로는 visual modalities가 HAR 발전에 큰 영향을 미쳐왔다. HAR의 많은 발전이 RGB Video 또는 images를 기반으로 이루어졌음을 보면 알 수 있다. RGB data는 관측 (surveillance) 또는 추적 (monitoring) 시스템에서 보편적으로 사용되어왔다. RGB 는 기본적으로 세 채널을 가진 ’이미지(들)’이기 때문에 큰 computing resource 를 필요로 하는데 이를 보완하기 위해 사용된 데이터가 skeleton 이다. skeleton data는 인간 관절의 움직임 (trajectory of human body joints) 을 encoding한 데이터로 간명하고 효과적이다. 그러나 물체가 포함되어있거나 장면간 맥락을 고려해야 하는 경우 skeleton data 만으로는 정보를 충분히 얻을 수 없는데 이 때 point cloud 와 depth data를 사용한다. 또한, ’본다’는 행위는 근본적으로 빛에 의존하는데 이 한계를 극복하기 위해 infrared data를 사용하며 event stream 은 움직였을 때를 감지하는 동작 중심 modality다.\n\n정보를 얻고, insight 또는 활용 가능한 부분만 추려내어 새로운 방법론을 창안하고, 기술이 최적화 되었을 때 더 필요한 정보를 얻기 위해 또 다른 modality를 사용하며 다시 불필요한 부분을 제거하는 방식으로 기술이 발전되어왔다.\n\n\n가시영역에서 얻을 수 있는 정보를 얻거나 : RGB\n인간이 눈으로 보고 이해하는 정보에 집중하여 인간 신체에서 이른바 ROI를 추출해내는 방법을 적용한다: skeleton\n사람과 환경의 상호작용 또는 시공간적 맥락을 고려하기 위해 3D 구조에서 주요한 정보를 추출하는 작업을 거친다: point cloud, depth data\n시각에 의존하지 않고 나아가 비가시영역인 적외선 영역에서 정보를 얻음으로써 빛에 의존해야 한다는 visual modalities의 태생적 한계를 극복한다: infrared data\n불필요한 중복, 또는 정보를 제거하여 HAR에 적합한 데이터를 구축한다: event stream\n\n\n\n1.2 non-visual modalities\n눈으로 봤을 때 직관적이지 않지만 사람들의 행동을 표현하는 또 다른 방식이다. 직관적이지 않음에도 사용될 수 있는 이유는 특정 상황에서 대상의 개인정보 보호가 필요할 때다. audio 는 시간에 따른 상황 (temporal sequence) 에서 움직임을 인지하기에 적절하며, acceleration 는 fine-grained HAR에 사용된다. 2 3 또한 radar는 물체 뒤의 움직임도 포착할 수 있다.\n2 fine-grained HAR : 세분화된 HAR. acceleration data가 이에 사용된다는 내용은 영상의 움직임에서 가속도를 알아내는 방향의 연구를 말하는 것으로 보인다.3 관련 논문: https://arxiv.org/abs/2211.01342\n\n1.3 Usage of modalities (single vs multi)\n\nsingle modality and effect of their fusion\n살펴본 바와 같이, 각 modality는 서로 다른 강점과 한계를 가지고 있으므로 여러 양식의 데이터들을 합치거나(fusion), 데이터 양식간 ‘transfer of knowledge’4 를 진행하여 정확도와 강건함을 높인다.\n4 Transfer learning 은 Transfer Learning과 Knowledge Distillation으로 나뉘는데 서로 다른 도메인에서 지식을 전달하는 방법이 Transfer Learning (fine tuning 필요) 이고, 같은 도메인에서 다른 모델 간 지식 전달이 이루어지는 것을 Knowledge Distillation이라고 하면 ’transfer of knowledge across modalities’는 Transfer Learning을 말하는 것으로 보인다.나아가 fusion은, 서로 다른 두 개 이상의 데이터에서 각 데이터간 장단점을 상호보완하기 위한 방법론으로 소리 데이터와 시각 데이터를 포함함으로써 단순히 ‘물건을 내려 놓는’ label을 가방을 내려 놓는지, 접시를 내려놓는지 구체적으로 구분할 수 있게 한다.\n\n참고: transfer learning\n\n\n\ndata modalities and its pros and cons"
  },
  {
    "objectID": "posts/HAR/modalities.html#single-modality",
    "href": "posts/HAR/modalities.html#single-modality",
    "title": "Modalities",
    "section": "2. Single modality",
    "text": "2. Single modality\nRGB, Skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, WiFi will be reviewed"
  },
  {
    "objectID": "posts/HAR/modalities.html#visible-modalities",
    "href": "posts/HAR/modalities.html#visible-modalities",
    "title": "Modalities",
    "section": "visible Modalities",
    "text": "visible Modalities\n\n2.1 RGB\n\n한계와 장점 모두 RGB Data의 특성에서 비롯된다.\n\n\n특성\n\n이미지(들) 로 이루어져있다. (\\(\\because\\) video is sequence of images)\nRGB data를 생성하는 카메라는 사람의 눈으로 보는 장면을 재생산하는 것을 목적으로 한다.\n수집하기 쉽고 상황과 맥락을 반영하고 있는 풍부한 외관 정보가 포함되어있다.\n\n‘rich appearance information’\n\n폭넓은 분야에 사용될 수 있다.\n\nvisual surveillance: 사람이 한 순간도 놓치지 않고 관찰할 수는 없는데 이를 보완할 수 있다.\nautonomous navigation: 자율주행(ANS)의 일부로써 사람의 개입 없이 정확하게 목적지까지 도달하도록 하는 기술이다.\nsport analysis: 눈으로 쫓기 힘든 순간들을 정밀하게 판독해야 하는 분야이므로 이 또한 ’사람의 눈’을 대신한다.\n\n\n한계\n\nvarious of background, viewpoints, scales of humans\n\n학습할 수 있는 데이터는 한정적이고, 이를 활용할 수 있는 변수는 너무 많다.\n\nillumination condition\n\n촬영이라는 개념이 갖는 근본적인 한계로, 광원 상태에 따라 결과가 달라질 가능성이 있다.\n\nhigh computational cost\n\n영상은 이미지의 연속이므로 공간과 시간을 동시에 고려하여 모델링하려면 많은 자원이 요구된다.\n\n\nmodeling methods\n\npre-deep learning : handcrafted feature-based approach, 수작업 특징 기반 접근법\n\nSpace-Time Volume-based methods\nSpace-Time Interest Point (STIP)\n\ndeep learning : currently mainstream\n\nbackbone model을 무엇으로 사용하느냐에 따라 나뉠 수 있다.\n\n\ntwo-stream CNN based method / multi stream architectures (extension of two stream)\n\nbackbone : 2D CNN\n시간정보가 포함될 수 밖에 없기 때문에 temporal information, spatial information 모두 고려하는 two-stream 접근이 제안되었다.\n\nRNN based method\n\nfeature extractor : RNN model with 2D CNNs\nRGB-based model\n\n3D CNN based method\n\n\n\n\n2.1.1 Two Stream 2D CNN-Based Methods\n\ntwo 2D CNN branches taking different input features extracted from RGB video for HAR and the final result is usually obtained through fusion strategy\n\n\nclassical approach\n고전적으로 two stream network는 각 network를 병렬적으로 학습시킨 후 결과를 융합 fusion 하여 최종 결과를 추론했다. 예를들어, input이 video이면 video에 내재된 정보들을 크게 1) rgb 프레임들은 공간 네트워크의, 2) multi-frame-based optical flow는 시간 네트워크의 학습 정보가 된다. 각 stream은 아래 특성을 각각 학습한다.\n\n모양 특성, appearance feature\n동작 특성 motion feature\n\n이때 multi-frame-based optical flow: 움직임을 묘사하는 방법이며 주로 짧은 시간 간격을 두고 연속된 이미지들로 구성된다. optical flow는 이미지의 velocity(속도) 를 계산하는데 이 속도는 이미지의 특정 지점이 다음의 어디로 이동할지 예측할 수 있게 한다.\n\n주로 video understanding 에서 사용되는 개념으로 보인다.\nacceleration data와 어떻게 다른지 알 필요가 있다: The optical flow is used to perform a prediction of the frame to code\nnetworks: SpyFlow, PWC-Net; compute pixel-wise motion vectors\n\n\n\novercome limitation\nRGB 양식 데이터를 사용함에 있어 주된 문제로 지적되는 점은 ’큰 데이터 용량으로 인한 computing resource 부담과 연산 속도 저하’이므로 연산 속도를 높이기 위해 해상도를 낮추거나, 고해상도 데이터에서 center crop을 하는 기법을 적용했다.\n\n\nbetter data representation\n모델의 성능은 데이터의 양과 질에 좌우된다. 따라서 더 나은 video representation 에 눈을 돌리게 된다. Wang은 multi-scale video frames, optical flow를 각 CNN stream에 넣어 특성맵 feature map을 추출했고 이에서 trajectories에 중심을 둔 spatio-temporal tubes5 or action tube6를 샘플링했다. 이렇게 한 결과, Fisher Vector representation7과 SVM을 통해 분류했다.\n5 Discovering Spatio-Temporal Action Tubes (2018), https://arxiv.org/abs/1811.122486 Finding action tubes, https://ieeexplore.ieee.org/document/72986767 https://zlthinker.github.io/Fisher-Vector\n왜 튜브일까? : Finding action tubes (Georgia Gkioxari, 2015) 가 시기상 더 먼저 나온 논문이므로 후자에서 제시된 개념일 것으로 추정된다.\n후자 논문의 Abstract에서 tube는 “예측된 동작을 연결함으로써 시간일관적으로 객체를 탐지하는” 개념이다.\ntube를 생성하는 과정은 아래와 같다.\n\nsuggest image region : 움직임이 두드러지는 영역을 선택\nCNN을 이용하여 공간적 특징을 추출\nAction tube를 생성\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 1. Discovering Spatio-Temporal Action Tubes\nFig 2. Finding action tubes; action tube approach\n\n\n\n\n\n\nLong term video level information\n\n정보를 mean pooling하거나 누적하여 단일한 움직임이 아닌 움직임의 연속; 좀 더 복잡한 행동을 인식\n\n각 비디오를 세 개의 segment로 나눈 후 two stream network에 입력한 후, 각 segment의점수를 average pooling 을 이용해 융합한다. 또는 segment 점수를 pooling하지 않고 element-wise multiplication으로 특성의 총계를 구한다. 이 때 two stream framework에 의해 샘플링된 외형과 동작 프레임들은 ’하나의 video-level multiplied’를 위해 aggregate 연산되며 이를 action words 라고 칭한다. 8\n8 R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell, “Actionvlad: Learning spatio-temporal aggregation for action classification,” in CVPR, 2017.\n\n\n\n\n\n\n\n\n\n\nFig 3. 동작들에서 행동과 관계된 action word를 추출한 후 이를 총 망라하는 하나의 분류를 선택하는 과정\n\n\n\n\n\nEXTENSION of two stream CNN based method*\n3 stream 으로 확장하는 등, “움직임” 또는 “프레임간 연속성”을 학습시키기 위해 다양한 방법론을 도입했다. 이후 2 stream siamese network (SNN) 로 확장되었는데 이는 동작 발생 전과 동작 후 프레임에서 특징을 추출하는 one shot learning의 일종으로 연속성이 아닌 동작 시작, 전, 후를 구분하여 학습하는 발상의 전환을 꾀한다.\n\none shot learning : 소량의 데이터로 학습할 수 있게 하는 방법이 few shot learning이라면 one shot은 그 극한으로 이미지 한장을 학습 데이터로 삼는 방법론이다. 사람은 물체간의 유사성을 학습하는데, 이 유사성은 물체를 배우고 물체간의 유사성을 또 다시 배우는 과정으로 나뉠 수 있다. 다시말해, 물체의 특성을 학습하고 이를 일반화할 줄 아는 능력을 학습시키는 방법이 one/few shot learning이다.\n\n\\(\\therefore\\) 이미지 자체의 특성을 학습하는 것이 아닌, 이미지간의 유사성을 파악하고 유사도를 파악할 때 쓰는 기법인 ’거리 함수’를 사용한다.\n\nsiamese network\n\ntwo stages: verification and generalization 가 포함된다.\n각각 다른 입력을 동일한 네트워크 인스턴스에 학습시키고, 이는 동일한 데이터셋에서 훈련되어 유사도를 반환한다.\n\n\n\n\nTackle high computational cost\nKnowledge distillation 9이 사용된다. “Data 에서 의미를 파악하면 Information 이 되고, Information 에서 문맥을 파악하면 Knowledge 이 되고, Knowledge 를 활용하면 Wisdom 이 된다.” 모델 압축을 위한 절차로, soft label과 hard label을 일치시키는 것이 목적이며 soft label에는 temperature scaling function을 적용하여 확률 분포를 부드럽게 만든다. 예를 들어 feature들의 label이 \\([0, 1, 0]^{T}\\) 이면 Hard label, \\([0.05, 0.75, 0.2]^{T}\\) 이면 soft label이다.\n9 Distilling the Knowledge in a Neural Network(2015), https://arxiv.org/abs/1503.02531각 feature들은 서로 다른 특성을 가지고 있지만 공통된 특성 또한 가지고 있기 때문에, 이 공통 요소를 포함하는 class score를 날려버리면 (hard label) 정보가 손실되는 셈이다. 이렇게 정보가 손실되지 않게 Teacher network를 구성하고 Student network가 teacher network에 최대한 가까운 정답을 반환하도록 학습시킨다. 위에서 언급한 temperature는 그 값이 낮을 때 입력값의 출력을 크게 만들어주는 등 필요에 따라 값에 가중치를 둠으로써 Soft label의 이점을 최대화 한다.\n\n참고: https://light-tree.tistory.com/196\n\n\n\n\nteacher network; optical flow data\n\n\n\n\n⬇︎ Knowledge Distillation ⬇︎\n\n\nstudent network; motion vector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Spectrum\nDistillation Architecture\n\n\n\n\n\nIn Conclusion\n여러개의 stream으로 CNN architecture들을 확장하거나 더 깊게 레이어를 쌓는 등 여러 시도를 해보았으나 수많은 video의 frame 개수를 고려할 때 깊이는 오히려 HAR에 방해가 될 수 있다. 선행 연구를 통해 ’차별화된 특징’을 예측하는 것이 중요함을 파악하게 되었다. 이 외에도, fusion strategy research의 마지막 conv layer에서 공간과 시간 네트워크를 융합하는 방법이 위에서 지적된 컴퓨팅 자원을 절약하면서 (params를 줄이면서) 정확도를 유지하는 효과적인 방법임을 알아냈다.\n\n\n\n2.1.2 RNN based\n\nfeature extractor 로 CNN을 사용한 hybrid architecture\n\n\nLSTM based model\nVanilla Recurrent Neural Network의 gradient vanishing 문제로 인해 RNN based solution은 gate 를 포함하는 RNN Architecture를 채택한다. (e.g. LSTM)\n\n\n\n\n\n\n\n\n\n\n\nFig 5. RGB modality modeling methods (CNN, RNN based)\n\n\n\n물론 ‘이미지’에서 공간적 요소를 빠트릴 수 없기 때문에 특징 추출은 여전히 2D CNN으로 진행하고, 시간요소를 LSTM에서 차용한 구조를 통해 모델링한다. 이를 LRCN (Long Term Recurrent Convolutional Network, Jeff Donahue et al. in 2016) 라고 하며 이는 ’2D CNN; 프레임 단위 RGB feature 추출’ + ’label 생성 LSTM’으로 구성된다.\n\n\nattention mechanism\nmulti layer LSTM model 설계 후 다음 프레임에 가중치를 부여한 Attention map 을 재귀적으로 출력함으로써 공간적 특성에 집중할 수 있게 되었다.\n\nrecap : main idea of attention; decoder에서 출력 단어를 추론하는 매 순간마다 encoder에서의 전체 문장을 참고한다는 점. 단, ’해당 시점에서 예측에 필요한 부분에 집중(attention)’해서 구한다. \\(Attention (Q, K, V) = Attention \\ Value\\) 로, Query에 대해 모든 Key와의 유사도를 구한 후 이에 관계된 Value에 반영한다. 유사도가 반영된 값, value는 attention value라고도 한다.\n\nQ : t 시점에서 디코더 셀에서의 은닉상태\nK : keys, ‘모든 시점에서’ 인코더 셀의 은닉 상태\nV : Values, ‘모든 시점에서’ 인코더 셀의 은닉 상태로 각 인코더의 attention 가중치와 은닉상태가 가중합 된 값이다. (a.k.a. context vector)\n\n\n\n\n\n2.1.3 3D CNN based method\n\nHAR의 공간과 시간을 모두 식별할 수 있다는 강력한 장점이 있으나 많은 양의 훈련 데이터를 요구한다.\n\n지금까지는 모두 2D CNN을 시간과 함께 모델링했다. 그러나 Tran et.al [66]은 raw video data에서 시공간 데이터를 end-to-end 학습하기 위해 3D CNN 모델을 도입한다. 단, 이 경우 클립 수준 (16 frames or so) 에서 사용되는 모델이므로 시간이 길어질수록 temporal 정보가 옅어지는 한계가 있다.\n이에, Diba et al.(github, paper)는 3D 필터 및 pooling kernel로 2D 구조였던 DenseNet을 확장한 T3D (Temporal 3D ConvNet) 과 새로운 시간 계층 TTL (Temporal Transition Layer) 을 제안했다.\n\n시간에 따라 convolution kernel depth가 달라지도록 모델링 한 것 같다. 3D CNN 모델이 2D 단위에서 학습한 것을 활용하지 않는 것에 착안해 2D와 3D를 함께 쓰는 방식을 채택한 것으로 보인다.\n그 외에, 시간 범위를 늘린 LTC (Long-term Temporal Convolution) 모델, multi scale ‘temporal-only’ convolution인 Timeception 모델 등이 제안되었다.\n이는 모두 복잡하거나 긴 작업에서 영상의 길이에 구애받지 않고 인식할 수 있는 강건한 모델을 만들기 위함이다.\n\n\n\n\n2.2 Skeleton\n시점 변화에 민감한 pose estimation에서는 motion capture system으로 수집한 데이터셋만 신뢰할 수 있다. (“NTU RGB+D 120: A large-scale benchmark for 3d human activity understanding,” TPAMI, 2020.) 최근의 많은 연구는 NTU Dataset의 depth map, 또는 RGB video를 사용한다.\nRGB video만 사용할 경우 옷 또는 신체의 부피를 포함해 RGB data의 문제였던 다양한 변수 (e.g. background, illumination environment) 로부터 상당부 자유로울 수 있다. 초기에는 수작업으로 특징을 추출하여 관절 또는 신체 부위 기반의 방법이 제안되었는데 딥러닝의 발전에 따라 RNN, CNN, GNN, GCN을 적용하게 되었다.\n\n\n\n\n\n\n\n\n\n\n\nFig 7. Performance of skeleton-based deep learning HAR methods on NTU RGB+D and NTU RGB+D 120 datasets.\n\n\n\n\n\n2.3 depth\n\nDepth maps refer to images where the pixel values represent the distance information from a given viewpoint to the points in the scene.\n\n색상, 질감 등의 변화에 강건하며 3차원 상의 정보이므로 신뢰할 수 있는 3D 구조 및 기하학적 정보를 제공한다.\ndepth map은 왜 필요한가? 3D 데이터를 2D 이미지로 변환하기 위함이다: depth 정보는 특수한 센서를 필요로 하는데, 이는 active sensors (e.g., Time-of-Flight and structured-light-based cameras) and pas- sive sensors (e.g., stereo cameras) 로 나뉜다.\n\nactive sensor는 방사선을 물체에 방출하여 반사되는 에너지를 측정하여 깊이정보를 얻는, 말그대로 능동적인 행동에 의해 발생하는 정보를 수집하는 센서다. Kinect, RealSense3D 등의 특수한 장치를 포함하는 센서가 포함된다.\npassive sensor는 물체가 방출하거나 반사하는 자연적인 에너지 를 말한다. 수동센서의 예인 stereo camera는 인간의 양안을 시뮬레이션 하는 카메라로 are recovered by seek- ing image point correspondences between stereo pairs 한다.\n\n둘을 비교했을 때, passive depth map generation은 RGB 이미지 사이에서 깊이를 연산해내는 과정이 포함되므로 계산 비용이 많이 들 뿐 아니라 질감이 없거나 반복 패턴이 있는; view point에 따라 크게 달라지지 않는 대상에는 효과를 보이지 않을 수 있다. 따라서 대부분의 연구는 active sensor를 이용한 depth map에 초점을 맞추고 있다. (“only a few works used depth maps captured by stereo cameras”)\n\ndatasets and methods\n데이터셋으로는 Deep Learning methods가 도래하기 전까지 사용했던 hand-crafted Depth Motion Map (DMM) features가 있다. 딥러닝 프레임워크도 이 DMM을 활용하는데, weighted hierarchial DMMs 이 제안되었다. 그러나 기존의 DMMs가 구체적인 시간정보를 포착하지 못하는 한계를 직면하자 Wang et.al 은 dynamic images at the body, body part, joint level 총 세가지를 짝지은 depth sequences 를 CNNs에 먹이는 방식을 제안했다.\ndepth modality의 성능은 아래의 발견에 힘입어 크게 성장했다.\n\ndepth maps including dynamic (depth images)\ndynamic depth normal images\ndynamic depth motion normal images\n\n발전을 위해 제안된 아이디어는 view invarient 을 이용한 방법론들이 다수인데, 다른 시각에서 본 이미지들을 high-level space로 옮김으로써 입체감을 부여하고 (Rahmani et al. [9]) CNN 모델이 human pose model과 Fourier Temporal Pyramids를 학습하게 하여 시점에 따른 행동 변화를 학습하게 하는 방식이 있다.\n\n\nestimate without depth sensor\n\nNewer methods can directly estimate depth by minimizing the regression loss, or by learning to generate a novel view from a sequence.\n\n그러나 depth 정보를 추정해낼 수 있는 방법 또한 있다. depth estimation 기술이 이미 존재하고 Zhu and Newsam [224] 는 depth estimation을 이용해 RGB video에서 depth 을 추출해낸 바 있다. 10 가장 많이 사용되는 벤치마크는 KITTI와 NYUv2이며 일반적으로 RMS 메트릭에 따라 평가된다. 해당 기술의 Subtask로는 Monocular Depth Estimation, Stereo Depth Estimation 등이 있다.\n10 https://paperswithcode.com/task/depth-estimation\n2022 큰 주목을 받았던 diffusion model 또한 depth estimation을 사용하고 있다. multi-view 이미지들에서 차이점이 되는 point들을 찾고, 이 차이를 “splatting and diffusion”하여 depth map을 생성한다. 11\n\n\n\n\n\n\n\n\n\n\n\ngenerating diffusion map by splatting and diffusion differences of Multi-View images\n\n\n\n\n11 differentiable diffusion for dense depth estimation from multi-view images (CVPR, 2021)\n\nlimitation\n그러나 일반적으로 depth information은 외형정보가 부족하므로 다른 data modality와 융합하여 사용된다. - section 3 에서 더 살펴볼 수 있으나, 본 포스트에서는 single modality까지만 다루겠다.\n\n\n\n2.4 infrared (IR)\n\nInfrared radiation is emitted from all surfaces that have a temperature above 0 K (−273.15 °C) and the strength of emitted radiation depends on the surface temperature higher temperatures have greater radiant energy 12\n12 sciencedirect/infrared-radiation, https://www.sciencedirect.com/topics/physics-and-astronomy/infrared-radiation\n주변광에 의지하지 않아도 되므로 야간 HAR에 적합하다. depth sensor와 마찬가지로 반사광선을 활용하여 물체를 인식하는데, 적외선을 내보내는 센서가 active sensor라면 대상에서 방출되는 광선 (열 에너지 등) 을 인식하는 방법은 passive sensor를 이용한 인식이다.\n\nmethods\n신호에서 minimum하게 구분할 수 있는 간격으로 탐지하기 위해 받아들이는게 신호라고 할 때 서로 다른 신호의 간격이 얼마나 가까이까지 구분해낼 수 있는가를 해상도라고 말한다. 즉, 실제로 서로 다른걸 다르다고 말할 수 있는 거리가 해상도다. 해상도가 높으면 같은 이미지도 높은 픽셀로 표현할 수 있다. 극히 낮은 해상도의 row resolution thermal images에서 먼저 사람의 무게중심을 기반으로 사람 부분만 추출하고, 다음으로 cropped sequences들과 frame간 차이를 LSTM기반의 CNN에 입력해 시공간 정보를 담은 모델을 생성할 수 있다. (Kawashima et.al, 2017)13\n13 Action recognition from extremely low-resolution thermal image sequence\n왜 굳이 저해상도 이미지를 사용하는가?\n\n적외선 이미지(InfraRed)는 해상도가 rgb 이미지보다 훨씬 낮다.\n적외선 방사선 검출기의 단점이자 한계, 해상도를 개선한 센서도 있다: High Resolution infrared Radiation Sounder14\n\n저해상도면 저해상도지 초저해상도 이미지를 굳이 사용하는 이유는?\n\n절대적인 위치 정보인 픽셀값이 아니라 상대적인 위치에 집중할 것이니 해상도 자체가 크게 의미있지 않다. 15\n초저해상도 이미지를 사용할 수 있는 이유는 다음과 같이 정리된다\n\nRGB high resolution image 도 human body edge를 명확하게 연산하기는 어려우며 pixel values들은 사람의 움직임, 센서와의 거리 등에 크게 영향을 받는다.\n따라서 high resolution image가 집중하는 human body edge가 연산해내기 어려운 특성이라면 아예 이를 제외하고 다른 부분에 초점을 맞추겠다는 선언으로 보인다.\n대신 대략적인 범위를 알 수 있으니 결국 얼만큼 움직이는지만 감지해내면 되고, 그 움직임은 pixel 값으로 표현되니 optical image 를 대신할 수 있다.\n이는 ’초’저해상도 이미지로도 가능하기 때문에 연산량 등의 이점을 얻기 위해 해상도를 최대한 내린 것으로 유추된다.\n\n\n\n14 HIRS15 Action recognition from extremely low-resolution thermal image sequence.(2017)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3. Example of images captured at night-time\nFigure 4. Example of a thermal image sequence\n\n\n\n\nthermal images와 thermal sequence의 차이는?\n\n이미지간 차이를 추출해낸 결과다. 이후 열화상 비디오들에서 학습된 시공간 정보를 동시에 학습하기 위해 3D CNN을 적용한 Shah et al의 연구가 이어졌고, Meglouli et al는 raw thermal images를 사용하는 대신 raw thermal sequences를 3D CNN에 적용하여 optical flow information를 연산해냈다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample of a thermal image sequence16\nExample of RGB and InfRared17\n\n\n\n16 Action Recognition from Extremely Low-Resolution Thermal Image Sequence17 IR pair images in real maritime dataset\n\n\n\n2.5 Point Cloud\n\nPoint cloud data is composed of a numerous collection of points that represent the spatial distribution and surface characteristics of the target under a spatial reference system.\n\n시공간보다는 2D와 3D 모두 풍부한 정보를 얻을 수 있는 데이터 양식으로 2차원에서는 실루엣을, 3차원에서는 대상의 기하학적 정보를 포함하기 때문에 3D HAR에서 확고한 입지를 다지고 있다. 해당 정보를 얻기 위한 방법은 두가지가 있는데 하나는 3D sensor (cf. LiDar, Kinect) 를 사용하는 것이고 다른 하나는 이미지를 기반으로 한 3차원 재구성 (image based 3D reconstruction) 을 수행하는 것이다.\n\n\n\n\n\n\n\n\n\n\n\nFig 1. lidar point cloud\n\n\n\n\nUsing point cloud sequence by Voxel\n또한 딥러닝의 발전으로 딥러닝 방법론이 주목받았고, 일반적으로 더 나은 성능을 보였다. 2020년 CVPR에서 “3dv: 3d dynamic voxel for action recognition in depth video,” 발표되어 raw point cloud sequence를 일반적으로 사용할 수 있는 3D 화소 집합 (voxel sets) 으로 변환한 바 있다.\n\nvoxel : 입체 화상을 구성하는 3D 화소로 volume element를 말한다. 데이터 포인트로 구성되는데 이는 하나 또는 여러개의 데이터 조각으로 구성된다. (e.g. 불투명도, 색상 …) 따라서 Vector (or Tensor) 데이터로 구성되며 다양한 속성을 표현할 수 있다. (e.g. CT에서 재료의 불투명도를 부여하는 Hounsfield scale: 방사선의 밀도를 표현)\n\npixel (picture + element): raster image를 구성하는 가장 작은 단위 또는 display에서 접근 가능한 모든 점들의 집합을 말하며 대부분 digital display 에서 표현되는 그래픽들의 가장 작은 단위로 사용된다.\nvoxel (volume + element): 3D Computer graphic에서 3차원 상의 일반 격자(regular grid)를 나타내기 위한 단위로, 고유한 state parameter를 가지며 모델 객체에 종속된다. 18\n\nregular grid? : grid는 regular grid와 irregular grid로 구분되는데 일반 격자는 테셀레이션(tesselation)의 n차원 유클리디안 공간으로 규칙적인 간격을 가진다.\ntesselation? : 테셀레이션은 computer graphic 용어로 장면의 객체를 렌더링하기에 적합하도록 나타내는 다각형 데이터 집합 또는 vertex sets 이다.*\n\ntexel (texture + element): texture map의 기본단위로, 이미지를 픽셀로 표현하는 것 처럼 배열을 texture 공간에 나타내어 질감을 표현한다.\nresel (resolution + element ): 실제 공간 해상도에서 이미지 또는 부피데이터셋이 차지하는 비율을 나타낸다. resels per pixel, resels per voxel 등으로 표현한다.\n\n\n18 어쩔 수 없이 wikipedia Voxel 결과이렇듯 voxel에는 다양한 속성을 표현할 수 있으므로 voxel sets 을 3D action information으로 인코딩할 수 있다. 이러한 추상화 과정을 통해 학습한 모델이 PointNet++다.\n물론, point cloud를 voxel로 변환하는 과정에서 다량의 양자화 오차(quantization errors)19가 발생하여 효력면에서 충분히 효과적이지 않은데 이를 해결하기 위해 제안된 모델이 MeteorNet이다. 해당 모델은 여러 프레임의 point cloud들을 local 특성으로 합산하는데 이 때 spatio-temporal neighboring point 들을 사용한다. 다시 말해, 모든 point cloud를 voxel로 변환할 때 유실되는 값이 많으므로 국소 범위에서 관계가 있을 것으로 추정되는 주변 값을 변환하고, 또 변환하여 오차를 줄이는 방식을 채용한 셈이다.\n19 ADC (Analog to Digital Converter) 에서 입력 아날로그 신호가 출력 디지털 신호로 변환될 때 유실되는 값이다.이와 반대로 점의 공간적 불규칙성이 정보값에 혼란을 줄 것을 우려한 PSTNet은 시공간 정보를 분리하기도 했다.\n\n\nModeling\n이렇게 재구성된 Point Cloud로 수행해야하는 바는 다른 modality와 동일하게, 시공간을 동시에 고려고 그 특성을 파악하는 작업이다. 3차원 공간의 정보의 누수를 막고 voxel sets을 구해낸 후의 연구는 시간을 모델링하는데 초점을 맞추는데 이는 여타 방법론과 유사하다. RNN 기반의 모델인 LSTM을 적용한다. 눈여겨 볼 점은 4D CNN이 도입되었다는 점인데, 이는 LSTM 도입의 연장선으로 이미 3차원인 공간 모델링에 시간 차원을 추가하는 방식이다. 20\n\n\n20 self-supervised modeling이 상대적으로 자주 언급되는데 point cloud 특성 때문인지 확인이 필요하다."
  },
  {
    "objectID": "posts/NLP/huggingface/PEFT.html",
    "href": "posts/NLP/huggingface/PEFT.html",
    "title": "PEFT",
    "section": "",
    "text": "Quicktour\n모든 peft는 PeftConfig class를 정의한다. PeftConfig는 PeftModel을 만드는데 중요한 parameters를 저장한다. huggingface Quicktour에서는 LoRA를 이용해 클래스 분류문제를 푸므로 여기서 생성해야 할 PeftConfig는 LoraConfig다.\nLoraConfig는 아래와 같이 정의한다\n# pip install peft\nfrom peft import LoraConfig, TaskType\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8,\n                        lora_alpha=32, lora_dropout=-0.3 )\n\nmajor parameters\n각 인자는 다음을 나타낸다. (more)\n\ntask_type : TaskType 에서 task 종류를 정의한다. 이 경우에는 Seq2Seq 언어 모델링이다.\n\n왜 하필 Seq2Seq?\n\ninference_mode : 추론을 할 때 사용한다.\nr : low-rank matrices의 차원을 결정한다.\n\nLoRA의 LoR가 Low Rank다. (LoRA: Low Rank Adaptation) 이 때의 r 값이 ’학습 가능한 분해행렬’에 해당한다.\nlow-rank matrices: 행렬의 랭크는 행렬에서 선형적으로 독립적인 열(또는 이에 상응하는 행)의 최대 수, 행렬로 표현되는 벡터가 포함하는 최대 차원 수로 행렬에 포함된 정보의 양이라고 이해할 수 있다.\n아래와 같이 구할 수 있다.\nimport numpy as np\nA = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])\nprint(\"Rank of A:\", np.linalg.matrix_rank(A))   # 1\n\nB = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Rank of B:\", np.linalg.matrix_rank(B))   # 2\n\nlora_alpha : low rank matrices를 위한 ‘scaling factor’, lora scaling에 필요한 파라미터다.\n\nlora scaling? (LoRA 참고) : LoRA는 사전학습된 가중치 행렬 \\(W_0\\) 이 업데이트 될 때 \\(\\Delta W\\) 를 \\(BA\\) 로 바꾸어 더하는 과정이다. \\[W_0 + \\Delta W = W_0 + BA\\] 여기서 \\(A\\)는 random Gaussian initialized matrix 이고 B는 0으로 initialization 된 값이다. 학습 과정에서 \\(\\Delta W x\\) 는 \\(\\frac{\\alpha}{\\gamma}\\) 로 scaling 되고 이후 Optimized 된다. 여기서\\(\\alpha\\) 를 튜닝하는 것은 learning rate처럼 tuning 될 수 있다.\n\nlora_dropout : LoRA 레이어를 dropout하는 확률이다.\n\n다음으로 PeftModel을 정의한다. get_peft_model() 로 불러오며 불러오는 방식은 다른 huggingface 모델의 방식과 동일하다.\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n이렇게 기본 모델과 peft_config을 get_peft_model로 wrapping한다. 이렇게 wrap된 모델이 PeftModel이다.\nfrom peft import get_peft_model\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282\"\n위 코드의 실행 결과를 보면 실제로 우리가 학습시켜야 하는 학습인자는 0.19%만 되는 것을 확인할 수 있다. 원체 초기 파라미터 개수가 방대라니 0.19%도 적지 않은 수겠지만 비율상 상당한 감소다.\n\n\nReference\n\nhttps://huggingface.co/blog/peft\nhttps://da2so.tistory.com/79"
  },
  {
    "objectID": "posts/NLP/LoRA.html",
    "href": "posts/NLP/LoRA.html",
    "title": "LoRA",
    "section": "",
    "text": "사전 학습된 모델 가중치를 동결하고 학습 가능한 순위 분해 행렬을 트랜스포머 아키텍처의 각 계층에 주입함으로써 LLM이 제기하는 비용 및 효율성 문제에 대한 해결책을 제시한다. 이 혁신적인 접근 방식은 다운스트림 작업에서 학습 가능한 파라미터의 수를 획기적으로 줄여 GPU 메모리 요구 사항을 크게 줄이고 학습 처리량을 개선한다. 여기서 ’Low Rank matrix’는 왜 중요할까? ’extensive deep learning model’의 가중치 행렬이 low rank matrix에 존재한다는 연구결과가 있었기 때문이다.\n\ne.g. 1000차원의 임베딩 벡터가 있다고 하자. 이렇게 하면 \\(1000x1000\\) 차원의 K, Q, V 행렬이 생성되며, 각각 \\(10^3 * 10^3 = 10^6\\) 개의 훈련 가능한 파라미터가 생성된다. 반면 이를 low rank matrix로 압축시키면 학습 가능한 파라미터는 20000개로 줄어든다. 따라서 LLM의 목표는 이 행렬들을 low rank로 압축하여 학습해야하는 파라미터의 수를 줄이는 것이다. (그림 참고)\n\n\n\n\n\n\n\n\n\n\n\nOur reparametrization. We only train A and B. For above example r=8 and d =1000.\n\n\n\nLLM에서의 fine tuning은 모델 내의 모든 가중치 행렬을 또 다른 가중치 행렬로 이동하는 과정으로 이해될 수 있는데 base model의 안정성을 위해 가중치행렬을 동결하고 (freeze) \\(W\\) 행렬을 두개의 low rank matrix인 \\(A\\), \\(B\\) 로 분해하는 과정을 거친다. 이 과정에서 가중치 행렬을 정확하게 찾아낼 수 있다면 좋겠지만 찾아내는 과정 또한 연산량에 포함된다. 그러므로 r 파라미터로 가중치 행렬이 있을만한 ’적당히 작은 랭크의 행렬’으로 정하고 근사화한다. (LoRA)\n\n\n\nfrom peft import PeftModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"bigcode/starcoder\",\n        use_auth_token=True,\n        device_map={\"\": Accelerator().process_index},\n    )\n\n\n# lora hyperparameters\nlora_config = LoraConfig(r=8,target_modules = [\"c_proj\", \"c_attn\", \"q_attn\"])\n\n\nmodel = get_peft_model(model, lora_config)\ntraining_args = TrainingArguments(\n    ...\n)\n\ntrainer = Trainer(model=model, args=training_args, \n                  train_dataset=train_data, eval_dataset=val_data)\n\nprint(\"Training...\")\ntrainer.train()\n\n# plugging the adapter into basemodel back\nmodel = PeftModel.from_pretrained(\"bigcode/starcoder\", peft_model_path)\n\n\n\n\n\nLoRA : https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "posts/NLP/LoRA.html#lora",
    "href": "posts/NLP/LoRA.html#lora",
    "title": "LoRA",
    "section": "",
    "text": "사전 학습된 모델 가중치를 동결하고 학습 가능한 순위 분해 행렬을 트랜스포머 아키텍처의 각 계층에 주입함으로써 LLM이 제기하는 비용 및 효율성 문제에 대한 해결책을 제시한다. 이 혁신적인 접근 방식은 다운스트림 작업에서 학습 가능한 파라미터의 수를 획기적으로 줄여 GPU 메모리 요구 사항을 크게 줄이고 학습 처리량을 개선한다. 여기서 ’Low Rank matrix’는 왜 중요할까? ’extensive deep learning model’의 가중치 행렬이 low rank matrix에 존재한다는 연구결과가 있었기 때문이다.\n\ne.g. 1000차원의 임베딩 벡터가 있다고 하자. 이렇게 하면 \\(1000x1000\\) 차원의 K, Q, V 행렬이 생성되며, 각각 \\(10^3 * 10^3 = 10^6\\) 개의 훈련 가능한 파라미터가 생성된다. 반면 이를 low rank matrix로 압축시키면 학습 가능한 파라미터는 20000개로 줄어든다. 따라서 LLM의 목표는 이 행렬들을 low rank로 압축하여 학습해야하는 파라미터의 수를 줄이는 것이다. (그림 참고)\n\n\n\n\n\n\n\n\n\n\n\nOur reparametrization. We only train A and B. For above example r=8 and d =1000.\n\n\n\nLLM에서의 fine tuning은 모델 내의 모든 가중치 행렬을 또 다른 가중치 행렬로 이동하는 과정으로 이해될 수 있는데 base model의 안정성을 위해 가중치행렬을 동결하고 (freeze) \\(W\\) 행렬을 두개의 low rank matrix인 \\(A\\), \\(B\\) 로 분해하는 과정을 거친다. 이 과정에서 가중치 행렬을 정확하게 찾아낼 수 있다면 좋겠지만 찾아내는 과정 또한 연산량에 포함된다. 그러므로 r 파라미터로 가중치 행렬이 있을만한 ’적당히 작은 랭크의 행렬’으로 정하고 근사화한다. (LoRA)\n\n\n\nfrom peft import PeftModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"bigcode/starcoder\",\n        use_auth_token=True,\n        device_map={\"\": Accelerator().process_index},\n    )\n\n\n# lora hyperparameters\nlora_config = LoraConfig(r=8,target_modules = [\"c_proj\", \"c_attn\", \"q_attn\"])\n\n\nmodel = get_peft_model(model, lora_config)\ntraining_args = TrainingArguments(\n    ...\n)\n\ntrainer = Trainer(model=model, args=training_args, \n                  train_dataset=train_data, eval_dataset=val_data)\n\nprint(\"Training...\")\ntrainer.train()\n\n# plugging the adapter into basemodel back\nmodel = PeftModel.from_pretrained(\"bigcode/starcoder\", peft_model_path)\n\n\n\n\n\nLoRA : https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "archive",
    "section": "",
    "text": "Focusing on how technology, especially AI, can solve blind spots in society.\nSolve what no one wants to do and hardly achieve only by manpower.\n\nContact: 4923.py@gmail.com\nPresent: Internship in Lab HAI, HUFS"
  },
  {
    "objectID": "CV.html#yeeun-hong",
    "href": "CV.html#yeeun-hong",
    "title": "archive",
    "section": "",
    "text": "Focusing on how technology, especially AI, can solve blind spots in society.\nSolve what no one wants to do and hardly achieve only by manpower.\n\nContact: 4923.py@gmail.com\nPresent: Internship in Lab HAI, HUFS"
  }
]