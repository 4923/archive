[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Categories\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nMMLAB\n\n\nMMLAB installation guide (non-official)\n\n\n7 min\n\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\n\n\nskeleton data plotting\n\n\nfor NTU RGB+D\n\n\n1 min\n\n\n\nHAR\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\nModalities\n\n\nfor Human Action Recognition\n\n\n23 min\n\n\n\nHAR\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nLLMs Are Zero-Shot Time Series Forecasters\n\n\n2310.07820\n\n\n3 min\n\n\n\nLLM\n\n\nZero Shot\n\n\ntime series forecast\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nLoRA: Low-Rank Adaptation of Large Language Models\n\n\n2106.09685\n\n\n2 min\n\n\n\nLLM\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\ntest\n\n\n2106.09685\n\n\n2 min\n\n\n\nLLM\n\n\npaper review\n\n\n\n\n\n\n\n\n\n\n\nZero-shot Learning\n\n\n배우는 방법을 배우기, Meta learning\n\n\n2 min\n\n\n\nMeta learning\n\n\ntheory\n\n\n\n\n\n\n\n\n\n\n\np tuning\n\n\nPEFT; p tuning\n\n\n1 min\n\n\n\nNLP\n\n\nLLM\n\n\ntechnique\n\n\n\n\n\n\n\n\n\n\n\nencoding\n\n\nPositional encoding\n\n\n10 min\n\n\n\nNLP\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\ntips\n\n\nhuggingface 알아보기\n\n\n3 min\n\n\n\nNLP\n\n\nhuggingface\n\n\n\n\n\n\n\n\n\n\n\nPEFT\n\n\nLLM Parameter Efficient Fine Tuning with huggingface tutorial\n\n\n3 min\n\n\n\nNLP\n\n\nhuggingface\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nModel freezing\n\n\npytorch model layer freezing\n\n\n1 min\n\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nDCGAN face tutorial\n\n\nGenerating Adversarial Example\n\n\n3 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nfgsm tutorial\n\n\nGenerating Adversarial Example\n\n\n2 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nfinetuning tutorial\n\n\nTorchVision Object Detection Finetuning Tutorial\n\n\n7 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\ntransfer learning tutorial\n\n\nTransfer Learning for Computer Vision Tutorial\n\n\n1 min\n\n\n\nPyTorch\n\n\ncomputer vision\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nPyTorch101\n\n\nIntroduction to PyTorch\n\n\n9 min\n\n\n\nPyTorch\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nGraph 101\n\n\nGraph Representation Learning (2020, William L. Hamilton)\n\n\n4 min\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nHandling imbalanced Data\n\n\nundersampling, oversampling, loss function, weight sampling\n\n\n3 min\n\n\n\ntechnique\n\n\n\n\n\n\n\n\n\n\n\nHow to read a paper\n\n\nS. Keshav, University of Waterloo\n\n\n9 min\n\n\n\ntutorial\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/papers/2106.09685.html",
    "href": "posts/papers/2106.09685.html",
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "section": "",
    "text": "github\n\n\nAbstract\nMS사에서 진행한 연구. 자연어 처리에서의 중요한 패러다임 중 하나는 일반적인 도메인에서 학습된 대규모 사전학습 모델을 특정 과업이나 도메인에 적용시키는 것이다. 이 작업을 fine tuning이라고 하는데 더 큰 사전학습 모델을 사용할수록 모델의 매개변수를 유지하는 것은 더욱 어려워진다. 이는 더 많은 컴퓨팅 파워를 요구하고, 더 많은 비용을 야기하므로 현재 패러다임에서 큰 모델을 효과적으로 fine tuning 하는 방법을 찾는 것은 중요하다.\n이에 연구진은 Low Rank Adaption (LoRA)를 제안한다. LoRA는 사전학습 모델의 가중치를 freeze하고 학습 가능한 rank decomposition matrices를 각 레이어에 끼워넣는 방식이다. 이는 downstream 작업에서 학습 가능한 매개변수의 개수를 크게 줄일 수 있다.\n연구진이 예시로 든 GPT-3 175B 모델은 매개변수가 175 Billion 개라는 의미로 수많은 매개변수가 모델 안에 내재되어있음을 이름으로 알 수 있다. 해당 모델을 비롯하여 현재의 LLM 모델들은 billion 단위의 매개변수를 기본적으로 탑재하고 있고 이는 자연히 막대한 컴퓨팅 자원을 필요로 하게 된다.\n예시 모델을 LoRA와 Adam으로 fine tuning한 경우 학습 가능한 매개변수는 10,000배 줄어들고 GPU memory 크기는 3배 줄어들게 된다. LoRA는 fine tuning된 RoBERTa, DeBERTa, GPT-2, GPT-3의 능력 동급 혹은 그 이상으로 기능한다. 더 적은 개수의 학습 가능 매개변수를 가졌음에도 불구하고 더 많은 학습 처리량을 보이며, adapter와는 다르게 추론 지연(inference latency)이 일어나지 않는다는 장점이 있다.\n연구진은 또한 경험적 조사를 통해 language model adaptation의 낮은 계수(rank of matrix)가 모델의 효과를 조명함을 알아냈다.\n이에 연구진은 RoBERTa, DeBERTa, GPT-2의 checkpoints와 LoRA를 PyTorch로 구현한 통합 구현체를 배포한다. : https://github.com/microsoft/LoRA\n\n\n\n\n\n\nfine tuning에 최적화 함수가 필요한 이유\n\n\n\n\n\n\nfine tuning은 새로운 데이터로 모델을 학습하여 일반적인 과제를 수행하는 대형 모델을 특수한 도메인에 정착시키는 과정이다. 이 과정은 단순 학습인데 왜 최적화 함수가 필요한가? 이전에 사용했던 최적화 함수를 다시 사용하면 되지 않나?\nfine tuning도 결과적으로 ’학습’을 하는 과정이다. 학습 과정에서 optimizer는 최적화 함수로 loss function의 값이 가장 작은 값으로 수렴하도록 돕는 역할을 한다. 즉, loss function을 최적화하는 것이 optimizer의 목표다. optimizer 함수는 모델의 매개변수(parameter)를 매번 조절하여 손실함수의 값을 최소화하는데, 이를 통해 fine tuning에서도 tuning이 제대로 되고 있는지 확인할 수 있다. 최적화는 모델 성능에도 영향을 미치므로 단순히 이전에 사용했던 최적화 함수를 재사용하기보다는 새로운 전략을 모색하는 것이 효과적이다. 최적화 전략은 데이터셋의 크기, 사용 가능한 계산 리소스를 포함한 여러가지 요소를 고려하기 때문이다.\n\n\n\n\n\n\n\n\n\n\n\n\noptimization process"
  },
  {
    "objectID": "posts/HAR/ntu-skl-plotting.html",
    "href": "posts/HAR/ntu-skl-plotting.html",
    "title": "skeleton data plotting",
    "section": "",
    "text": "!pwd\n!ls\n!cd ntu_utils/parser_repo; ls; git remote -v\n\n/home/devin/wdir/datasets/utils\nmmskeleton  ntu_utils  plotting  plotting.ipynb  working.ipynb\nconf.py  __pycache__  README.md  read_skeleton.py  utils.py\norigin  https://github.com/Ugenteraan/NTU-RGB-Skeleton-Python.git (fetch)\norigin  https://github.com/Ugenteraan/NTU-RGB-Skeleton-Python.git (push)\n\n\n\n# basic tools\nimport os\nimport random\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\n\n# basic data handling tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# image processing\nimport cv2\nimport imageio\nfrom PIL import Image\nimport matplotlib.image as mpimg\n\n# Modules\nfrom ntu_utils.parser_repo.read_skeleton import read_skeleton\n# 원래 용법대로 사용하면 가져와서 쓸 수가 없어서 import 해서 쓸 수 있도록 변경\n# !cd parser_repo; ls; python3 read_skeleton.py --skel=\"/home/devin/wdir/datasets/NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/S001C001P001R001A001.skeleton\" --save\n\n\n'''\nGLOBAL VARIABLE\n'''\n\n\n@dataclass\nclass args:\n    # wdir/datasets/NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/S001C001P001R001A001.skeleton\n    file_name = \"S001C001P001R001A002\"\n    skel_raw = f\"../NTU_RGB+D/1_NTU-RGB+D/1_3DSkeletons/nturgb+d_skeletons/{file_name}.skeleton\"\n    skel_processed_sub = \"../processed/st-gcn-processed-data/NTU-RGB-D/xsub/train_data.npy\"\n    skel_processed_view = \"../processed/st-gcn-processed-data/NTU-RGB-D/xview/train_data.npy\"\n    subject_ID = None\n    video = None\n    save = None\n\nSAMPLE = read_skeleton(args)  # args : class, global variable\nSAMPLE_NUMBER = int(args.file_name.split(\"A\")[-1])\n\nrandom_seed = 1\nrandom.seed(random_seed)\n\n# {color_name(str) : (color code B,G,R)}\ncolors = {\n    'blue' : (255, 0, 0,),\n    'green' : (0, 255, 0),\n    'red' : (0, 0, 255),\n    'white' : (255, 255, 255),\n    'black' : (0, 0, 0,),\n    'cyan' : (0,255,255),\n    'magenta' : (255,0,255),\n    'yellow' :  (255,255,0)\n    }\n\n# {joint_name(str) : int}\njoint_annotations = {\n    \"base of spine\" : 1,\n    \"middle of spine\" : 2,\n    \"neck\" : 3,\n    \"head\" : 4,\n    \"left shoulder\" : 5,\n    \"left elbow\" : 6,\n    \"left wrist\" : 7,\n    \"left hand\" : 8,\n    \"right shoulder\" : 9,\n    \"right elbow\" : 10,\n    \"right wrist\" : 11,\n    \"right hand\" : 12,\n    \"left hip\" : 13,\n    \"left knee\" : 14,\n    \"left ankle\" : 15,\n    \"left foot\" : 16,\n    \"right hip\" : 17,\n    \"right knee\" : 18,\n    \"right ankle\" : 19,\n    \"right foot\" : 20,\n    \"spine\" : 21,\n    \"tip of left hand\" : 22,\n    \"left thumb\" : 23,\n    \"tip of right hand\" : 24,\n    \"right thumb\": 25\n}\n\n# {int : labels(str)}\nntu_label_annotations = {\n    1   :  \"drink water\",\n    2   :  \"eat meal/snack\",\n    3   :  \"brushing teeth\",\n    4   :  \"brushing hair\",\n    5   :  \"drop\",\n    6   :  \"pickup\",\n    7   :  \"throw\",\n    8   :  \"sitting down\",\n    9   :  \"standing up (from sitting position)\",\n    10  :  \"clapping\",\n    11  :  \"reading\",\n    12  :  \"writing\",\n    13  :  \"tear up paper\",\n    14  :  \"wear jacket\",\n    15  :  \"take off jacket\",\n    16  :  \"wear a shoe\",\n    17  :  \"take off a shoe\",\n    18  :  \"wear on glasses\",\n    19  :  \"take off glasses\",\n    20  :  \"put on a hat/cap\",\n    21  :  \"take off a hat/cap\",\n    22  :  \"cheer up\",\n    23  :  \"hand waving\",\n    24  :  \"kicking something\",\n    25  :  \"reach into pocket\",\n    26  :  \"hopping (one foot jumping)\",\n    27  :  \"jump up\",\n    28  :  \"make a phone call/answer phone\",\n    29  :  \"playing with phone/tablet\",\n    30  :  \"typing on a keyboard\",\n    31  :  \"pointing to something with finger\",\n    32  :  \"taking a selfie\",\n    33  :  \"check time (from watch)\",\n    34  :  \"rub two hands together\",\n    35  :  \"nod head/bow\",\n    36  :  \"shake head\",\n    37  :  \"wipe face\",\n    38  :  \"salute\",\n    39  :  \"put the palms together\",\n    40  :  \"cross hands in front (say stop)\",\n    41  :  \"sneeze/cough\",\n    42  :  \"staggering\",\n    43  :  \"falling\",\n    44  :  \"touch head (headache)\",\n    45  :  \"touch chest (stomachache/heart pain)\",\n    46  :  \"touch back (backache)\",\n    47  :  \"touch neck (neckache)\",\n    48  :  \"nausea or vomiting condition\",\n    49  :  \"use a fan (with hand or paper)/feeling warm\",\n    50  :  \"punching/slapping other person\",\n    51  :  \"kicking other person\",\n    52  :  \"pushing other person\",\n    53  :  \"pat on back of other person\",\n    54  :  \"point finger at the other person\",\n    55  :  \"hugging other person\",\n    56  :  \"giving something to other person\",\n    57  :  \"touch other person's pocket\",\n    58  :  \"handshaking\",\n    59  :  \"walking towards each other\",\n    60  :  \"walking apart from each other\",\n    61  :  \"put on headphone\"\n}\n\n\nprint(f\"Sample Data has {SAMPLE.keys().__len__()} keys\")        # frame이 각각 다른 것 같음 \njoint_number = 1\nif SAMPLE[str(joint_number)].__len__() == SAMPLE[str(joint_number+1)].__len__():\n    print(f\"Each key has {SAMPLE['1'][0].__len__()} values : joints\")\n\nSample Data has 158 keys\nEach key has 25 values : joints\n\n\n\ndef extract_coordinates(data:dict, target_joint:str, joint_annotations:dict=joint_annotations):\n    \"\"\"\n    Args : \n        - data : dictionary type, skeleton extension raw NTU120 data\n            - keys : total SAMPLE.keys().__len__() length, MUST be string   / length is not fixed\n            - each key has 25 indexes which represents 25 joints\n            - each key has 3 data which represents coordinate dimension: X, Y, (Z)\n        - target_joint : configuration joint of ntu set (exact name)\n        - joint_annotation : dictionary, match string 'target_joint' into its number\n    \"\"\"\n    \n    # Arguments\n    X, Y, Z = 0, 1, 2\n    dontknow = 0    # 이중리스트임 [[값]]\n    start_frame, end_frame = 0, len(data.keys())\n    joint_number = joint_annotations[target_joint] - 1      # 0부터 시작하므로\n\n    # print\n    print(\"=\"*60)\n    print(f\"[{target_joint}]\")\n\n    for next_frame in range(1, end_frame, 10):\n        print(f\"frame no. {start_frame+next_frame}\\t\", end=\" \")\n        print(f\"X : {data[str(start_frame+next_frame)][dontknow][joint_number][X]:.3f}\", end=\"\\t\")\n        print(f\"Y : {data[str(start_frame+next_frame)][dontknow][joint_number][Y]:.3f}\", end=\"\\t\")\n        print(f\"Z : {data[str(start_frame+next_frame)][dontknow][joint_number][Z]:.3f}\")\n\n# test\nfor joint in ['base of spine', 'middle of spine', 'head', 'right foot', 'left foot']:\n    extract_coordinates(SAMPLE, joint)\n    break\n\n============================================================\n[base of spine]\nframe no. 1  X : 1047.710   Y : 513.642 Z : -0.191\nframe no. 11     X : 1048.290   Y : 513.821 Z : -0.183\nframe no. 21     X : 1048.281   Y : 511.923 Z : -0.216\nframe no. 31     X : 1049.081   Y : 509.494 Z : -0.219\nframe no. 41     X : 1048.423   Y : 512.998 Z : -0.216\nframe no. 51     X : 1048.136   Y : 513.229 Z : -0.224\nframe no. 61     X : 1048.910   Y : 511.708 Z : -0.197\nframe no. 71     X : 1049.379   Y : 512.329 Z : -0.202\nframe no. 81     X : 1049.375   Y : 512.324 Z : -0.237\nframe no. 91     X : 1049.288   Y : 511.088 Z : -0.226\nframe no. 101    X : 1048.591   Y : 510.243 Z : -0.277\nframe no. 111    X : 1048.618   Y : 510.258 Z : -0.298\nframe no. 121    X : 1048.800   Y : 510.808 Z : -0.230\nframe no. 131    X : 1048.711   Y : 511.964 Z : -0.236\nframe no. 141    X : 1049.196   Y : 511.940 Z : -0.268\nframe no. 151    X : 1050.422   Y : 511.508 Z : -0.296\n\n\n\ndef check_label(sample_number:int=SAMPLE_NUMBER, ntu_label_annotations=ntu_label_annotations) -&gt; str:\n    '''\n    Args:\n        - label: ntu_train_label, (0~59까지의 값:int, skeleton_file_name)\n\n    # Note: actions labelled from A1 to A60 are contained in \"NTU RGB+D\" (https://rose1.ntu.edu.sg/dataset/actionRecognition/)\n    '''\n    label_name:str = ntu_label_annotations[SAMPLE_NUMBER]\n    label_name_no_blank:str = label_name.replace(\" \", \"_\").replace(\"/\", \"_\")\n    return label_name_no_blank\n\n# test \ncheck_label(sample_number=SAMPLE_NUMBER)   \n\n'eat_meal_snack'\n\n\n\n# PARAMS\n# target_joints = ['base of spine', 'middle of spine', 'spine', 'head','right elbow', 'left elbow', 'tip of left hand', 'tip of right hand','right foot', 'left foot']     # temp selection\ntarget_joints = ['base of spine', 'middle of spine', 'neck', 'head', \n                'left shoulder', 'left elbow', 'left wrist', 'left hand', \n                'right shoulder', 'right elbow', 'right wrist', 'right hand', \n                'left hip', 'left knee', 'left ankle', 'left foot', \n                'right hip', 'right knee', 'right ankle', 'right foot', \n                'spine',\n                'tip of left hand', 'left thumb', 'tip of right hand', 'right thumb']     # full joints (total 25)\nstart_frame, dontknow = 1, 0\nX, Y, Z = 0, 1, 2\n\njoint_numbers = [joint_annotations[target_joint] - 1 for target_joint in target_joints]\nXs = [SAMPLE[str(start_frame)][dontknow][joint_number][X] for joint_number in joint_numbers]\nYs = [SAMPLE[str(start_frame)][dontknow][joint_number][Y] for joint_number in joint_numbers]\nZs = [SAMPLE[str(start_frame)][dontknow][joint_number][Z] for joint_number in joint_numbers]\n\n\n# Canvas (CV2) initial setting\n# canvas : [ref](https://bkshin.tistory.com/entry/OpenCV-5-%EC%B0%BD-%EA%B4%80%EB%A6%AC-%EB%B0%8F-%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EC%B2%98%EB%A6%AC)\ncanvas_shape = (1080, 1920, 3)    # 임의 지정 가능, video 가 1920 * 1080 이므로 그 기준으로 설정\ncation_classes = 120            # ref: 3.1.2, 82 daily actions, 12 health-related actions, 26 mutual actions\nsubjects = 106                  # ref: 3.1.3 106 distinct subject\n\nimg = np.zeros((canvas_shape), np.uint8)    # uint : 부호 없는 정수 or 0을 포함하는 양수로 uint 또는 int 뒤에 오는 숫자는 bit의 개수를 의미한다. (uint8 : 2^8개의 정수 표현 가능, 0~255)\n\n\n# '''\n# 'base of spine', 'middle of spine', 'spine', 'head',\n# 'right elbow', 'left elbow', 'tip of left hand', 'tip of right hand',\n# 'right foot', 'left foot'\n# '''\n\n# base_of_spine     = (int(Xs[0]), int(Ys[0]))  # green\n# middle_of_spine   = (int(Xs[1]), int(Ys[1]))  # blue\n# spine             = (int(Xs[2]), int(Ys[2]))  # green\n# head              = (int(Xs[3]), int(Ys[3]))  # red\n# right_elbow       = (int(Xs[4]), int(Ys[4]))  # yellow\n# left_elbow        = (int(Xs[5]), int(Ys[5]))  # yellow\n# tip_of_left_hand  = (int(Xs[6]), int(Ys[6]))  # magenta\n# tip_of_right_hand = (int(Xs[7]), int(Ys[7]))  # magenta\n# right_foot        = (int(Xs[8]), int(Ys[8]))  # cyan\n# left_foot         = (int(Xs[9]), int(Ys[9]))  # cyan\n\n\n# draw line (temp selection / for 10 joints)\ndef draw_line_10(img, Xs, Ys):\n    ## Params\n    base_of_spine     = (int(Xs[0]), int(Ys[0]))  # green\n    middle_of_spine   = (int(Xs[1]), int(Ys[1]))  # blue\n    spine             = (int(Xs[2]), int(Ys[2]))  # green\n    head              = (int(Xs[3]), int(Ys[3]))  # red\n    right_elbow       = (int(Xs[4]), int(Ys[4]))  # yellow\n    left_elbow        = (int(Xs[5]), int(Ys[5]))  # yellow\n    tip_of_left_hand  = (int(Xs[6]), int(Ys[6]))  # magenta\n    tip_of_right_hand = (int(Xs[7]), int(Ys[7]))  # magenta\n    right_foot        = (int(Xs[8]), int(Ys[8]))  # cyan\n    left_foot         = (int(Xs[9]), int(Ys[9]))  # cyan\n    \n    \n    ## line\n    custom_thick = 3\n\n    result = cv2.line(img, pt1=base_of_spine,    pt2=middle_of_spine,    color=colors['green'],      thickness=custom_thick)  \n    result = cv2.line(img, pt1=middle_of_spine,  pt2=spine,              color=colors['blue'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=head,               color=colors['red'],        thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=right_elbow,        color=colors['cyan'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=spine,            pt2=left_elbow,         color=colors['cyan'],       thickness=custom_thick)  \n    result = cv2.line(img, pt1=right_elbow,      pt2=tip_of_right_hand,  color=colors['yellow'],     thickness=custom_thick)  \n    result = cv2.line(img, pt1=left_elbow,       pt2=tip_of_left_hand,   color=colors['yellow'],     thickness=custom_thick)  \n    result = cv2.line(img, pt1=base_of_spine,    pt2=left_foot,          color=colors['magenta'],    thickness=custom_thick)  \n    result = cv2.line(img, pt1=base_of_spine,    pt2=right_foot,         color=colors['magenta'],    thickness=custom_thick)  \n\n    return result\n\n\n\n# draw line (full joints: 25)\ndef draw_line(img, Xs, Ys):\n    ## Args\n    connects:List[Tuple[int]] = [(1, 2), (2, 21), (21, 3), (3, 4),\\\n                                (21, 9), (9, 10), (10, 11), (11, 12), (12, 24), (12, 25),\\\n                                (21, 5), (5,6), (6,7), (7,8), (8,22), (8,23),\\\n                                (1, 17), (17, 18), (18, 19), (19, 20),\\\n                                (1, 13), (13, 14), (14, 15), (15, 16)]  # 논문 기준 (1 ~ 25)\n    \n    rgbcodes = [colors['green'], colors['blue'], colors['red'], colors['cyan'], colors['yellow'], colors['magenta']] * 4\n    custom_thick = 3\n    for i, (idx1, idx2) in enumerate(connects):\n        # args\n        former:tuple = (int(Xs[idx1-1]), int(Ys[idx1-1]))   # list index에 맞게 idx1 -1\n        latter:tuple = (int(Xs[idx2-1]), int(Ys[idx2-1]))   # list index에 맞게 idx2 -1\n        \n        # draw\n        result = cv2.line(img, pt1=former, pt2=latter, color=rgbcodes[i], thickness=custom_thick)\n    return result\n\n# draw_line(img, Xs, Ys)\n\n\n## Local Environment ##\n# to prevent kernel crash\n\n# cv2.imshow('plotting canvas', img)      # plotting canvas 창에 이미지 표시\n# cv2.waitKey(0)                          # 아무 키나 누르면\n# cv2.destroyAllWindows()                 # 모든 창 닫기\n\n\n# GIF으로 만들 이미지 저장\nPATH_images = f\"./plotting/images/{args.file_name}/\"\nPATH_gifs = f\"./plotting/gifs/\"\n\nos.makedirs(PATH_images, exist_ok=True)\nos.makedirs(PATH_gifs, exist_ok=True)\n\n# 프레임 생성\nfor next_frame in range(0, SAMPLE.keys().__len__()):\n    # [NOTE] Global Var\n    # start frame = 1, dontknow = 0\n    # target_joints = ['base of spine', 'middle of spine', 'spine', 'head','right elbow', 'left elbow', 'tip of left hand', 'tip of right hand','right foot', 'left foot']     # temp selection\n    \n    Xs = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][X] for joint_number in joint_numbers] \n    Ys = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][Y] for joint_number in joint_numbers]\n    Zs = [SAMPLE[str(start_frame+next_frame)][dontknow][joint_number][Z] for joint_number in joint_numbers]\n    \n    # draw line\n    img = np.zeros((canvas_shape), np.uint8)    # 매번 초기화\n    result = draw_line(img, Xs, Ys)\n    rgb_result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n\n    try:\n        cv2.imwrite(f'{PATH_images}/{start_frame+next_frame}.jpg', result)\n    except: break\n\n\npath = [f\"{PATH_images}/{i}\" for i in os.listdir(PATH_images)]\npaths = [ Image.open(i) for i in path]\n\nimageio.mimsave(f'{PATH_gifs}/{args.file_name}_{check_label(sample_number=SAMPLE_NUMBER)}_{len(Xs)}.gif', paths, fps=5)\n\n\nNTU RGB+D\n\n\n\n\n\n\n\n임의의 joint 10개 추출\n25개 joint 전부 Plotting\n\n\n\n\nS001C001P001R001A001\ndrink water\n\n\n\n\n\n\nS001C001P001R001A002\neat_meal/snack\n\n\n\n\n\n\nS001C001P001R001A003\nbrushing_teeth"
  },
  {
    "objectID": "posts/Graph/GNN101.html",
    "href": "posts/Graph/GNN101.html",
    "title": "Graph 101",
    "section": "",
    "text": "denote\nmeaning\nnote\n\n\n\n\n\\(\\cal{G} = (\\cal{V, E})\\)\nformal definition of graph\ncalligraphic font used\n\n\n\\(\\cal{V}\\)\na set of nodes\nnode = vertex\n\n\n\\(\\cal{E}\\)\na set of edges, edges between nodes\n\\(\\therefore\\) edge needs coordinate-likely information to denote each one\n\n\n\\(u \\in \\cal{V}\\)\na node of node set \\(\\cal{V}\\)\nnormally node denoted by \\(\\cal{u}\\) or \\(\\cal{v}\\)\n\n\n\\((u, v) \\in \\cal{E}\\)\nwhen u, v in \\(\\cal{V}\\), it means an edge from u to v vice versa\ncheck it is direct graph or not"
  },
  {
    "objectID": "posts/Graph/GNN101.html#general-notation",
    "href": "posts/Graph/GNN101.html#general-notation",
    "title": "Graph 101",
    "section": "",
    "text": "denote\nmeaning\nnote\n\n\n\n\n\\(\\cal{G} = (\\cal{V, E})\\)\nformal definition of graph\ncalligraphic font used\n\n\n\\(\\cal{V}\\)\na set of nodes\nnode = vertex\n\n\n\\(\\cal{E}\\)\na set of edges, edges between nodes\n\\(\\therefore\\) edge needs coordinate-likely information to denote each one\n\n\n\\(u \\in \\cal{V}\\)\na node of node set \\(\\cal{V}\\)\nnormally node denoted by \\(\\cal{u}\\) or \\(\\cal{v}\\)\n\n\n\\((u, v) \\in \\cal{E}\\)\nwhen u, v in \\(\\cal{V}\\), it means an edge from u to v vice versa\ncheck it is direct graph or not"
  },
  {
    "objectID": "posts/Graph/GNN101.html#what-is-a-graph",
    "href": "posts/Graph/GNN101.html#what-is-a-graph",
    "title": "Graph 101",
    "section": "1.1 What is a graph",
    "text": "1.1 What is a graph\n그래프에서의 머신러닝을 논하기 이전에 ’그래프 데이터’가 정확히 무엇을 의미하는지 나타내는 공적인 표현에 대해 조금 알아둘 필요가 있다. 공식적으로, 그래프 \\(\\cal{G} = (\\cal{V, E})\\) 는 정점의 집합인 \\(\\cal{V}\\) 와 정점 사이의 간선의 집합인 \\(\\cal{E}\\) 로 정의된다. 정점 \\(u \\in \\cal{V}\\) 와 \\(v \\in \\cal{V}\\) 로 이루어진 간선은 다음과 같이 정의한다: \\((u, v) \\in \\cal{E}\\). 많은 경우에 (우리는) 단순 그래프 (simple graph) 만을 고려하는데, 단순 그래프는 대부분 하나의 간선이 각 정점의 쌍 사이에 존재하는 집합으로 어떤 간선도 정점 하나에 존재하지는 않는 그래프이다.\n그래프를 편리하게 표현하는 방법은 adjacency matrix (인접행렬) \\(A \\in \\mathbb{R}^{|\\cal{V}|*|\\cal{V}|}\\) 를 사용하는 방법이다.1 \\(A\\) 로 그래프를 표현하기 위해서는 그래프의 정점을 순서대로 배열함으로써 모든 정점의 색인들(indexes)이 인접행렬 \\(A\\)의 각각의 행과 열이 되게 해야 한다. 그렇게 하면 다음 조건의 행렬에서의 모든 간선의 존재를 표현할 수 있다: \\(A[u, v] = 1\\) if \\((u,v) \\in \\cal(E)\\) and \\(A[u, v] = 0\\) otherwise 2 만약 그래프가 방향이 없는 간선 (undirected edge)으로만 구성된 경우 인접행렬 \\(A\\)는 대칭행렬이 된다. 하지만 간선들이 방향성이 있다면 (edge direction matters) \\(A\\)는 대칭이지 않아도 된다. 몇몇 그래프들은 가중치를 가질 수도 있는데 (weighted edges) 그 경우 그래프에 기재되는 값이 {0, 1}이 아닌 임의의 실수가 된다. 예를 들어 가중 그래프 중 단백질간 상호작용 그래프는 두 단백질 사이의 연관된 힘을 나타내는 그래프로 쓰일 수 있다.\n\n\n\\(\\cal{V = V_1} \\cup \\cal{V_2} \\cup \\dots \\cup \\cal{V_k}\\) where \\(\\cal{V_i} \\cap \\cal{V_j} = \\emptyset, \\forall_i \\neq j\\)↩︎\n\\(A[u, v] = 1\\) if \\((u,v) \\in \\cal(E)\\) and \\(A[u, v] = 0\\) - \\(A[u, v] = 1\\): \\(u\\)와 \\(v\\) 사이에 간선이 존재할 때 (\\(A[u, v] = 0\\): 존재하지 않을 때) - \\((u, v)\\) 가 그래프 안에 있고 간선 집합 \\(\\cal{E}\\) 안에 존재할 때 \\(A\\)의 위치 \\((u, v)\\)에 올 수 있는 값은 간선이 존재하거나 (1) 존재하지 않는 (0) 두 가지 경우의 수 뿐이다.↩︎\n\n\n\n1.1.1 Multi-relational Graph\nmulti-relational graph는 방향이 있는 간선, 없는 간선, 가중치가 있는 간선을 넘어 다양한 종류의 간선이 있는 그래프를 고려한다. 예를 들어, 약물과 약물의 상호작용 그래프에서 각 간선이 두 약물을 동시에 복용할 때 발생할 수 있는 부작용에 대하여 서로 두가지의 간선이 필요할 수 있다. 이 예에서 간선 표기법을 확장하여 다음을 표현할 수 있다:\n간선 또는 관계 유형 \\(\\tau\\), (\\(u\\), \\(\\tau\\), \\(v\\)) \\(\\in\\) \\(\\cal{E}\\), 그리고 하나의 인접행렬 \\(A_{\\tau}\\) 를 간선 종류마다 정의할 수 있다. 이러한 그래프를 multi-relational 하다고 말하며 전체의 그래프는 인접 텐서 \\(\\cal{A} \\in \\mathbb{R}^{\\cal{|V| * |R| * |V|}}\\) 로 정의된다.3 multi-relational graph의 두가지 중요한 부분집합은 1. heterogenous, 2. multiplex 그래프로 나뉜다. tau (\\(\\tau\\))는 간선의 타입을 의미하며, 간선의 종류에는 위에 기술한 바와 같이 방향이 있는 것, 없는 것, 가중치가 있는 것 등이 포함된다. 이렇게 간선의 종류가 달라지면 Adjacency Matrix \\(A\\)도 따로 정의해야 한다.\n\n\n\\(\\cal{V = V_1} \\cup \\cal{V_2} \\cup \\dots \\cup \\cal{V_k}\\) where \\(\\cal{V_i} \\cap \\cal{V_j} = \\emptyset, \\forall_i \\neq j\\)↩︎\n\n\nHeterogeneous graph\nHeterogeneous graph에서, 정점들은 type에 물들어있다. 다시 말해, 정점 집합의 일부는 다음과 같이 해체될 수 있다: \\(\\cal{V = V_1} \\cup \\cal{V_2} \\cup \\dots \\cup \\cal{V_k}\\) where \\(\\cal{V_i} \\cap \\cal{V_j} = \\emptyset, \\forall_i \\neq j\\) 4. heterogeneous graph의 간선은 일반적으로 제한이 걸려있는데 이는 집합 \\(\\cal{V}\\)가 서로 겹치지 않는 부분집합 \\(\\cal{V_1}\\) 부터 \\(\\cal{V_k}\\)5 의 합집합이다.\n\n\n\\(\\cal{V = V_1} \\cup \\cal{V_2} \\cup \\dots \\cup \\cal{V_k}\\) where \\(\\cal{V_i} \\cap \\cal{V_j} = \\emptyset, \\forall_i \\neq j\\)↩︎\n\\(\\forall\\) : forall↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html",
    "href": "posts/LabHAI/tutorial/pytorch101.html",
    "title": "PyTorch101",
    "section": "",
    "text": "이전에 왜 텐서냐, 부터 시작해야 한다.\n왜 텐서일까?\n\n텐서란 무엇일까?\n\n텐서는 배열이나 행렬과 같은 특수 자료 구조. GPU에서 사용할 수 있도록 NumPy의 ndarray를 개량했다. 개념적으로는 배열(array)와 다를게 없다.\n정말로 다를게 없나?\n\n그렇다! 텐서와 넘파이배열은 종종 내부 메모리를 공유하며 서로 형태를 전환할 수 있기까지 하다.\n\n\n\n그러므로 다른 자료형과 구분되는 텐서의 특징은 GPU 사용이 가능하다는 점이다. 즉, 병렬연산에 최적화 되어있다는 것과 같다.\n그 빠른 병렬연산으로 하는 일이 automatic differentiation 즉, 자동 미분이다.\n\n\nTensor is optimized at automatic differentiation"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#왜-파이토치냐",
    "href": "posts/LabHAI/tutorial/pytorch101.html#왜-파이토치냐",
    "title": "PyTorch101",
    "section": "",
    "text": "이전에 왜 텐서냐, 부터 시작해야 한다.\n왜 텐서일까?\n\n텐서란 무엇일까?\n\n텐서는 배열이나 행렬과 같은 특수 자료 구조. GPU에서 사용할 수 있도록 NumPy의 ndarray를 개량했다. 개념적으로는 배열(array)와 다를게 없다.\n정말로 다를게 없나?\n\n그렇다! 텐서와 넘파이배열은 종종 내부 메모리를 공유하며 서로 형태를 전환할 수 있기까지 하다.\n\n\n\n그러므로 다른 자료형과 구분되는 텐서의 특징은 GPU 사용이 가능하다는 점이다. 즉, 병렬연산에 최적화 되어있다는 것과 같다.\n그 빠른 병렬연산으로 하는 일이 automatic differentiation 즉, 자동 미분이다.\n\n\nTensor is optimized at automatic differentiation"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#주요-내용",
    "href": "posts/LabHAI/tutorial/pytorch101.html#주요-내용",
    "title": "PyTorch101",
    "section": "2. 주요 내용",
    "text": "2. 주요 내용\n\n데이터셋 구축 코드와 학습 코드를 분리하는 것이 가독성 및 유지보수면에서 좋다.\n\n\n2.1. 데이터는 어떻게?\nPyTorch의 데이터셋 관리 방식은 독특하지만 편리하다. 데이터 작업을 위한 기본 요소 두가지가 존재하는데 이는 각각 DataLoader 와 Dataset이다. 1 데이터를 받아오는건 샘플(feature), 정답(label)으로 구성된 Dataset이고 각각의 값을 iterable한 객체로 감싸 접근하게 쉽게 만든 객체가 DataLoader다.\n\n\n모두 torch.utils.data 하위의 모듈이다.↩︎\n\n\n\nDataset: sample, label; 정답이 매칭된 데이터\n\nDataset을 직접 생성하는 경우도 있는데, 이후에 DataLoader로 사용하기 위해 다음 세가지 magic method를 구현해야 한다: __init__, __len__, __getitem__\n\nDataLoader iterable data; data를 minibatch에 전달하는 역할을 하며, 에폭마다 섞는 shuffle을 수행한다. PyTorch의 장점인 multiprocessing으로 속도 향상을 꾀할 수 있다.\n\n\n\n\n\n\n\n__init__\n\n\n\n\n\n\nDataset 객체가 구축될 때 한 번 실행되는 초기화 함수.\n예를 들어, 이미지 파일과 주석 파일이 포함된 디렉토리와 변형방법 (transform, target_transform) 을 초기화한다.\n\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None) -&gt; None:\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform\n\n\n\n\n\n\n\n\n\n__getitem__\n\n\n\n\n\n\nlen 으로 총 개수를 알았으니 인덱스로 값을 불러올 수 있다.\n계속해서 이미지를 예로 들었을 때, pd.readcsv 로 self.img_labels를 불러왔으니 DataFrame형태다.\n\n\n디스크에서 이미지 위치를 식별한다.\ntorchvision2 의 read_image method 를 이용해 이미지를 텐서로 변환 한다.\n__init__에서 정의한 self.img_labels 로 텐서로 변환된 이미지의 라벨 값을 호출한다.\n\n(선택) 필요시 transform 절차를 거친다. 3\n\n텐서 이미지와 라벨을 최종 형태인 dictionary 로 변환한다.\n\ndef __getitem__(self, idx) -&gt; dict:\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = read_image(img_path)\n    label = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        label = self.target_transform(label)\n    sample = {\"image\": image, \"label\": label}\n    return sample\n\n\n\n\n\ntorchvision.io↩︎\n왜 하필 이 단계에서 진행하는지는 target_transform을 확인할 것↩︎\n\n\n\n2.1.1. 활용은 이렇게 한다.\n\nfrom torch.utils.data import DataLoader로 데이터로더를 받아오고, 인자로 데이터셋과 배치사이즈를 전달한다.\nDataset마다 loader가 있어야 하니 train, test 모두 DataLoader로 받아와야한다.\n배치 자동화, 샘플링, 섞기 등 다양한 기능을 내부에서 제공한다.\n배치사이즈에 맞는 개수의 feature와, label을 묶은 객체의 요소 (batch) 를 반환한다.\n\n\niterable한 객체이므로 for문으로 간단하게 테스트 할 수 있다.\n\nfrom torch.utils.data import Dataset  # 이전 단계에서 정의한 학습용, 테스트용 데이터셋\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data:Dataset, batch_size:int=64, shuffle:bool=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n\n\n\n2.2. 모델은 어떻게?\nPyTorch의 모델들은 nn.Module 을 상속받는 클래스를 생성해서 정의한다. 모델을 구성하는 기본 요소가 이미 세팅되어있어 __init__에서 세팅만 하면 되니 편리하다.\n\n__init__ 함수에서 계층들을 정의하고\nforward 메서드에서 데이터를 전달하는 방식을 정한다.\n어떤 하드웨어(cpu, gpu, mps)를 사용할지 결정하는 것도 이 단계다.\n\n\n2.2.1. 예시 모델 확인\n\n\ntutorials.pytorch.py\n\n# 학습에 사용할 CPU나 GPU, MPS 장치를 얻습니다.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\n# 모델을 정의합니다.\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\n\n\n\n\n\n\nlogits?\n\n\n\n\n\n선형 딥러닝 모델의 최종 값이다.\n\nlog + odds\n정규화 되지 않은 로그 확률로, 모든 실수가 될 수 있다.\nlog-odds function은 \\(0~1\\) 사이의 값을 계산하는 시그모이드의 역함수다.4\n\nclassification을 진행할 때 중간 레이어에 무엇을 넣든 마지막에 FC layer로 마무리하는데 이때 노드를 분류하는 클래스의 수만큼 만든다. 즉, 이 FC layer를 통과한 결과가 특정 클래스에 해당할 가능성을 의미한다고 볼 수 있다. FC layer에 들어가기 전단계인 활성화 함수로는 Sigmoid, Softmax, ReLU 등을 사용한다. 활성화 함수에 넣기 전의 로그 확률을 logits라고 말하며 PyTorch에서는 실질적으로 이 값을 다룬다.\n\n역순으로 생각하면 쉽다.\n1. 분류 문제에서 각 클래스에 해당할 확률을 알아낸다.\n2. 그 확률은 활성화 함수라는 값을 통해 나온 값이다.5\n3. 위의 활성화 함수에 넣는 값이 logit 이다\n\n왜 최종 확률이 아니라 logits을 남겨두는가?\n활성화 함수를 통한 값, 확률을 보면 직관적인 이해도가 높아지지만 잠재적으로 값이 누수되거나 연산 과정에서 값이 누락될 위험이 잔존한다.6 활성화 함수를 통해 확률을 구하는 과정은 어렵지 않으므로 그 원형인 logits을 보존하는게 나은 선택이다. 그뿐 아니라 logit은 entropy 연산에서도 사용되므로 남겨두는 편이 활용도가 좋다.\n\n즉, logit을 남겨두는 이유를 아래로 정리할 수 있다.\n1. 정보의 손실을 막기 위해\n2. cross entropy loss 등의 loss 계열에 사용하기 위해\n\n\n\n\n\n\n@haje01↩︎\n활성화 함수는 확률을 연산하는 함수다.↩︎\n@KFrank↩︎\n\n\n\n\n\n\n\n\nbackward는?\n\n\n\n\n\n순전파와 역전파는 역할이 분리된 함수 각각에 포함되어 있다.\n\n순전파는 모델을 구성하는 방식이고\n역전파는 학습에서의 파라미터 ‘최적화’ 과정이다.\n\n.backward()로 구현되어 있다. 다음 단계인 train 함수에서 사용한다.\n\n\n\n\n\n\n\n\n\n\n\n굳이 model().to(device) 를 해줘야 하는 이유?\n\n\n\n\n\n\n기본적으로 텐서의 생성 위치는 CPU인데 tensor.to(‘cuda’) 를 통해 GPU로 텐서를 이동 할 수 있다.\n모든 텐서의 위치가 동일해야 연산을 할 수 있으니 코드 설계에 유의하자.\n\n\n\n\n\n\n\n2.3. 학습은 어떻게?\n텐서플로우에서 처럼 fit으로 끝나는게 아니라 train함수를 따로 정의해야했다. DataLoader는 학습에 필요한 데이터니 이때 모델이 학습할 수 있게 데이터를 넘겨주고 위에서 선언한 모델과 손실함수, 최적화 함수를 함께 전달한다. 실질적인 학습이 진행되는 곳이라 위에서 정의한 값들을 다 여기에 전달해주는게 맞다.\n\n최적화 단계: 하이퍼파라미터를 정의하고 학습하며 파라미터를 조정한다. (train_loop)\n\nepoch, batch size7, learning rate\n각 epoch마다 어떤 단계를 거칠 것인가, 모델 설계는 이쪽에 들어간다\n\n\n\n\nsize에 맞추어 batch를 넘겨줄 수 있는것도 DataLoader가 배치를 만들어주는 역할을 하기 때문이다.↩︎\n\n\n\n\n\n\n\n\n배치 정규화\n\n\n\n\n\n\n학습 데이터의 분포를 정하는 방식, 학습 단계에서만 사용한다.\n\n학습을 하면서 분포를 정하는 방식인 배치 정규화는 과적합 외에도 기울기 소실 및 폭주를 완화하여 학습을 안정적으로 할 수 있게 돕는데 가중치 \\(w\\) 가 커질 경우 다음 층에서 학습해야 하는 범위가 커진다. 따라서 학습 중 레이어 단위의 가중치 조절이 필요한데 이 역할이 배치 정규화다.\n\n\n\n\n검증 단계 (test_loop)\n\n손실함수: 모델 출력인 logit이 여기서 사용된다.\neval() 로 평가모드 전환 잊지 말 것\n\n\n\n\n\n\n\n\neval이 무엇인가\n\n\n\n\n\n\nevaluation의 약자로 모델을 평가하는 과정이다.\n모델을 평가모드로만 전환하는 단계다.\n\ndropout 비활성화\n배치정규화(의 이동평균, 이동분산) 업데이트 정지\n\n\n일관성 있는 결과를 얻을 수 있다. (모델 자체의 성능에 집중할 수 있다.)\n\n\n\n\n\n\n\ntutorials.pytorch.py\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # 예측(prediction)과 손실(loss) 계산\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # 역전파\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n\n\n2.4. 모델 관리는 어떻게?\n\n저장하고 불러오기\n모델의 형태를 포함하여 저장하고 불러오는 것을 목표로 한다.\n\n모델 전체를 불러오는 torch.save()\n\n\ntutorials.pytorch.py\n\n# 경로 지정\nPATH = \"entire_model.pt\"\n\n# 저장하기\ntorch.save(net, PATH)\n\n# 불러오기\nmodel = torch.load(PATH)\nmodel.eval()\n\n[권장] 매개변수만 저장하는 torch.save_dict()\n\n문법이 직관적이다.\n하지만 모델 저장시 사용한 클래스 밑 디렉토리 구조에 종속된다. (매개변수이니 어쩔 수 없다.)\n\n\n\ntutorials.pytorch.py\n\n# 경로 지정\nPATH = \"state_dict_model.pt\"\n\n# 저장하기\ntorch.save(net.state_dict(), PATH)\n\n# 불러오기\nmodel = Net()\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\n\n\n\n불러올 때에도 .eval() 과정을 거친단걸 잊지 말자!"
  },
  {
    "objectID": "posts/LabHAI/tutorial/pytorch101.html#전체-흐름",
    "href": "posts/LabHAI/tutorial/pytorch101.html#전체-흐름",
    "title": "PyTorch101",
    "section": "3. 전체 흐름",
    "text": "3. 전체 흐름\n\n데이터셋 구축\n\nDataset? DataLoader?\n값들이 모두 가속장치로 옮겨가 있는가?\n\n모델 구축\n\n__init__ 에 모델을 선언하고\n__forward__ 에 학습 과정을 정의했는가?\n\n하이퍼파라미터 정의 (학습)\n\n학습은 얼마나?\n배치 사이즈는 얼마나?\n학습률은 얼마나?\n\n최적화 단계는 어떻게? (학습)\n\n손실 함수는?\n최적화 함수는?\n\n모델 정의 방법?\n\n리팩토링 등을 예정하지 않고 있다면 torch.save_dict()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html",
    "title": "DCGAN face tutorial",
    "section": "",
    "text": "학습 데이터들의 분포를 학습해\n새로운 데이터를 생성\n\n\n생성자와 구분자로 구별되는 두 모델을 가지고 있다.\n\n생성자: 실제 이미지와 유사한 정교한 이미지 생성\n구분자: 실제 이미지인지 생성자의 이미지인지 판별\n\n생성자와 구분자의 상호작용을 통해 실제와 같은 이미지를 만들어낸다.\n구분자가 생성자의 이미지 중 50%를 제대로 판별할 때 균형상태에 도달했다고 말한다.\n\n\nDGGAN: GAN에서 파생된 모델로 생성자와 구분자에서 ’합성곱 신경망(Convolution)’과 ’전치 합성곱 신경망(Convolution-transpose)’를 사용했다\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn \nimport torch.nn.parallel\nimport torch.backends.mps as mps \n\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# seed\nmanualSeed = 42\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\nnp.random.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nRandom Seed:  42\n\n\n&lt;torch._C.Generator at 0x11820d070&gt;\n\n\n\n\n\n# dataset dir root path\ndataroot = \"./data/celeba\" \n\n# dataloader에서 사용할 thread 수\nworkers = 2\n\nbatch_size = 256\n\n# 이미지의 크기; 모든 이미지 변환하여 크기 통일함\nimage_size = 64\n\n# number channel; RGB이미지이므로 3\nnc = 3\n\n# latent vector size = 생성자의 입력값 크기\nnz = 100\n\n# 생성자를 통과하는 특징 데이터의 채널 크기\nngf = 64\n\n# 구분자를 통과하는 특징 데이터의 채널 크기\nndf = 64\n\nnum_epochs = 1\n\nlr = 0.0002\n\n# adam optimizer의 beta1 하이퍼파라미터\nbeta1 = 0.5\n\n# 사용가능한 gpu 번호; cpu = 0\nngpu = 0\n\n\n\n\n\ndataset = dset.ImageFolder(root=dataroot,\n                            transform=transforms.Compose([\n                                transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                            ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# 데이터가 너무 커서 샘플로 몇개만 뽑음\nsample_size = batch_size*1000\ndataset, _dataset = torch.utils.data.random_split(dataset, lengths=[sample_size, len(dataset)-sample_size])\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# device\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n\n# real batch check\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Image\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균 0, 분산 0.02인 정규분포\n\nmean = 0 stdev=0.02\n구분자와 생성자 모두 무작위 초기화 진행\n\n\n\n# 매개변수로 '모델'을 받아 합성곱, 전치합성곱, 배치정규화 계층을 초기화\n\ndef weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('ConV') != -1:\n        nn.init.normal_(model.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(model.weight.data, 1.0, 0.02)\n        nn.init.constant_(model.bias.data, 0)\n\n\n\n\n\n잠재공간 \\(z\\)를 데이터공간, 즉 이미지로 변환하는 과정을 거친다 (\\(3 \\times 64 \\times 64\\))\nstride 2를 가진 전치 합성곱 계층을 이어서 구성\n\n각 전치 합성곱 계층 하나당 아래 두개 레이어를 쌍으로 묶어 사용함\n\n2차 배치 정규화 계층\nrelu 활성화 함수\n\n\n마지막 출력 계층에서는 tanh 함수를 사용하는데 이는 출력값을 \\([-1, 1]\\) 사이로 조정하기 위함 이다.\n배치 정규화 계층이 중요한데, 이 계층이 경사 하강법의 흐름에 중요한 역할을 했다고 함 (논문 원문 참조)\n\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # ngf: 생성자를 통과하는 특징 데이터의 채널 크기\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0 ),     # 왜 ngf를 8배 하나?\n            nn.BatchNorm2d( ngf * 8 ),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d( ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)\n\n\nnetG = Generator(ngpu).to(device)\n\n# 모든 가중치의 평균을 0, 분산을 0.02로 초기화\nnetG.apply(weights_init)\n\n# 모델 구조 확인\nnetG\n\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n\n\n\n\n\n\n입력: \\(3 \\times 64 \\times 64\\)\nConv2d, BatchNorm2d, LeakyReLU로 데이터를 가공\n\nstride를 합성곱 계층에서 적용하는데 논문에서는 스스로 pooling을 학습하기 때문에 pooling 계층을 넣지 않아도 효과가 좋았다고 한다.\n\n마지막 계층 Sigmoid로 확률값 반환\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 입력 데이터의 크기는 ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n\nnetD = Discriminator(ngpu).to(device)\nnetD.apply(weights_init)\nnetD\n\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n\n\n\n\n\n손실함수로 BCELoss1 사용\n\\(\\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\\)\n\n여기서 조정해야하는 \\(log(D(x))\\) 와 \\(log(1 - D(G(Z)))\\) 를 확인할 수 있다.\n\n\n\n목표: \\(y\\)를 이용하여 손실함수를 최대화하는 방법 찾기\n\n\ncriterion = nn.BCELoss()\n\n# 입력, 잠재공간 벡터 생성\n# device = device 아니고 device만 넣으면 에러\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# 참 거짓의 라벨\nreal_label = 1\nfake_label = 0 \n\n# optimizer 생성\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\n\n\n\n\n손실함수를 어떻게 최대화 할 것인가?\n\n\n실제 데이터만 가져와 미니배치를 만든 후 훈련한다: \\(D\\) (학습)\n\n1의 결과로 얻은 \\(log(D(x))\\) 의 손실값을 계산한다. (수식 계산)\n역전파 과정에서의 변화도를 계산한다.\n\n\n이들 변화값은 축적(accumulate)시켜야 한다.\n이후 최적화 함수를 사용한다.\n\n\\(log(D(G(z)))\\) 를 최대화 하는 방식으로 학습2\n\n진짜 이미지들에서 G의 손실값을 구하고 가짜이미지에서도 같은 방식을 적용한다.\n이 과정에서 BCELoss의 일부인 \\(log(x)\\)를 일부 사용할 수 있다.\n\n\n\n실제값과 거짓값의 학습에서 나타나는 차이점을 통해 손실함수를 분리한다.\n\n튜토리얼에서는 다음과 같이 사용한다.\n\nLoss_D - 진짜 데이터와 가짜 데이터들 모두에서 구해진 손실값. (\\(log(D(x)) + log(1 - D(G(z)))\\)).\nLoss_G - 생성자의 손실값. \\(log(D(G(z)))\\)\nD(x) - 구분자가 데이터를 판별한 확률값입니다. 처음에는 1에 가까운 값이다가, G가 학습할수록 0.5값에 수렴하게 됩니다.\nD(G(z)) - 가짜데이터들에 대한 구분자의 출력값입니다. 처음에는 0에 가까운 값이다가, G가 학습할수록 0.5에 수렴하게 됩니다\n\n\n# 손실값들을 저장\n\nimg_list, G_losses, D_losses = [], [], []\niters = 0\n\n# `배치` 반복\ndef batch_loop(dataloader, netD, netG, criterion, epoch):\n    for i, data in enumerate(dataloader, 0):\n        ##################################\n        # (1) D 신경망 업데이트\n        # 실제 데이터로 학습\n        # 전체 손실함수 값을 최대화\n        ##################################\n\n        # --- 실제 데이터 학습 --- #\n        netD.zero_grad()\n\n        # 배치의 사이즈나 사용할 디바이스에 맞게 조정하는 이유? \n        # &gt;&gt; 효율성?\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label,\n                            dtype=torch.float, device=device)\n        \n        # 실제 데이터 투입\n        output = netD(real_cpu).view(-1)\n        # 손실값\n        errD_real = criterion(output, label)\n        # print(\"errD_real: \", errD_real)\n        # 역전파 하며 변화도 계산\n        errD_real.backward()\n        D_x = output.mean().item()\n        # print(\"D_x: \", D_x)\n        \n\n        # --- 가짜 데이터 학습 --- #\n        # 생성자에 사용할 잠재공간 벡터\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # 가짜 이미지 생성\n        fake = netG(noise)\n        # fill_ ?\n        label.fill_(fake_label)\n        \n        # D를 이용해 데이터 진위여부 판별\n        output = netD(fake.detach()).view(-1)\n        # D의 손실값 계산\n        errD_fake = criterion(output, label)\n        # 가짜 이미지의 변화도를 계산한 후 변화도에 더한다 (accumulate)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # 손실값들을 모두 더한다.\n        # errD는 학습 상태를 리포팅 할 때 사용한다! &gt; 임의로 생성한 결과니\n        errD = errD_real + errD_fake \n\n        # D업데이트\n        optimizerD.step()\n        # print(\"errD: \", errD)\n\n\n        ##################################\n        # (1) G 신경망 업데이트\n        # 실제 데이터를 생성\n        # log(D(G(z))) 값을 최대화\n        ##################################\n        netG.zero_grad()\n        label.fill_(real_label)     # 생성자의 손실값을 위해\n        # D를 제대로 업데이트했으므로 다시 가짜데이터 투입\n        output = netD(fake).view(-1)\n        # G의 손실값 계산\n        errG = criterion(output, label)\n        # G의 변화도 계산\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # G 업데이트\n        optimizerG.step()\n\n        # ---------------------------------\n        # 훈련 상태를 출력합니다\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                    % (epoch, num_epochs, i, len(dataloader),\n                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # 이후 그래프를 그리기 위해 손실값들을 저장해둡니다\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # fixed_noise를 통과시킨 G의 출력값을 저장해둡니다\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n\n# print(\"Starting Training Loop...\")\n# for epoch in range(num_epochs):\n#     batch_loop(dataloader, netD, netG, criterion, epoch)\n\n#     iters += 1\n\n'''\nStarting Training Loop...\n[0/1][0/792]    Loss_D: 1.3083  Loss_G: 3.2819  D(x): 0.5943    D(G(z)): 0.5336 / 0.0397\n[0/1][50/792]   Loss_D: 0.2891  Loss_G: 15.2257 D(x): 0.8146    D(G(z)): 0.0000 / 0.0000\n[0/1][100/792]  Loss_D: 0.2566  Loss_G: 4.2529  D(x): 0.8360    D(G(z)): 0.0341 / 0.0382\n[0/1][150/792]  Loss_D: 1.1949  Loss_G: 1.6857  D(x): 0.3761    D(G(z)): 0.0447 / 0.2113\n[0/1][200/792]  Loss_D: 0.9831  Loss_G: 2.3191  D(x): 0.4773    D(G(z)): 0.0473 / 0.1316\n[0/1][250/792]  Loss_D: 0.9653  Loss_G: 4.5867  D(x): 0.8587    D(G(z)): 0.5122 / 0.0142\n[0/1][300/792]  Loss_D: 0.7742  Loss_G: 2.0908  D(x): 0.5511    D(G(z)): 0.0705 / 0.1749\n[0/1][350/792]  Loss_D: 0.7144  Loss_G: 5.1598  D(x): 0.8541    D(G(z)): 0.3957 / 0.0082\n[0/1][400/792]  Loss_D: 1.8883  Loss_G: 1.6982  D(x): 0.2203    D(G(z)): 0.0174 / 0.2421\n[0/1][450/792]  Loss_D: 0.7316  Loss_G: 5.0640  D(x): 0.8330    D(G(z)): 0.3798 / 0.0097\n[0/1][500/792]  Loss_D: 0.4904  Loss_G: 3.8918  D(x): 0.8738    D(G(z)): 0.2750 / 0.0271\n[0/1][550/792]  Loss_D: 0.3976  Loss_G: 3.4012  D(x): 0.7631    D(G(z)): 0.0879 / 0.0493\n[0/1][600/792]  Loss_D: 1.3403  Loss_G: 7.2067  D(x): 0.9398    D(G(z)): 0.6828 / 0.0012\n[0/1][650/792]  Loss_D: 0.5217  Loss_G: 3.1440  D(x): 0.8256    D(G(z)): 0.2559 / 0.0536\n[0/1][700/792]  Loss_D: 0.3997  Loss_G: 3.3902  D(x): 0.8364    D(G(z)): 0.1774 / 0.0445\n[0/1][750/792]  Loss_D: 0.4414  Loss_G: 3.4481  D(x): 0.8536    D(G(z)): 0.2275 / 0.0386\n'''\n\n\n\n\n\n\n# dataloader에서 진짜 데이터들을 가져옵니다\nreal_batch = next(iter(dataloader))\n\n# 진짜 이미지들을 화면에 출력합니다\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# 가짜 이미지들을 화면에 출력합니다\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#gan",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#gan",
    "title": "DCGAN face tutorial",
    "section": "",
    "text": "학습 데이터들의 분포를 학습해\n새로운 데이터를 생성\n\n\n생성자와 구분자로 구별되는 두 모델을 가지고 있다.\n\n생성자: 실제 이미지와 유사한 정교한 이미지 생성\n구분자: 실제 이미지인지 생성자의 이미지인지 판별\n\n생성자와 구분자의 상호작용을 통해 실제와 같은 이미지를 만들어낸다.\n구분자가 생성자의 이미지 중 50%를 제대로 판별할 때 균형상태에 도달했다고 말한다.\n\n\nDGGAN: GAN에서 파생된 모델로 생성자와 구분자에서 ’합성곱 신경망(Convolution)’과 ’전치 합성곱 신경망(Convolution-transpose)’를 사용했다\n\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn \nimport torch.nn.parallel\nimport torch.backends.mps as mps \n\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\n# seed\nmanualSeed = 42\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\nnp.random.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nRandom Seed:  42\n\n\n&lt;torch._C.Generator at 0x11820d070&gt;\n\n\n\n\n\n# dataset dir root path\ndataroot = \"./data/celeba\" \n\n# dataloader에서 사용할 thread 수\nworkers = 2\n\nbatch_size = 256\n\n# 이미지의 크기; 모든 이미지 변환하여 크기 통일함\nimage_size = 64\n\n# number channel; RGB이미지이므로 3\nnc = 3\n\n# latent vector size = 생성자의 입력값 크기\nnz = 100\n\n# 생성자를 통과하는 특징 데이터의 채널 크기\nngf = 64\n\n# 구분자를 통과하는 특징 데이터의 채널 크기\nndf = 64\n\nnum_epochs = 1\n\nlr = 0.0002\n\n# adam optimizer의 beta1 하이퍼파라미터\nbeta1 = 0.5\n\n# 사용가능한 gpu 번호; cpu = 0\nngpu = 0\n\n\n\n\n\ndataset = dset.ImageFolder(root=dataroot,\n                            transform=transforms.Compose([\n                                transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                            ]))\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# 데이터가 너무 커서 샘플로 몇개만 뽑음\nsample_size = batch_size*1000\ndataset, _dataset = torch.utils.data.random_split(dataset, lengths=[sample_size, len(dataset)-sample_size])\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                            shuffle=True, num_workers=0)\n\n# device\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n\n# real batch check\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Image\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n평균 0, 분산 0.02인 정규분포\n\nmean = 0 stdev=0.02\n구분자와 생성자 모두 무작위 초기화 진행\n\n\n\n# 매개변수로 '모델'을 받아 합성곱, 전치합성곱, 배치정규화 계층을 초기화\n\ndef weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('ConV') != -1:\n        nn.init.normal_(model.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(model.weight.data, 1.0, 0.02)\n        nn.init.constant_(model.bias.data, 0)\n\n\n\n\n\n잠재공간 \\(z\\)를 데이터공간, 즉 이미지로 변환하는 과정을 거친다 (\\(3 \\times 64 \\times 64\\))\nstride 2를 가진 전치 합성곱 계층을 이어서 구성\n\n각 전치 합성곱 계층 하나당 아래 두개 레이어를 쌍으로 묶어 사용함\n\n2차 배치 정규화 계층\nrelu 활성화 함수\n\n\n마지막 출력 계층에서는 tanh 함수를 사용하는데 이는 출력값을 \\([-1, 1]\\) 사이로 조정하기 위함 이다.\n배치 정규화 계층이 중요한데, 이 계층이 경사 하강법의 흐름에 중요한 역할을 했다고 함 (논문 원문 참조)\n\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # ngf: 생성자를 통과하는 특징 데이터의 채널 크기\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0 ),     # 왜 ngf를 8배 하나?\n            nn.BatchNorm2d( ngf * 8 ),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d( ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            # 위의 계층을 통과한 데이터의 크기. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n    \n    def forward(self, input):\n        return self.main(input)\n\n\nnetG = Generator(ngpu).to(device)\n\n# 모든 가중치의 평균을 0, 분산을 0.02로 초기화\nnetG.apply(weights_init)\n\n# 모델 구조 확인\nnetG\n\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n\n\n\n\n\n\n입력: \\(3 \\times 64 \\times 64\\)\nConv2d, BatchNorm2d, LeakyReLU로 데이터를 가공\n\nstride를 합성곱 계층에서 적용하는데 논문에서는 스스로 pooling을 학습하기 때문에 pooling 계층을 넣지 않아도 효과가 좋았다고 한다.\n\n마지막 계층 Sigmoid로 확률값 반환\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # 입력 데이터의 크기는 ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 위의 계층을 통과한 데이터의 크기. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n\nnetD = Discriminator(ngpu).to(device)\nnetD.apply(weights_init)\nnetD\n\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n\n\n\n\n\n손실함수로 BCELoss1 사용\n\\(\\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\\)\n\n여기서 조정해야하는 \\(log(D(x))\\) 와 \\(log(1 - D(G(Z)))\\) 를 확인할 수 있다.\n\n\n\n목표: \\(y\\)를 이용하여 손실함수를 최대화하는 방법 찾기\n\n\ncriterion = nn.BCELoss()\n\n# 입력, 잠재공간 벡터 생성\n# device = device 아니고 device만 넣으면 에러\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# 참 거짓의 라벨\nreal_label = 1\nfake_label = 0 \n\n# optimizer 생성\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\n\n\n\n\n손실함수를 어떻게 최대화 할 것인가?\n\n\n실제 데이터만 가져와 미니배치를 만든 후 훈련한다: \\(D\\) (학습)\n\n1의 결과로 얻은 \\(log(D(x))\\) 의 손실값을 계산한다. (수식 계산)\n역전파 과정에서의 변화도를 계산한다.\n\n\n이들 변화값은 축적(accumulate)시켜야 한다.\n이후 최적화 함수를 사용한다.\n\n\\(log(D(G(z)))\\) 를 최대화 하는 방식으로 학습2\n\n진짜 이미지들에서 G의 손실값을 구하고 가짜이미지에서도 같은 방식을 적용한다.\n이 과정에서 BCELoss의 일부인 \\(log(x)\\)를 일부 사용할 수 있다.\n\n\n\n실제값과 거짓값의 학습에서 나타나는 차이점을 통해 손실함수를 분리한다.\n\n튜토리얼에서는 다음과 같이 사용한다.\n\nLoss_D - 진짜 데이터와 가짜 데이터들 모두에서 구해진 손실값. (\\(log(D(x)) + log(1 - D(G(z)))\\)).\nLoss_G - 생성자의 손실값. \\(log(D(G(z)))\\)\nD(x) - 구분자가 데이터를 판별한 확률값입니다. 처음에는 1에 가까운 값이다가, G가 학습할수록 0.5값에 수렴하게 됩니다.\nD(G(z)) - 가짜데이터들에 대한 구분자의 출력값입니다. 처음에는 0에 가까운 값이다가, G가 학습할수록 0.5에 수렴하게 됩니다\n\n\n# 손실값들을 저장\n\nimg_list, G_losses, D_losses = [], [], []\niters = 0\n\n# `배치` 반복\ndef batch_loop(dataloader, netD, netG, criterion, epoch):\n    for i, data in enumerate(dataloader, 0):\n        ##################################\n        # (1) D 신경망 업데이트\n        # 실제 데이터로 학습\n        # 전체 손실함수 값을 최대화\n        ##################################\n\n        # --- 실제 데이터 학습 --- #\n        netD.zero_grad()\n\n        # 배치의 사이즈나 사용할 디바이스에 맞게 조정하는 이유? \n        # &gt;&gt; 효율성?\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label,\n                            dtype=torch.float, device=device)\n        \n        # 실제 데이터 투입\n        output = netD(real_cpu).view(-1)\n        # 손실값\n        errD_real = criterion(output, label)\n        # print(\"errD_real: \", errD_real)\n        # 역전파 하며 변화도 계산\n        errD_real.backward()\n        D_x = output.mean().item()\n        # print(\"D_x: \", D_x)\n        \n\n        # --- 가짜 데이터 학습 --- #\n        # 생성자에 사용할 잠재공간 벡터\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # 가짜 이미지 생성\n        fake = netG(noise)\n        # fill_ ?\n        label.fill_(fake_label)\n        \n        # D를 이용해 데이터 진위여부 판별\n        output = netD(fake.detach()).view(-1)\n        # D의 손실값 계산\n        errD_fake = criterion(output, label)\n        # 가짜 이미지의 변화도를 계산한 후 변화도에 더한다 (accumulate)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # 손실값들을 모두 더한다.\n        # errD는 학습 상태를 리포팅 할 때 사용한다! &gt; 임의로 생성한 결과니\n        errD = errD_real + errD_fake \n\n        # D업데이트\n        optimizerD.step()\n        # print(\"errD: \", errD)\n\n\n        ##################################\n        # (1) G 신경망 업데이트\n        # 실제 데이터를 생성\n        # log(D(G(z))) 값을 최대화\n        ##################################\n        netG.zero_grad()\n        label.fill_(real_label)     # 생성자의 손실값을 위해\n        # D를 제대로 업데이트했으므로 다시 가짜데이터 투입\n        output = netD(fake).view(-1)\n        # G의 손실값 계산\n        errG = criterion(output, label)\n        # G의 변화도 계산\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # G 업데이트\n        optimizerG.step()\n\n        # ---------------------------------\n        # 훈련 상태를 출력합니다\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                    % (epoch, num_epochs, i, len(dataloader),\n                        errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # 이후 그래프를 그리기 위해 손실값들을 저장해둡니다\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # fixed_noise를 통과시킨 G의 출력값을 저장해둡니다\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n\n# print(\"Starting Training Loop...\")\n# for epoch in range(num_epochs):\n#     batch_loop(dataloader, netD, netG, criterion, epoch)\n\n#     iters += 1\n\n'''\nStarting Training Loop...\n[0/1][0/792]    Loss_D: 1.3083  Loss_G: 3.2819  D(x): 0.5943    D(G(z)): 0.5336 / 0.0397\n[0/1][50/792]   Loss_D: 0.2891  Loss_G: 15.2257 D(x): 0.8146    D(G(z)): 0.0000 / 0.0000\n[0/1][100/792]  Loss_D: 0.2566  Loss_G: 4.2529  D(x): 0.8360    D(G(z)): 0.0341 / 0.0382\n[0/1][150/792]  Loss_D: 1.1949  Loss_G: 1.6857  D(x): 0.3761    D(G(z)): 0.0447 / 0.2113\n[0/1][200/792]  Loss_D: 0.9831  Loss_G: 2.3191  D(x): 0.4773    D(G(z)): 0.0473 / 0.1316\n[0/1][250/792]  Loss_D: 0.9653  Loss_G: 4.5867  D(x): 0.8587    D(G(z)): 0.5122 / 0.0142\n[0/1][300/792]  Loss_D: 0.7742  Loss_G: 2.0908  D(x): 0.5511    D(G(z)): 0.0705 / 0.1749\n[0/1][350/792]  Loss_D: 0.7144  Loss_G: 5.1598  D(x): 0.8541    D(G(z)): 0.3957 / 0.0082\n[0/1][400/792]  Loss_D: 1.8883  Loss_G: 1.6982  D(x): 0.2203    D(G(z)): 0.0174 / 0.2421\n[0/1][450/792]  Loss_D: 0.7316  Loss_G: 5.0640  D(x): 0.8330    D(G(z)): 0.3798 / 0.0097\n[0/1][500/792]  Loss_D: 0.4904  Loss_G: 3.8918  D(x): 0.8738    D(G(z)): 0.2750 / 0.0271\n[0/1][550/792]  Loss_D: 0.3976  Loss_G: 3.4012  D(x): 0.7631    D(G(z)): 0.0879 / 0.0493\n[0/1][600/792]  Loss_D: 1.3403  Loss_G: 7.2067  D(x): 0.9398    D(G(z)): 0.6828 / 0.0012\n[0/1][650/792]  Loss_D: 0.5217  Loss_G: 3.1440  D(x): 0.8256    D(G(z)): 0.2559 / 0.0536\n[0/1][700/792]  Loss_D: 0.3997  Loss_G: 3.3902  D(x): 0.8364    D(G(z)): 0.1774 / 0.0445\n[0/1][750/792]  Loss_D: 0.4414  Loss_G: 3.4481  D(x): 0.8536    D(G(z)): 0.2275 / 0.0386\n'''\n\n\n\n\n\n\n# dataloader에서 진짜 데이터들을 가져옵니다\nreal_batch = next(iter(dataloader))\n\n# 진짜 이미지들을 화면에 출력합니다\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# 가짜 이미지들을 화면에 출력합니다\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()"
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#footnotes",
    "href": "posts/LabHAI/tutorial/GAN/DCGAN_tutorial.html#footnotes",
    "title": "DCGAN face tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBinary Cross Entropy loss↩︎\n\\(log(1 - D(G(z)))\\) 최소화는 원문 논문에서 효율적이지 못하다고 언급함↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html",
    "href": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html",
    "title": "finetuning tutorial",
    "section": "",
    "text": "pytorch                               2.2.1-py3.10_0\n  torchaudio                            2.2.1-py310_cpu\n  torchvision                           0.17.1-py310_cpu\nimport torchvision\n\n# FastRCNN: https://arxiv.org/abs/1506.01497\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
  },
  {
    "objectID": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html#footnotes",
    "href": "posts/LabHAI/tutorial/maskrcnn/torchvision_finetuning_instance_segmentation.html#footnotes",
    "title": "finetuning tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n보행자↩︎\nConvolution에서 kernel이 특징을 추출하는 과정과 비슷해보이기도 한다. 실제로 개념은 같다.↩︎\ne.g. selective search, edge boxes↩︎\n이 다양성이 다양한 bbox를 생성하는데 영향을 미치는 것 같다.↩︎\n비효율적이라면 sliding window를 쓰면 됐다.↩︎\n‘the creation’?↩︎\nCPU 혹은 GPU 중 하나의 리소스가 부족한 경우 느려지는 현상.↩︎\n2012년에 발표된 논문에서 언급한 병목현상이니 해당 알고리즘은 CPU에서 동작할 것으로 보인다.↩︎\nlocalize approximately↩︎\nlocalize by edging↩︎\n논문에서는 3x3 filter 사용↩︎\nbox중앙, x좌표, y좌표, height, width↩︎\n그래서 RPN에서 feature map을 사용할 수 있었다.↩︎\nsegmentation 보충↩︎"
  },
  {
    "objectID": "posts/techniques/zero_shot.html",
    "href": "posts/techniques/zero_shot.html",
    "title": "Zero-shot Learning",
    "section": "",
    "text": "딥러닝의 성능은 데이터의 질, 양과 정비례하는데 데이터 수집의 어려움, 레이블링의 까다로움, 레이블링의 시간 및 금전적 비용 부담으로 더 이상 데이터에 의존하기엔 어려운 단계에 이르렀다. 이러한 수렴단계에서 데이터의 절대량에 구애받지 않는 모델의 중요성이 대두되었고 데이터의 context를 읽어내는 이른바 meta learning이 발전하게 되었다. Zero-shot Learning은 그 중 한 방법론으로 한 번도 본 적 없는 레이블을 구분해 낼 수 있는 모델이다.\n\nZero-shot learning 이란: label이 지정된 소수의 클래스 집합 데이터와 클래스에 대한 ‘추가 정보만을’ 사용하여 한 번도 본 적 없는 많은 클래스까지 잘 예측하도록 학습한 모델.\n\n그렇다면 어떤 추가정보를 사용해 학습했기에 본 적 없는 데이터까지 추론할 수 있을까? 얼룩말을 학습한 모델에게 호랑이 이미지를 주고 어떤 동물인지 알아내라는 질문을 했다고 하자.\n\n\n \n\n\n얼룩말을 학습한 모델이 호랑이를 본다면 먼저 얼룩말에서 학습한 특성을 살필 것이다. 꼬리가 있는가, 줄무늬가 있는가, 갈기가 있는가. 이러한 정보를 살핀 모델은 호랑이를 ‘꼬리가 있고 검은 줄무늬가 있으나 갈기는 없는 주황색 가죽을 가진 동물’ 로 설명할 수 있을 것이다. 기존처럼 이미지 모델을 이용해 분류 문제를 풀면 이 동물이 얼룩말이 아니라는 결론만 얻을 수 있지만 Zero-shot learning은 이미지를 묘사한 context를 얻을 수 있음에 주목한다. 이렇게 얻어낸 호랑이를 설명한 묘사를 언어 모델에 넣는다면 어떨까? 언어모델은 위키피디아를 포함한 다량의 언어를 학습한 모델이므로, 이미지 모델에서 동물의 종류를 찾기엔 부족했던 ‘갈기가 없고 주황색 가죽을 가진’ 특징을 찾아낼 수 있을 것이다. 주황색 바탕에 검은 줄무늬를 가진 얼룩말 크기의 동물은 무엇일까? 호랑이다.\n이렇게 이미지 모델에서 이미지의 특성을 찾아내고, 그 특성을 언어모델에 물어 학습하지 않았던 label을 분류하는 모델이 Zero-shot model이다.\n그런데 이미지 모델에서 이미지의 특성은 어떻게 찾아낼까?\n\n\n\n\n\n이미지 모델은 이미지를 이미지가 아닌 행렬로 받아들인다. 이미지의 특성은 숫자로 표현되어 행렬이 되는데, 이렇게 특성을 반영하는 방식을 ’(semantic) embedding’이라고 한다. 임베딩 된 두 이미지를 비교하면 공통된 특성은 유사한 숫자로 표현되어 있을 것이다. 이렇게 학습되지 않은 특성도 임베딩 벡터 값으로 보존할 수 있다.\n\n\n\n\n\n이렇게 Zero-shot learning의 직관을 얻을 수 있었다. 추후 모델이 작용하는 구체적인 기작과 Meta Learning에 대해 알아보도록 하겠다.\n\n\n\n\n\n\nFew-shot learning task with meta-learning\n\n\n\n\n\nDMQM 연구실의 세미나 기록은 청자들의 후기에서도 사고를 확장할 수 있는데 이번에 눈에 들어온 부분은 아래와 같다.\n\nFew-shot learning task with meta-learning Meta learning은 Meta training(경험을 쌓고)과 Meta testing(관심 대상의 소수데이터로 다수데이터를 잘 예측)으로 이루어진다. Meta training을 위해 확보된 데이터 셋에서 여러 개의 과업을 나눈다. 즉, 각 과업은 예측하고자 하는 클래스, 소수데이터, 다수데이터가 서로 다르다. 이렇게 여러 과업으로 나누어 학습하는 방법을 에피소딕 학습(episodic training)으로 말한다. 이렇게 얻어낸 “경험”으로 부터 관심 대상이 되는 과업(task new)을 잘 수행해야 하는데 어떤 “경험”을 반영할 지 선택하는 게 중요하다.\n\n메타러닝은 배우는 방법을 배우는 방법이라고 간략하게 알고 있다. 조금 더 사람의 이해에 가까운 학습 방법으로 느껴져 흥미가 간다."
  },
  {
    "objectID": "posts/NLP/LoRA.html",
    "href": "posts/NLP/LoRA.html",
    "title": "test",
    "section": "",
    "text": "사전 학습된 모델 가중치를 동결하고 학습 가능한 순위 분해 행렬을 트랜스포머 아키텍처의 각 계층에 주입함으로써 LLM이 제기하는 비용 및 효율성 문제에 대한 해결책을 제시한다. 이 혁신적인 접근 방식은 다운스트림 작업에서 학습 가능한 파라미터의 수를 획기적으로 줄여 GPU 메모리 요구 사항을 크게 줄이고 학습 처리량을 개선한다. 여기서 ’Low Rank matrix’는 왜 중요할까? ’extensive deep learning model’의 가중치 행렬이 low rank matrix에 존재한다는 연구결과가 있었기 때문이다.\n\ne.g. 1000차원의 임베딩 벡터가 있다고 하자. 이렇게 하면 \\(1000x1000\\) 차원의 K, Q, V 행렬이 생성되며, 각각 \\(10^3 * 10^3 = 10^6\\) 개의 훈련 가능한 파라미터가 생성된다. 반면 이를 low rank matrix로 압축시키면 학습 가능한 파라미터는 20000개로 줄어든다. 따라서 LLM의 목표는 이 행렬들을 low rank로 압축하여 학습해야하는 파라미터의 수를 줄이는 것이다. (그림 참고)\n\n\n\n\n\n\n\n\n\n\n\nOur reparametrization. We only train A and B. For above example r=8 and d =1000.\n\n\n\nLLM에서의 fine tuning은 모델 내의 모든 가중치 행렬을 또 다른 가중치 행렬로 이동하는 과정으로 이해될 수 있는데 base model의 안정성을 위해 가중치행렬을 동결하고 (freeze) \\(W\\) 행렬을 두개의 low rank matrix인 \\(A\\), \\(B\\) 로 분해하는 과정을 거친다. 이 과정에서 가중치 행렬을 정확하게 찾아낼 수 있다면 좋겠지만 찾아내는 과정 또한 연산량에 포함된다. 그러므로 r 파라미터로 가중치 행렬이 있을만한 ’적당히 작은 랭크의 행렬’으로 정하고 근사화한다. (LoRA)\n\n\n\nfrom peft import PeftModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"bigcode/starcoder\",\n        use_auth_token=True,\n        device_map={\"\": Accelerator().process_index},\n    )\n\n\n# lora hyperparameters\nlora_config = LoraConfig(r=8,target_modules = [\"c_proj\", \"c_attn\", \"q_attn\"])\n\n\nmodel = get_peft_model(model, lora_config)\ntraining_args = TrainingArguments(\n    ...\n)\n\ntrainer = Trainer(model=model, args=training_args, \n                  train_dataset=train_data, eval_dataset=val_data)\n\nprint(\"Training...\")\ntrainer.train()\n\n# plugging the adapter into basemodel back\nmodel = PeftModel.from_pretrained(\"bigcode/starcoder\", peft_model_path)\n\n\n\n\n\nLoRA : https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "posts/NLP/LoRA.html#lora",
    "href": "posts/NLP/LoRA.html#lora",
    "title": "test",
    "section": "",
    "text": "사전 학습된 모델 가중치를 동결하고 학습 가능한 순위 분해 행렬을 트랜스포머 아키텍처의 각 계층에 주입함으로써 LLM이 제기하는 비용 및 효율성 문제에 대한 해결책을 제시한다. 이 혁신적인 접근 방식은 다운스트림 작업에서 학습 가능한 파라미터의 수를 획기적으로 줄여 GPU 메모리 요구 사항을 크게 줄이고 학습 처리량을 개선한다. 여기서 ’Low Rank matrix’는 왜 중요할까? ’extensive deep learning model’의 가중치 행렬이 low rank matrix에 존재한다는 연구결과가 있었기 때문이다.\n\ne.g. 1000차원의 임베딩 벡터가 있다고 하자. 이렇게 하면 \\(1000x1000\\) 차원의 K, Q, V 행렬이 생성되며, 각각 \\(10^3 * 10^3 = 10^6\\) 개의 훈련 가능한 파라미터가 생성된다. 반면 이를 low rank matrix로 압축시키면 학습 가능한 파라미터는 20000개로 줄어든다. 따라서 LLM의 목표는 이 행렬들을 low rank로 압축하여 학습해야하는 파라미터의 수를 줄이는 것이다. (그림 참고)\n\n\n\n\n\n\n\n\n\n\n\nOur reparametrization. We only train A and B. For above example r=8 and d =1000.\n\n\n\nLLM에서의 fine tuning은 모델 내의 모든 가중치 행렬을 또 다른 가중치 행렬로 이동하는 과정으로 이해될 수 있는데 base model의 안정성을 위해 가중치행렬을 동결하고 (freeze) \\(W\\) 행렬을 두개의 low rank matrix인 \\(A\\), \\(B\\) 로 분해하는 과정을 거친다. 이 과정에서 가중치 행렬을 정확하게 찾아낼 수 있다면 좋겠지만 찾아내는 과정 또한 연산량에 포함된다. 그러므로 r 파라미터로 가중치 행렬이 있을만한 ’적당히 작은 랭크의 행렬’으로 정하고 근사화한다. (LoRA)\n\n\n\nfrom peft import PeftModel\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, set_peft_model_state_dict\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"bigcode/starcoder\",\n        use_auth_token=True,\n        device_map={\"\": Accelerator().process_index},\n    )\n\n\n# lora hyperparameters\nlora_config = LoraConfig(r=8,target_modules = [\"c_proj\", \"c_attn\", \"q_attn\"])\n\n\nmodel = get_peft_model(model, lora_config)\ntraining_args = TrainingArguments(\n    ...\n)\n\ntrainer = Trainer(model=model, args=training_args, \n                  train_dataset=train_data, eval_dataset=val_data)\n\nprint(\"Training...\")\ntrainer.train()\n\n# plugging the adapter into basemodel back\nmodel = PeftModel.from_pretrained(\"bigcode/starcoder\", peft_model_path)\n\n\n\n\n\nLoRA : https://medium.com/@Shrishml/lora-low-rank-adaptation-from-the-first-principle-7e1adec71541\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "posts/NLP/huggingface/tips.html",
    "href": "posts/NLP/huggingface/tips.html",
    "title": "tips",
    "section": "",
    "text": "다운로드 받은 모델 삭제하는 방법 (cache cli에서 지우기)\n➜ huggingface-cli scan-cache\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS                LOCAL PATH\n--------------------------- --------- ------------ -------- ------------- ------------- ------------------- -------------------------------------------------------------------------\nglue                        dataset         116.3K       15 4 days ago    4 days ago    2.4.0, main, 1.17.0 /home/wauplin/.cache/huggingface/hub/datasets--glue\ngoogle/fleurs               dataset          64.9M        6 1 week ago    1 week ago    refs/pr/1, main     /home/wauplin/.cache/huggingface/hub/datasets--google--fleurs\nJean-Baptiste/camembert-ner model           441.0M        7 2 weeks ago   16 hours ago  main                /home/wauplin/.cache/huggingface/hub/models--Jean-Baptiste--camembert-ner\nbert-base-cased             model             1.9G       13 1 week ago    2 years ago                       /home/wauplin/.cache/huggingface/hub/models--bert-base-cased\nt5-base                     model            10.1K        3 3 months ago  3 months ago  main                /home/wauplin/.cache/huggingface/hub/models--t5-base\nt5-small                    model           970.7M       11 3 days ago    3 days ago    refs/pr/1, main     /home/wauplin/.cache/huggingface/hub/models--t5-small\n\nDone in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\nGot 1 warning(s) while scanning. Use -vvv to print details.\n삭제 : rm -r {LOCAL PATH}\nref : https://huggingface.co/docs/huggingface_hub/guides/manage-cache"
  },
  {
    "objectID": "posts/NLP/ptuning.html",
    "href": "posts/NLP/ptuning.html",
    "title": "p tuning",
    "section": "",
    "text": "p tuning\n“P-tuning is a method for automatically searching and optimizing for better prompts in a continuous space.”\n\nPLM의 일부 가중치만 미세조정하여 continuous prompt emobedding만 tuninng한다.\n\n\n\nReference\n\nhttps://huggingface.co/blog/peft"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": " ",
    "section": "",
    "text": "Focusing on how technology, especially AI, can solve blind spots in society.\nSolve what no one wants to do and hardly achieve only by manpower.\n\nContact: 4923.py@gmail.com\nPresent: Internship in Lab HAI, HUFS"
  },
  {
    "objectID": "CV.html#yeeun-hong",
    "href": "CV.html#yeeun-hong",
    "title": " ",
    "section": "",
    "text": "Focusing on how technology, especially AI, can solve blind spots in society.\nSolve what no one wants to do and hardly achieve only by manpower.\n\nContact: 4923.py@gmail.com\nPresent: Internship in Lab HAI, HUFS"
  },
  {
    "objectID": "posts/NLP/encoding.html",
    "href": "posts/NLP/encoding.html",
    "title": "encoding",
    "section": "",
    "text": "encoding을 왜 할까? 컴퓨터가 자연어를 이해할 수 있을까? 아니다. 따라서 컴퓨터가 이해할 수 있도록, 단어 자체의 정보는 보존한 채 자연어를 숫자로 바꾸는 방법이 encoding이다.\n그렇다면 positional encoding에서 보존하고자 하는 정보는 무엇일까? 자리다. 자리가 왜 중요한가? 자리가 의미를 반영하는 경우가 있기 때문이다. 부정어구가 대표적이다. 아래 두 문장을 보면 알 수 있다. 부정어구 안 이 위치하는 곳에 따라 음식의 재료가 바뀐다.\n위치가 중요함을 알았으니 위치 정보를 컴퓨터가 이해할 수 있는 언어로 바뀐 단어에 주입하는 방법에 대해 알아보자."
  },
  {
    "objectID": "posts/NLP/encoding.html#embedding",
    "href": "posts/NLP/encoding.html#embedding",
    "title": "encoding",
    "section": "Embedding",
    "text": "Embedding\n\nDocstring\nclass Embedding(Module):\n    r\"\"\"A simple lookup table that stores embeddings of a fixed dictionary and size.\n\n    This module is often used to store word embeddings and retrieve them using indices.\n    The input to the module is a list of indices, and the output is the corresponding\n    word embeddings.\npytorch의 모든 신경망은 nn 을 상속받음으로써 시작된다. Module은 nn.Module로 공식문서에서 별도로 import 했다. docstring에 의하면 본 모듈은 word embedding 또는 index들을 사용하여 word embedding을 검색하는데 사용된다. 이 모듈을 향한 입력 은 index들로 구성된 list 이고, 이 모듈의 결과값은 word embedding에 해당한다.\n\n\nArguments\n\ninstance를 생성할 때 입력할 params\n\n    Args:\n        num_embeddings (int): size of the dictionary of embeddings\n        embedding_dim (int): the size of each embedding vector\n        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                     i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                     the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                     but can be updated to another value to be used as the padding vector.\n        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                    is renormalized to have norm :attr:`max_norm`.\n        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                                the words in the mini-batch. Default ``False``.\n        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                                 See Notes for more details regarding sparse gradients.\n\nnum_embeddings : int 값으로, embedding 될 단어쌍 (dictionary) 의 최대 값을 말한다.\nembedding_dim : int 값으로, 각 embedding 벡터의 길이를 말한다.\n\n\n\n\n\n\n\n왜 num_embeddings 에는 임베딩 될 값의 크기보다 더 큰 값을 넣어야 하는가?\n\n\n\n\n\ninstance인 embedding 을 어떻게 쓸 지 생각해보면 좋다. 우리는 어떤 값을 숫자로 표현할 것이고 input 값에 9가 들어가든 len(‘가나다라마바사’) 가 들어가든 아무런 상관이 없다. 단지 숫자로 변환 할 때 모듈이 nn.Embedding 이고 instance를 생성할 때 미리 parameter를 준비해 놔야 하는 점만 중요하게 여기면 된다.\ninput = torch.LongTensor([[1,2,4,5],[4,3,2,len('가나다라마바사')]])\nembedding = nn.Embedding(10, 3)\n\nembedding(input[1][-1])\n# tensor([ 0.2074,  0.0673, -0.1462], grad_fn=&lt;EmbeddingBackward0&gt;)\nnn.Embedding 을 통해 인스턴스를 생성할 때 (num_embeddings * embedding_eim) 모양의 파라미터가 생긴다. embedding(input) 을 통해 input 값을 임베딩 하면 미리 만들어둔 파라미터에 임베딩 된 값이 걸리게 되는 셈이다.\n\nreference\n\nhttps://discuss.pytorch.kr/t/embedding/942\n\n\n\n\n\n\n\nAttributes\n    Attributes:\n        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                         initialized from :math:`\\mathcal{N}(0, 1)`\n이전에 instance를 생성할 때 num_embeddings, embedding_dim 을 할당하면 그 형태의 파라미터가 생성된다고 적은 바 있다. 이 형태에 따라 랜덤하게 가중치를 할당하는 역할을 한다. 최초에는 0부터 1 사이의 값으로 할당되며 이 Tensor는 직접적으로 학습되는 값이다.\n\n\nShape\n    Shape:\n        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n    .. note::\n        Keep in mind that only a limited number of optimizers support\n        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n    .. note::\n        When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n        :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n        modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n        calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n        :attr:`max_norm` is not ``None``. For example::\n\n            n, d, m = 3, 5, 7\n            embedding = nn.Embedding(n, d, max_norm=True)\n            W = torch.randn((m, d), requires_grad=True)\n            idx = torch.tensor([1, 2])\n            a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n            b = embedding(idx) @ W.t()  # modifies weight in-place\n            out = (a.unsqueeze(0) + b.unsqueeze(1))\n            loss = out.sigmoid().prod()\n            loss.backward()\n\n\nExamples\n    Examples::\n\n        &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n        &gt;&gt;&gt; embedding = nn.Embedding(10, 3)\n        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n        &gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n        &gt;&gt;&gt; embedding(input)\n        tensor([[[-0.0251, -1.6902,  0.7172],\n                 [-0.6431,  0.0748,  0.6969],\n                 [ 1.4970,  1.3448, -0.9685],\n                 [-0.3677, -2.7265, -0.1685]],\n\n                [[ 1.4970,  1.3448, -0.9685],\n                 [ 0.4362, -0.4004,  0.9400],\n                 [-0.6431,  0.0748,  0.6969],\n                 [ 0.9124, -2.3616,  1.1151]]])\n\n\n        &gt;&gt;&gt; # example with padding_idx\n        &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)\n        &gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])\n        &gt;&gt;&gt; embedding(input)\n        tensor([[[ 0.0000,  0.0000,  0.0000],\n                 [ 0.1535, -2.0309,  0.9315],\n                 [ 0.0000,  0.0000,  0.0000],\n                 [-0.1655,  0.9897,  0.0635]]])\n\n        &gt;&gt;&gt; # example of changing `pad` vector\n        &gt;&gt;&gt; padding_idx = 0\n        &gt;&gt;&gt; embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n        &gt;&gt;&gt; embedding.weight\n        Parameter containing:\n        tensor([[ 0.0000,  0.0000,  0.0000],\n                [-0.7895, -0.7089, -0.0364],\n                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n        &gt;&gt;&gt; with torch.no_grad():\n        ...     embedding.weight[padding_idx] = torch.ones(3)\n        &gt;&gt;&gt; embedding.weight\n        Parameter containing:\n        tensor([[ 1.0000,  1.0000,  1.0000],\n                [-0.7895, -0.7089, -0.0364],\n                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    \"\"\"\n    __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',\n                     'norm_type', 'scale_grad_by_freq', 'sparse']"
  },
  {
    "objectID": "posts/NLP/huggingface/PEFT.html",
    "href": "posts/NLP/huggingface/PEFT.html",
    "title": "PEFT",
    "section": "",
    "text": "Quicktour\n모든 peft는 PeftConfig class를 정의한다. PeftConfig는 PeftModel을 만드는데 중요한 parameters를 저장한다. huggingface Quicktour에서는 LoRA를 이용해 클래스 분류문제를 푸므로 여기서 생성해야 할 PeftConfig는 LoraConfig다.\nLoraConfig는 아래와 같이 정의한다\n# pip install peft\nfrom peft import LoraConfig, TaskType\npeft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8,\n                        lora_alpha=32, lora_dropout=-0.3 )\n\nmajor parameters\n각 인자는 다음을 나타낸다. (more)\n\ntask_type : TaskType 에서 task 종류를 정의한다. 이 경우에는 Seq2Seq 언어 모델링이다.\n\n왜 하필 Seq2Seq?\n\ninference_mode : 추론을 할 때 사용한다.\nr : low-rank matrices의 차원을 결정한다.\n\nLoRA의 LoR가 Low Rank다. (LoRA: Low Rank Adaptation) 이 때의 r 값이 ’학습 가능한 분해행렬’에 해당한다.\nlow-rank matrices: 행렬의 랭크는 행렬에서 선형적으로 독립적인 열(또는 이에 상응하는 행)의 최대 수, 행렬로 표현되는 벡터가 포함하는 최대 차원 수로 행렬에 포함된 정보의 양이라고 이해할 수 있다.\n아래와 같이 구할 수 있다.\nimport numpy as np\nA = np.array([[1, 2, 3], [2, 4, 6], [3, 6, 9]])\nprint(\"Rank of A:\", np.linalg.matrix_rank(A))   # 1\n\nB = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(\"Rank of B:\", np.linalg.matrix_rank(B))   # 2\n\nlora_alpha : low rank matrices를 위한 ‘scaling factor’, lora scaling에 필요한 파라미터다.\n\nlora scaling? (LoRA 참고) : LoRA는 사전학습된 가중치 행렬 \\(W_0\\) 이 업데이트 될 때 \\(\\Delta W\\) 를 \\(BA\\) 로 바꾸어 더하는 과정이다. \\[W_0 + \\Delta W = W_0 + BA\\] 여기서 \\(A\\)는 random Gaussian initialized matrix 이고 B는 0으로 initialization 된 값이다. 학습 과정에서 \\(\\Delta W x\\) 는 \\(\\frac{\\alpha}{\\gamma}\\) 로 scaling 되고 이후 Optimized 된다. 여기서\\(\\alpha\\) 를 튜닝하는 것은 learning rate처럼 tuning 될 수 있다.\n\nlora_dropout : LoRA 레이어를 dropout하는 확률이다.\n\n다음으로 PeftModel을 정의한다. get_peft_model() 로 불러오며 불러오는 방식은 다른 huggingface 모델의 방식과 동일하다.\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n이렇게 기본 모델과 peft_config을 get_peft_model로 wrapping한다. 이렇게 wrap된 모델이 PeftModel이다.\nfrom peft import get_peft_model\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\"output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282\"\n위 코드의 실행 결과를 보면 실제로 우리가 학습시켜야 하는 학습인자는 0.19%만 되는 것을 확인할 수 있다. 원체 초기 파라미터 개수가 방대라니 0.19%도 적지 않은 수겠지만 비율상 상당한 감소다.\n\n\nReference\n\nhttps://huggingface.co/blog/peft\nhttps://da2so.tistory.com/79"
  },
  {
    "objectID": "posts/techniques/handling_imbalanced_data.html",
    "href": "posts/techniques/handling_imbalanced_data.html",
    "title": "Handling imbalanced Data",
    "section": "",
    "text": "under sampling : 너무 많은 양의 데이터를 잘라내는 방식\nover sampling : 적은 양의 데이터를 증강하는 방식\nweight sampling : 학습할 배치에 데이터가 들어갈 확률을 지정하는 방식\nloss function : 적은 양의 데이터 학습과정에 가중치를 주는 방식\n\n데이터의 절대량을 조정하는 방법과 (1, 2) 학습할 때 데이터의 균형을 맞추는 방법이 (3, 4) 있다.\n이들 중 weight sampling 방법과 imbalanced data task에 적합한 loss function를 알아보겠다.\n\n\n배치(batch)크기는 하이퍼파라미터(hyperparameter)의 한 종류로 한 번 기울기를 갱신할 때(step) 사용하는 데이터의 개수를 말한다. 배치는 미니배치(mini batch)라고도 불리며 \\(2^n\\) 개로 구성된다. 이 때 배치를 구성하는 방식을 샘플링(Sampling)이라고 하는데 Weight Sampling은 배치를 구성하는 데이터를 각각 다른 확률에 따라 추출하는 샘플링 방식이다. 따라서 데이터의 절대적인 개수가 작아 뽑힐 확률이 적은 데이터에게 가중치를 주어 더 자주 뽑힐 수 있게 조정하는 과정을 거칠 수 있다.\n예를 들어, 아래 표와 같은 데이터가 있을 때, c가 뽑힐 확률은 0.1, d가 뽑힐 확률은 0.4로 d가 뽑힐 확률이 네배 더 크다. 불균형 데이터(imbalanced data) 에서는 치명적으로, 한 배치에 뽑힌 데이터가 모두 한 label로 구성될 가능성이 있기 때문이다. 아래 표에서 배치가 32라고 할 때, 배치를 구성하는 label이 모두 d라면 모델은 균형있는 학습을 하지 못하게 되거나 d에 과적합 될 수 있다. 그러므로 전체 데이터가 불균형하더라도 배치 안에서는 균형있는 학습을 진행하기 위해 torch.utils.data.WeightedRandomSampler 메서드를 사용한다.\n\n\n\nlabel\ncount\n\n\n\n\na\n30\n\n\nb\n20\n\n\nc\n10\n\n\nd\n40\n\n\n\nWeightedRandomSampler 를 사용하면 코드 셀과 같은 결과를 확인할 수 있다. 첫번째 예제에서는 index 1의 가중치가 0.9로 가장 크며 복원추출(replacement=True)한 결과 역시 1이 세번으로 가장 많이 추출된 것을 확인 할 수 있다.\n\nfrom torch.utils.data import WeightedRandomSampler\nprint(\"replacement = True\\t-&gt; \", list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)))\nprint(\"replacement = False\\t-&gt; \", list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)))\n\nreplacement = True  -&gt;  [4, 1, 1, 4, 1]\nreplacement = False -&gt;  [0, 1, 2, 4, 5]\n\n\n\n\n\n다른 방법으로는 학습 과정에서 가중치를 주는 방법이 있다. 모델이 문제를 풀 때 해당 문제가 쉬운지 어려운지는 어떻게 판별할까? 분류 문제에서는 최종 확률값으로 문제의 난이도를 판별한다. 이것 같기도 하고, 저것 같기도 해서 헷갈리니 각 label이 답일 확률이 비슷비슷하게 높은 것이다. 따라서 \\(logit\\) \\(^1)\\) 값의 평균은 낮을 수밖에 없다. 답을 결정하는 최종 확률은 그 중 가장 높은 값을 고른 것이니 최종 확률이 낮을수록 어려운 문제다.\n\n\\(^1)\\) \\(logit\\) : 어떤 사건이 벌어질 확률 \\(p\\)가 \\([0,1]\\) 사이의 값일때 이를 \\([-\\infty, +\\infty]\\) 사이 실수값으로 변환하는 과정을 로짓(logit) 변환이라고 한다.\n\n\n\n문제가 어려운 문제인지 아닌지 알아야 하는 이유는 여기에 있다. focal loss는 불균형 데이터 문제를 해결할 때 대표적으로 쓰이는 손실함수로, 쉬운 문제를 틀렸을 때엔 작은 loss 값을, 어려운 문제를 틀렸을 때엔 큰 loss 값을 반환한다. 데이터가 적어 상대적으로 잘 학습하지 못한 label은 틀렸을 때 모델의 성능에 크게 영향을 미치게 되므로 학습 과정에서 가중치를 준다고 생각할 수 있다. 그렇다면 focal loss의 최대값은 어떻게 될까? focal loss는 기본적으로 연산한 loss에서 난이도만큼 값을 ‘깎는’ 원리이므로 focal loss의 최대값은 기본 손실값과 같을 것이다.\n\ntorchvision에서 제공하는 focal loss : source code\n\n이 외에도 기존 손실함수에 가중치를 줄 수 있는데 f1, cross entropy, fbeta, accuracy 등의 함수가 그러하다. 해당 함수들의 ‘average’ 인자값에 ’weighted’를 주면 가중된 손실이 누적된다. 이렇게 가중된 손실함수를 여러개 사용하면 모델의 성능이 개선될 수 있다."
  },
  {
    "objectID": "posts/techniques/handling_imbalanced_data.html#summary",
    "href": "posts/techniques/handling_imbalanced_data.html#summary",
    "title": "Handling imbalanced Data",
    "section": "",
    "text": "under sampling : 너무 많은 양의 데이터를 잘라내는 방식\nover sampling : 적은 양의 데이터를 증강하는 방식\nweight sampling : 학습할 배치에 데이터가 들어갈 확률을 지정하는 방식\nloss function : 적은 양의 데이터 학습과정에 가중치를 주는 방식\n\n데이터의 절대량을 조정하는 방법과 (1, 2) 학습할 때 데이터의 균형을 맞추는 방법이 (3, 4) 있다.\n이들 중 weight sampling 방법과 imbalanced data task에 적합한 loss function를 알아보겠다.\n\n\n배치(batch)크기는 하이퍼파라미터(hyperparameter)의 한 종류로 한 번 기울기를 갱신할 때(step) 사용하는 데이터의 개수를 말한다. 배치는 미니배치(mini batch)라고도 불리며 \\(2^n\\) 개로 구성된다. 이 때 배치를 구성하는 방식을 샘플링(Sampling)이라고 하는데 Weight Sampling은 배치를 구성하는 데이터를 각각 다른 확률에 따라 추출하는 샘플링 방식이다. 따라서 데이터의 절대적인 개수가 작아 뽑힐 확률이 적은 데이터에게 가중치를 주어 더 자주 뽑힐 수 있게 조정하는 과정을 거칠 수 있다.\n예를 들어, 아래 표와 같은 데이터가 있을 때, c가 뽑힐 확률은 0.1, d가 뽑힐 확률은 0.4로 d가 뽑힐 확률이 네배 더 크다. 불균형 데이터(imbalanced data) 에서는 치명적으로, 한 배치에 뽑힌 데이터가 모두 한 label로 구성될 가능성이 있기 때문이다. 아래 표에서 배치가 32라고 할 때, 배치를 구성하는 label이 모두 d라면 모델은 균형있는 학습을 하지 못하게 되거나 d에 과적합 될 수 있다. 그러므로 전체 데이터가 불균형하더라도 배치 안에서는 균형있는 학습을 진행하기 위해 torch.utils.data.WeightedRandomSampler 메서드를 사용한다.\n\n\n\nlabel\ncount\n\n\n\n\na\n30\n\n\nb\n20\n\n\nc\n10\n\n\nd\n40\n\n\n\nWeightedRandomSampler 를 사용하면 코드 셀과 같은 결과를 확인할 수 있다. 첫번째 예제에서는 index 1의 가중치가 0.9로 가장 크며 복원추출(replacement=True)한 결과 역시 1이 세번으로 가장 많이 추출된 것을 확인 할 수 있다.\n\nfrom torch.utils.data import WeightedRandomSampler\nprint(\"replacement = True\\t-&gt; \", list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)))\nprint(\"replacement = False\\t-&gt; \", list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)))\n\nreplacement = True  -&gt;  [4, 1, 1, 4, 1]\nreplacement = False -&gt;  [0, 1, 2, 4, 5]\n\n\n\n\n\n다른 방법으로는 학습 과정에서 가중치를 주는 방법이 있다. 모델이 문제를 풀 때 해당 문제가 쉬운지 어려운지는 어떻게 판별할까? 분류 문제에서는 최종 확률값으로 문제의 난이도를 판별한다. 이것 같기도 하고, 저것 같기도 해서 헷갈리니 각 label이 답일 확률이 비슷비슷하게 높은 것이다. 따라서 \\(logit\\) \\(^1)\\) 값의 평균은 낮을 수밖에 없다. 답을 결정하는 최종 확률은 그 중 가장 높은 값을 고른 것이니 최종 확률이 낮을수록 어려운 문제다.\n\n\\(^1)\\) \\(logit\\) : 어떤 사건이 벌어질 확률 \\(p\\)가 \\([0,1]\\) 사이의 값일때 이를 \\([-\\infty, +\\infty]\\) 사이 실수값으로 변환하는 과정을 로짓(logit) 변환이라고 한다.\n\n\n\n문제가 어려운 문제인지 아닌지 알아야 하는 이유는 여기에 있다. focal loss는 불균형 데이터 문제를 해결할 때 대표적으로 쓰이는 손실함수로, 쉬운 문제를 틀렸을 때엔 작은 loss 값을, 어려운 문제를 틀렸을 때엔 큰 loss 값을 반환한다. 데이터가 적어 상대적으로 잘 학습하지 못한 label은 틀렸을 때 모델의 성능에 크게 영향을 미치게 되므로 학습 과정에서 가중치를 준다고 생각할 수 있다. 그렇다면 focal loss의 최대값은 어떻게 될까? focal loss는 기본적으로 연산한 loss에서 난이도만큼 값을 ‘깎는’ 원리이므로 focal loss의 최대값은 기본 손실값과 같을 것이다.\n\ntorchvision에서 제공하는 focal loss : source code\n\n이 외에도 기존 손실함수에 가중치를 줄 수 있는데 f1, cross entropy, fbeta, accuracy 등의 함수가 그러하다. 해당 함수들의 ‘average’ 인자값에 ’weighted’를 주면 가중된 손실이 누적된다. 이렇게 가중된 손실함수를 여러개 사용하면 모델의 성능이 개선될 수 있다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html",
    "href": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html",
    "title": "fgsm tutorial",
    "section": "",
    "text": "FGSM1 으로 MNIST를 속여보자.\n여기서 FGSM은 오분류를 목표로 하는 화이트박스 공격이다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html#footnotes",
    "href": "posts/LabHAI/tutorial/GAN/fgsm_tutorial.html#footnotes",
    "title": "fgsm tutorial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFast Gradient Sign Attack↩︎"
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html",
    "title": "How to read a paper",
    "section": "",
    "text": "처음부터 정독하며 필요한 부분을 발췌 혹은 보충한다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#abstract",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#abstract",
    "title": "How to read a paper",
    "section": "Abstract",
    "text": "Abstract\n연구자들이 논문을 읽는데 사용하는 시간에 비하여 그것을 읽는 방법은 충분히 가르쳐지지 않고 있으며 이는 상당한 손실을 야기한다. 본 논문은 논문을 읽을 때에 실용적이고 효과적인 three-pass method 개요를 서술할 것이며 포괄적 문헌 조사 (literature survey)에서 이를 어떻게 사용하는지 보이려 한다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#introduction",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#introduction",
    "title": "How to read a paper",
    "section": "Introduction",
    "text": "Introduction\n연구자들은 논문을 여러가지 이유로 읽는다.\n\n최신 동향을 살피기 위해\n새로운 분야로의 진출을 위해\n컨퍼런스나 수업의 리뷰를 위함\n\n리뷰란 정확히 무엇을 말하는가?: 전반적으로 검토하는 행위 혹은 리뷰 논문을 읽는 행위 등을 말할 수 있으나 본 맥락에서는 피어리뷰(peer review)1를 말하는 것 같다.\n\n\n1 동료 평가로, 타 연구자의 연구를 객관적으로 분석하고 저널에 실리기 적합한지 판정하는 과정이다. 저널의 영역, 목적, 독자층을 살피는 과정이 수반되며 논문의 가치를 판정하기 위해 몇 가지 항목을 검토하게 된다.\n검토되는 항목은 다음과 같다.\n\n보존 가치\n참신성\n연구 윤리/직업적 무결성\n글의 완성도\n데이터의 질과 분석의 타당성\n결론의 타당성\n\n\n\n\n\n\n\n\nReviewer Guidelines for Journal Selections\n\n\n\n\n\n\n논문 작성 시 유념할 것\n\n\n보존가치 Archival: 저널에 싣는다는 것은 어떤 형태로든 해당 연구가 기록되고 외부에 퍼진다는 의미다. 따라서 저널에 실릴 논문은 결과나 해석의 가치가 있는 결과를 포함해야 한다.\n\n논문의 내용이 과거의 연구와 관계있거나 향후 연구의 초석이 될 가능성이 있는가?\n결과나 해석이 지속 가능한 과학적 가치를 가지고 있는가?\n해당 주제가 연구 분야에서 중요한가?\nSOTA (State Of The Art) 를 넘어서는 성능 혹은 강점을 가지고 있는가?\n\n참신성 Innovative: 기술적으로 새롭거나, 혁신적이거나, 구조적인 리뷰여야 한다. 연구의 가치를 판정함에 있어 가장 중요한 지표로 보인다.\n\n주제가 오늘날의 청중에게 흥미로운 내용이거나 중요한 내용인가?\n아이디어/정보/방법이 가치있거나 새롭거나 창의적인가?\n저자가 제시한 정보가 완전히 새로운 것인가?\n분석적이거나, 수치적이거나, 실험적인 결과 혹은 해석이 독창적인가? (Are ~ original 에서 original을 독창적으로 해석)\n결과가 미칠 영향이 선명하게 기술되어 있는가?\n\n직업적 무결성 Professional Integrity: 기본적인 윤리에 대한 문제다. 연구자로서 개인의 사적 이익과 견해를 최대한 배제하고 객관적이고 공정하게 연구에 임했는지 확인한다.\n\n특정 자본과 밀접한 관계를 맺고 있지 않은가?\n개인적이거나 편향된 관점을 가지지는 않았는가?\n선행연구가 적절히 인용되어 있는가?\n(만약 그렇게 주장할 경우) 실제로 최초로 사용된 기술인가?\n반대되는 주장이나 경쟁 구도에 있는 연구 등을 폄하하지 않는가?\n선행연구를 공정하고 건설적인 방식으로 참조하고 있는가?\n연구자로서 적절한 규정 및 윤리 지침을 따랐는가?\n\n발표 Presentation: 논문도 글이다. 읽히기에 적합한 글인지, 연구의 내용을 명료하고 병확하게 기술했는지 판정한다.\n\n서론에서 동기를 설명하고 독자의 방향을 제시하는가?\n논문에 수행 작업, 방법, 주된 결과가 설명되어 있는가?\n주제에서 벗어나지 않았는가?\n표와 그림이 명확하고 정확한가?\n논문에서 사용한 개념이 명확히 제시되어 있는가?\n제목과 주요 단어가 오용되지 않았는가?\n논문의 길이가 논문의 범위에 적합한지까지 살피는 줄 몰랐는데 그렇다고 한다.\n기본적인 작문 기술을 갖추었는가?; 단어 선택, 문장 구조, 단락 구성, 참고 문헌 인용 등\n\n질 Quality: 데이터의 질과 분석과 기술의 타당성을 확인한다. 아무리 좋은 데이터라도 적합한 분석 과정을 거치지 않는다면 무의미하며, 아무리 좋은 기술이라도 적절한 데이터를 사용하지 않는다면 적합한 결과를 확인할 수 없다.\n\n기술적으로 건전한가?\n강점 뿐 아니라 그 한계까지 평가하고 있는가?\n평가 지표가 명시되어 있는가?\n선행 연구를 충분히 검토하였는가?\n선행 연구에서 이어지는 연구인가? 즉, 이전에 입증된 연구에서 참조된 가정으로부터 비롯된 연구인가?\n\n결론의 타당성 Soundness of conclusions: 글과 주장을 올바르게 맺었는지 확인한다.\n\n주장이 확고한가?\n결론이 이론적, 실험적으로 타당한가?\n제시한 사실이 충분히 결론을 뒷받침 할 수 있는가?\n\n\n\n출처: SAE International\n\n\n\n\n이와 같은 이유로 연구자들은 매년 수백시간을 논문을 읽는데 사용한다. 당연히 논문을 읽는 방법이 중요할 수 밖에 없는데, 적절하게 논문을 읽는 방법이 가르쳐지지 않은 실정이다. 대학원생들은 스스로 시행착오를 겪으며 ’논문 읽기’를 습득해야 하는데 이는 노력을 낭비하는 일이 아닐 수 없다.\n여러해 동안 저자는 쉽고 효율적으로 논문에 접근하는 방법을 사용해왔는데, 이를 이 논문에서 제시하겠다고 한다.\n여기까지가 도입부의 내용이다. callout으로 정리한 내용에 따르면 일반적으로 introduction 에는 ‘동기’ 및 ’연구의 방향’을 제시하는 과정이 필요하다. 이번 논문의 내용을 살펴보자. 서론은 세줄로 요약이 가능하다. (1) 연구자들은 논문을 많이 읽는다/읽어야 한다. (2) 그러나 도움을 받을 방법이 없기에 그 접근 장벽은 상당히 높아졌다. (3) 저자는 이를 쉽게 해결할 방법(three-pass approach)을 알고 있다.\n짧은 도입부지만 여기에는 동기 (1~2) 와 연구의 방향 (3) 이 모두 제시되어 있다. 도입부로써 적절하다고 판단할 수 있겠다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#the-three-pass-approach",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#the-three-pass-approach",
    "title": "How to read a paper",
    "section": "The Three-pass approach",
    "text": "The Three-pass approach\n주요 아이디어는: 처음부터 끝까지 냅다 읽어내리지 말고, 세번정도에 나누어 읽으라는 것이다. 각각의 ’pass’에는 성취해야 할 목표들이 있는데 이를 나열하면 다음과 같다.\n\n논문의 전반적인 핵심 논제를 파악할 것\n실제 논문의 내용을 이해할 것, 그러나 모든 세부 내용까지 알 필요는 없다.\n깊이 읽으며 앞선 pass에서 읽어내지 못한 부분까지 파악한다.\n\n\nThe first pass\n읽어볼만 한지, 그렇다면 얼마나 깊게 읽어야 하는지 전반적으로 판단하는 단계다. 5분에서 10분을 들여 빠르게 스키밍 한다. 단계는 다음과 같다.\n\n제목, 초록, 소개를 읽고\n섹션과 하위섹션의 제목을 읽으며 무슨 내용일지 파악한다.\n결론을 확인한다.\n참고문헌을 훑어보며2 읽은 내용을 전반적으로 검토한다.3\n\n2 glance: 흘낏 보다, 휙휙 훑어보다.3 tick sbd/sth off: (이미 처리했음을 나타내기 위해) ~에 체크 표시(✓)를 하다해당 단계가 제대로 진행 되었다면 다음 항목에 대답할 수 있어야 한다.\n\n범주: 어떤 종류의 논문인가? - 주제, 연구방법 등\n맥락: 다른 논문과의 연관성은 어떠한가?\n정확도: 가정은 타당해 보이는가?\n기여도: 주된 기여도는 무엇인가? - novelty 등을 말하는 것으로 보임\n명확성: 논문은 잘 쓰여졌는가?\n\n위 질문에 대답함으로써 이해도를 파악하고, 그 대답을 통해 논문을 마저 읽을지 말지 결정할 수 있는데 배경 지식이 부족해 충분히 이해하지 못했거나, 논문의 주장이 빈약하거나, 논문의 질 자체가 좋지 않은 경우 더 읽지 않아도 좋다. 다만 연구 분야가 아니더라도 향후 도움이 될 것으로 판단되면 첫번째 단계는 통과한 것으로 본다.\n\n\n\n\n\n\n제목과 소제목들은 이래서 중요하다.\n\n\n\n\n\n첫번째 단계에서 더 읽을지 말지가 결정된다. 작성한 논문이 읽힐지 말지 결정되는건 이 짧은 5~10분 사이의 시간이므로 섹션 제목을 일관성 있게 선택하고 간결하며 포괄적인 초록을 작성하는데 주의를 기울여라.\n\n리뷰 통과 뿐 아니라, 일반 연구자들을 위해서도 그렇게 해야 한다.\n5분이 지나도 논문의 핵심을 이해하지 못하면 논문이 읽히지 않을 가능성이 높다.\n\n\n\n\n\n\nThe second pass\n첫번째 단계를 통과했다면 더 주의깊게 읽을 때가 됐다. 세부사항은 잠시 넘겨두고 논문을 이해하는데 초점을 맞추자. 메모하거나 의견을 정리하는 것도 도움이 된다.\n\n시각자료 검토\n특히 논문의 그림, 도표 등 시각 자료를 시간들여 살피는 것이 좋은데, 이는 논문의 질을 판단하는 주요한 요소가 되기 때문이다. 예를 들어; 축에 레이블이 제대로 붙어있는지, 결과가 오차 막대와 함께 포함되어 있는지4 결론이 통계적으로 유효한지 확인할 수 있는 방법이다.] 등을 확인하는 것이 좋다.\n참고문헌 검토\n또한, 참고문헌의 경우 나중에 읽을 것을 분류하는게 좋은데 이는 논문의 배경에 대해 알 수 있는 좋은 방법이기 때문이다. 읽지 않은 참고 문헌을 나중에 읽을 수 있게 표시하라고 명시해 두었으니, 참고문헌을 찾아 읽는 습관도 함께 들여두자.\n\n그렇다면 두번째 단계를 통과했는지 알아보는 척도는 무엇일까. 아마도 내용의 이해일 것이다.\n\nAfter this pass, you should be able to grasp the content of the paper. You should be able to summarize the main thrust of the paper, with supporting evidence, to someone else. This level of detail is appropriate for a paper in which you are interested, but does not lie in your research speciality.\n\n한시간 안에 이 단계를 끝마치는게 좋다.5 만약 한시간이 지나도 논문을 이해하지 못한다면 우리에겐 세가지 선택지가 남는다.\n5 전문 분야는 한시간 이내에 끝내야 한다는 의미일까? 아니면 더 오랜 시간을 들여서 검토해야 한다는 의미일까? 전자인것 같긴 하다.\n포기한다.\n미뤄둔다.\n계속한다.\n\n포기한다고 해서 세상이 끝나는건 아니다. 미룬다고 해서 큰일이 당장 벌어지는 것도 아닐 것이다. 특히 두번째 선택지는, 전진을 위한 후퇴에 가깝다. 배경지식을 습득하고 보는 것이기 때문이다. 하지만 언제나 준비만 할 수는 없는 것 같다.\n대개 나는 두번째 방법을 선택하는데, 지난한 시간 끝에 때로 3번을 강행하는게 좋다는 결론을 내리게 되었다. 모르면 모르는대로 실험해보자. 그 과정에서 얻은 것이 배경지식보다 더 빠른 길을 제시할 수 있다.\n\n\nThe third pass\n이제 논문을 완전히 이해할 때가 됐다. 완전히 이해했다는 건 어떤 의미일까? 전 단계에서 다른 사람에게 설명할 수 있을 정도로 충분히 이해했다면, 다시말해 이해의 수준이 ’다른 사람에게 내보일만큼’에 도달했다면 마지막은 나에게 떳떳할만큼의 수준을 갖추는 것이 순서일 것이다.\n\nThe key to the third pass is to attempt to virtually re-implement the paper: that is, making the same assumptions as the authors, re-create the work.\n\n재현이다. 이 과정과 실제 논문을 비교하면 첫번째 단계에서 가늠해보았던 논문의 혁신적인 부분을 직접 체험할 수 있을 뿐 아니라 두번째 단계에서 찾아내지 못했던 실험의 미비함이나 가정의 불완전함 또한 발견할 수 있다. 그 과정에서 새로운 연구 주제를 찾을 수 있게 된다.\n이 과정은 까다롭게 진행해야 한다. 모든 가정을 파악하고, 모든 요소에 이의를 제기하며, 본인이라면 어떻게 증명했을지 생각한 후에 논문과 비교하는 과정을 거쳐야한다. 이 과정은 초심자에겐 네다섯시간, 숙련자에겐 한시간정도 소요된다.\n세번째 단계를 거쳤다면 다음을 해낼 수 있어야 한다. 거의 해당 논문을 리뷰하는 수준까지 가야 하는 것 같다.\n\nAt the end of this pass, you should be able to reconstruct the entire structure of the paper from memory, as well as be able to identify its strong and weak points.\n\n특히 논문에 직접적으로 드러나지 않은 암시적인 가정6, 인용의 누락, 분석 기법의 잠재적 문제점까지 찾아내는 것이 목표다.\n6 implicit assumptions"
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#doing-a-literature-survey",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#doing-a-literature-survey",
    "title": "How to read a paper",
    "section": "Doing A Literature survey",
    "text": "Doing A Literature survey\n실제로 논문을 찾아보는 단계다. 간단히 방법만 기록하겠다.\n\nGoogle Scholar, CiteSeer 등의 학술 검색 엔진에서 키워드를 잘 선택하여 최신 논문 세개에서 다섯개를 찾을 것\n논문을 훑어보며 내용을 파악하고 관련 내용을 검토할 것\n\n이때 서베이 논문을 찾을 수도 있는데, 그렇다면 기뻐하며 끝내자.\n\n참고문헌에서 반복되는 저자 이름을 찾을 것.\n\n이때 반복되는 논문과 주요 저자의 최근 연구 이력을 잘 살피자.\n\n영향력이 큰 컨퍼런스의 최근 발표 내용을 살펴보고, 수준 높은 연구를 익힐 것.\n3번에서 모아둔 주요 연구자료와 4번에서 찾은 연구들을 함께 살피고, 반복해서 읽는다."
  },
  {
    "objectID": "posts/LabHAI/tutorial/How_2_read_a_paper.html#experience-related-work",
    "href": "posts/LabHAI/tutorial/How_2_read_a_paper.html#experience-related-work",
    "title": "How to read a paper",
    "section": "4~5 experience, related work",
    "text": "4~5 experience, related work\n경험에 근거하여 숲을 보기 전에 나무부터 살피고 있지 말라는 조언을 전한다.\n또한\n\n리뷰 논문을 작성하는 중이라면 아래 논문을 참고하라\n\nS. Peyton Jones, “Research Skills,”\n\nhttp://research.microsoft.com/ simonpj/Papers/giving- a-talk/giving-a-talk.html\n\n\n기술 논문을 작성하는 중이라면 아래 웹 사이트와 프로세스 개요를 탐독하라\n\nH. Schulzrinne, “Writing Technical Articles,”\n\nhttp://www.cs.columbia.edu/ hgs/etc/writing- style.html\n\nG.M. Whitesides, “Whitesides’ Group: Writing a Paper,”\n\nhttp://www.che.iitm.ac.in/misc/dd/writepaper.pdf\n\n\n이 모든 연구 기술의 스펙트럼을 다루는 웹 사이트도 하나 추천한다.\n\nACM SIGCOMM Computer Communication Review Online\n\nhttp://www.sigcomm.org/ccr/drupal/"
  },
  {
    "objectID": "posts/LabHAI/tutorial/transfer_learning/transfer_learning_tutorial.html",
    "href": "posts/LabHAI/tutorial/transfer_learning/transfer_learning_tutorial.html",
    "title": "transfer learning tutorial",
    "section": "",
    "text": "from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n# import torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\n# cudnn.benchmark = True\nplt.ion()   # 대화형 모드\n\n&lt;contextlib.ExitStack at 0x10465a1a0&gt;\n\n\n\n데이터 불러오기\n\n# augmentation\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),   # 왜 224?\n        transforms.RandomHorizontalFlip(),   # default=0.5\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225]\n        )\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n\ndata_dir = \"data/hymenoptera_data\"\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                    data_transforms[x])\n                    for x in ['train', 'val']}\n\nbatch_size = 8\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n                                                shuffle=True, num_workers=0)\n                                                for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes \n\ndevice = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n\n이미지 시각화해서 확인\n\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n\n# 학습 데이터의 배치를 얻고, 매번 다른 배치를 보여준다.\ninputs, classes = next(iter(dataloaders['train']))\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n\n\n\n\n\n\n\n\n\n학습\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=0):\n    since = time.time()\n\n    # 여기서 최고 성능의 모델을 판별하므로 함수 안에서 반복을 해야한다.\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    # 학습\n    for epoch in range(num_epochs):\n        if epoch % 10 == 0:\n            print('-' * 10)\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n        for phase in ['train', 'val']:  # epoch당 두번 돈다.\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()    # 평가모드로 전환\n            \n            # 먼저 선언\n            running_loss = 0.0\n            running_corrects = 0\n\n            # 학습/검증 데이터를 가져온다.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # 최적화를 위해 경사도를 0으로 선언\n                optimizer.zero_grad()\n\n                # forward\n                # 학습때만 연산\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # 학습 단계인 경우 역전파 + 최적화\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                # 연산 결과\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            # 이건 또 뭐야\n            if phase == 'train':\n                scheduler.step()\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            # 원래 .double() 로 데이터형태를 맞춰줬었다.\n            # TypeError: Cannot convert a MPS Tensor to float64 dtype \n            # as the MPS framework doesn't support float64. \n            # Please use float32 instead.\n            # default 값이 float() 다. \n            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n\n            if epoch % 10 == 0:\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # 모델을 깊은 복사(deep copy)함\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n\n    time_elapsed = time.time() - since\n    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # 가장 나은 모델 가중치를 불러옴\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n\n시각화\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # 결과값과 예측값\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)\n\n\n\n미세조정\n마지막 계층을 초기화하는 단계다.\n\n# 사전학습모델 resnet\n# FineTuning의 약자로 ft를 사용하는 것 같음\nmodel_ft = models.resnet18(weights='IMAGENET1K_V1')\n# 사전학습모델로부터 fc layer 입력 채널 수를 얻음\nnum_ftrs = model_ft.fc.in_features  \n\n# 사전학습의 FC layer를 nn.Linear~ 로 교체하는 작업\n# 출력 샘플의 크기: ``nn.Linear(num_ftrs, len (class_names))`` 로 일반화\n# Linear(in_features=512, out_features=8, bias=True)\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))\n\nmodel_ft = model_ft.to(device)\n\n# (역전파) 손실함수 지정\ncriterion = nn.CrossEntropyLoss()\n\n# (최적화) 모든 매개변수들이 최적화되었는지 관찰\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# 7 에폭마다 0.1씩 학습률 감소\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n\nmodel_ft.fc\n\nLinear(in_features=512, out_features=2, bias=True)\n\n\n\n# model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n#                         num_epochs=25)\n\n----------\nEpoch 0/24\n----------\ntrain Loss: 0.5382 Acc: 0.7172\nval Loss: 0.2261 Acc: 0.9216\n----------\nEpoch 10/24\n----------\ntrain Loss: 0.1790 Acc: 0.9303\nval Loss: 0.1501 Acc: 0.9608\n----------\nEpoch 20/24\n----------\ntrain Loss: 0.1212 Acc: 0.9467\nval Loss: 0.1478 Acc: 0.9477\n\nTraining complete in 1m 36s\nBest val Acc: 0.960784\n\n\n\nvisualize_model(model_ft)"
  },
  {
    "objectID": "posts/HAR/mmlab.html",
    "href": "posts/HAR/mmlab.html",
    "title": "MMLAB",
    "section": "",
    "text": "MMCV-Full with openmim\nbest practice를 따라 mim으로 mmcv-full을 설치함\npip3 install -U openmim\nmim install mmcv-full\n\n\nMMPose with pip (for third party)\n\nOfficial Install Guide\n\n# pip을 사용해도 무관 (Case b)\npip3 install mmpose\n\nverify the installation of mmpose\n\ndownload config and checkpoint\n# 본인이 원하는 폴더 생성\nmkdir verify-mmpose; cd verify-mmpose   # e.g.\n\n# download\nmim download mmpose --config associative_embedding_hrnet_w32_coco_512x512  --dest .\nverify the inference demo\n\npip을 이용해 third party 용 mmpose를 설치했으므로 demo용 python script 생성\n# (optional) option (a)를 따라 /demo 폴더를 생성\nmkdir demo; cd demo\nvi bottom_up_img_demo.py    # open vim\n# bottom_up_img_demo.py\nfrom mmpose.apis import (init_pose_model, inference_bottom_up_pose_model, vis_pose_result)\n\nconfig_file = 'associative_embedding_hrnet_w32_coco_512x512.py'\ncheckpoint_file = 'hrnet_w32_coco_512x512-bcb8c247_20200816.pth'\npose_model = init_pose_model(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n\nimage_name = 'demo/persons.jpg'\n# test a single image\npose_results, _ = inference_bottom_up_pose_model(pose_model, image_name)\n\n# show the results\nvis_pose_result(pose_model, image_name, pose_results, out_file='demo/vis_persons.jpg')\n\ndownload image for test\n\n스크립트를 수정하지 않았다면 verify-mmpose/demo/ 안에 사람이 포함된 persons.jpg 를 추가\n\nrun script\npwd # /home/devin/env/verify-mmpose\npython3 demo/bottom_up_img_demo.py\n결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npersons.jpg(input)\nvis_persons.jpg(result)\n\n\n\n└── verify-mmpose\n    ├── associative_embedding_hrnet_w32_coco_512x512.py # config\n    ├── demo\n    │   ├── bottom_up_img_demo.py\n    │   ├── persons.jpg\n    │   └── vis_persons.jpg\n    └── hrnet_w32_coco_512x512-bcb8c247_20200816.pth\n\n\nIssue (updated 2022.12.28)\n\n[alias expired over ver 1.24] ‘numpy’ has no attribute ‘int’\n# ERROR LOG\nTraceback (most recent call last):   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;   File \"/home/ubuntu/.local/lib/python3.8/site-packages/numpy/__init__.py\", line 284, in __getattr__\n    raise AttributeError(\"module {!r} has no attribute \" AttributeError: module 'numpy' has no attribute 'int'\n\n# Environment\nOS Release              Ubuntu 20.04.4 LTS\nmmcv-full               1.7.0\nmmpose                  0.29.0\nnumpy                   &gt;=1.19.5\n최신버전으로 numpy를 설치했는데 demo를 실행하는 과정에서 'numpy' has no attribute 'int' 에러가 발생했다. 공식문서에서 추가하라고 지시한 코드에는 numpy가 없었고, 로그를 살펴보니 이미지의 크기를 정하는 과정에서 int type이 필요했다. 실행한 데모는 bottom_up_transform 이었지만 np.int가 사용된 내역을 살펴보면 bottom_up 뿐 아니라 gesture에서도 사용되는 것 같았다.\n# ./datasets/pipelines/gesture_transform.py\n# ./datasets/pipelines/bottom_up_transform.py\n\ninput_size = np.array([input_size, input_size], dtype=np.int)\nstackoverflow 에 의하면 numpy1.20 부터 np.float 또는 np.int의 alias사용이 중단 되었다. np.int_ 로 대체하거나 int로 변환하라는 권고가 나왔는데, 소스코드를 전부 수정할 수 없는 상황이므로 requirements 에 따라 1.19 로 downgrade 하면 해결된다.\n\nNumPy requirements of mmpose: numpy&gt;=1.19.5\nnumpy 1.20 relesas note &gt; For np.int a direct replacement with np.int_ or int is also good and will not change behavior, but the precision will continue to depend on the computer and operating system. If you want to be more explicit and review the current use, you have the following alternatives:\nnumpy 1.24 relesas note &gt; The deprecation for the aliases np.object, np.bool, np.float, np.complex, np.str, and np.int is expired (introduces NumPy 1.20). Some of these will now give a FutureWarning in addition to raising an error since they will be mapped to the NumPy scalars in the future.\n\n\n\n\n\nMMDetection\n\nOfficial Install Guide\n\n\n마찬가지로 mim을 통해 mmcv-full을 설치한다.\n\npip3 install -U openmim\nmim install mmcv-full\n\npip으로 mmdet을 설치한다.\n\npip3 install mmdet\n\nVerify installation MMdet\n\ndownload config and checkpoint\n# 본인이 원하는 폴더 생성\nmkdir verify-mmdet; cd verify-mmdet   # e.g.\n\n# download\nmim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .\nverify the inference demo\n\npip을 이용해 third party 용 mmdet를 설치했으므로 demo용 python script 생성\n# (optional) option (a)를 따라 /demo 폴더를 생성\nmkdir demo; cd demo\nvi img_demo.py    # open vim\n# img_demo.py\nfrom mmdet.apis import init_detector, inference_detector\nconfig_file = 'yolov3_mobilenetv2_320_300e_coco.py'\ncheckpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'\nmodel = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n여기까지가 공식문서의 demo script인데 이렇게 되면 inference_detector된 결과가 CLI환경에 출력되지 않는다. https://greeksharifa.github.io/references/2021/08/30/MMDetection/#high-level-apis-for-inference 위 포스트를 참고하면 결과 이미지를 확인할 수 있다.\nimg = 'demo/demo.jpg'\ninference_detector(model, img)\n\nresult = inference_detector(model, img)\n# visualize the results in a new window\nmodel.show_result(img, result)\n# or save the visualization results to image files\nmodel.show_result(img, result, out_file='demo/demo_result.jpg')\n\n# test a video and show the results\nvideo = mmcv.VideoReader('demo/demo.mp4')\nfor frame in video:\n    result = inference_detector(model, frame)\n    model.show_result(frame, result, wait_time=1)\n\nprepare for demo\n\nmmpose와는 다르게 mmdet에는 demo용 이미지와 동영상이 있으므로 그 파일을 사용하거나 다운로드한다.\n\npwd # /home/devin/env/verify-mmdet\nwget https://github.com/open-mmlab/mmdetection/blob/master/demo/demo.jpg ./demo/demo.jpg\nrun script bash     pwd # /home/devin/env/verify-mmdet     python3 demo/img_demo.py\n결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemo.jpg (input)\ndemo_result.jpg (result)\n\n\n\n└── verify-mmdet\n    ├── yolov3_mobilenetv2_320_300e_coco.py # config\n    ├── demo\n    │   ├── img_demo.py\n    │   ├── demo.jpg\n    │   └── demo_result.jpg\n    └── yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth"
  },
  {
    "objectID": "posts/HAR/modalities.html",
    "href": "posts/HAR/modalities.html",
    "title": "Modalities",
    "section": "",
    "text": "mainstream deep learning architectures\n\nsingle modalities\nmultiple modalities for enhanced HAR\n\nDATA : short trimmed video segments\n\none, only action instance\n\nBenchmark Datasets"
  },
  {
    "objectID": "posts/HAR/modalities.html#introduction",
    "href": "posts/HAR/modalities.html#introduction",
    "title": "Modalities",
    "section": "1. introduction",
    "text": "1. introduction\n\n다양한 data modalities들의 장점, 한계를 및 modality간의 연구 흐름 파악\n\n기술의 발전과 방법론의 창안은 선행 연구의 한계와 발전 가능성에 기초하므로 기술 발전의 흐름과 맥락을 숙지하는 과정은 중요하다. 본 논문은 2022 IEEE에 발표된 Review 논문으로 다양한 인간 행동 표현형을 인식하는 HAR 연구의 최신 흐름을 기술하고 있다. 개요에 따르면 인간 행동은 다양한 데이터 양식으로 표현될 수 있다. 데이터 양식은 크게 가시성 visibility 에 따라 visual modalities, non-visual modalities 로 나뉜다.\n\n\n\nvisual modalities\nnon-visual modalities\n\n\n\n\nRGB\naudio\n\n\nSkeleton\nacceleration\n\n\ndepth\nradar\n\n\ninfrared\nwifi signal\n\n\npoint cloud \\(^{1}\\)\n\n\n\nevent stream\n\n\n\n\n\npoint cloud \\(^{1}\\) 3차원 공간상에 퍼져 있는 여러 포인트(Point)의 집합(set cloud)으로 Lidar 센서와 RGB-D 센서로 수집된 데이터다.\n\n\n1.1 visual modalities\n일반적으로는 visual modalities가 HAR 발전에 큰 영향을 미쳐왔다. HAR의 많은 발전이 RGB Video 또는 images를 기반으로 이루어졌음을 보면 알 수 있다. RGB data는 관측 (surveillance) 또는 추적 (monitoring) 시스템에서 보편적으로 사용되어왔다. RGB 는 기본적으로 세 채널을 가진 ’이미지(들)’이기 때문에 큰 computing resource 를 필요로 하는데 이를 보완하기 위해 사용된 데이터가 skeleton 이다. skeleton data는 인간 관절의 움직임 (trajectory of human body joints) 을 encoding한 데이터로 간명하고 효과적이다. 그러나 물체가 포함되어있거나 장면간 맥락을 고려해야 하는 경우 skeleton data 만으로는 정보를 충분히 얻을 수 없는데 이 때 point cloud 와 depth data를 사용한다. 또한, ’본다’는 행위는 근본적으로 빛에 의존하는데 이 한계를 극복하기 위해 infrared data를 사용하며 event stream 은 움직였을 때를 감지하는 동작 중심 modality다.\n\n정보를 얻고, insight 또는 활용 가능한 부분만 추려내어 새로운 방법론을 창안하고, 기술이 최적화 되었을 때 더 필요한 정보를 얻기 위해 또 다른 modality를 사용하며 다시 불필요한 부분을 제거하는 방식으로 기술이 발전되어왔다.\n\n\n가시영역에서 얻을 수 있는 정보를 얻거나 : RGB\n인간이 눈으로 보고 이해하는 정보에 집중하여 인간 신체에서 이른바 ROI를 추출해내는 방법을 적용한다: skeleton\n사람과 환경의 상호작용 또는 시공간적 맥락을 고려하기 위해 3D 구조에서 주요한 정보를 추출하는 작업을 거친다: point cloud, depth data\n시각에 의존하지 않고 나아가 비가시영역인 적외선 영역에서 정보를 얻음으로써 빛에 의존해야 한다는 visual modalities의 태생적 한계를 극복한다: infrared data\n불필요한 중복, 또는 정보를 제거하여 HAR에 적합한 데이터를 구축한다: event stream\n\n\n\n1.2 non-visual modalities\n눈으로 봤을 때 직관적이지 않지만 사람들의 행동을 표현하는 또 다른 방식이다. 직관적이지 않음에도 사용될 수 있는 이유는 특정 상황에서 대상의 개인정보 보호가 필요할 때다. audio 는 시간에 따른 상황 (temporal sequence) 에서 움직임을 인지하기에 적절하며, acceleration 는 fine-grained HAR에 사용된다. \\(^2\\) 또한 radar는 물체 뒤의 움직임도 포착할 수 있다.\n\\(^2\\) fine-grained HAR : 세분화된 HAR. acceleration data가 이에 사용된다는 내용은 영상의 움직임에서 가속도를 알아내는 방향의 연구를 말하는 것으로 보인다. (관련 논문)\n\n\n1.3 Usage of modalities (single vs multi)\n\nsingle modality and effect of their fusion\n살펴본 바와 같이, 각 modality는 서로 다른 강점과 한계를 가지고 있으므로 여러 양식의 데이터들을 합치거나(fusion), 데이터 양식간 ‘transfer of knowledge’\\(^3\\) 를 진행하여 정확도와 강건함을 높인다.\n나아가 fusion은, 서로 다른 두 개 이상의 데이터에서 각 데이터간 장단점을 상호보완하기 위한 방법론으로 소리 데이터와 시각 데이터를 포함함으로써 단순히 ‘물건을 내려 놓는’ label을 가방을 내려 놓는지, 접시를 내려놓는지 구체적으로 구분할 수 있게 한다.\n\\(^3\\) Transfer learning 은 Transfer Learning과 Knowledge Distillation으로 나뉘는데 (ref) 서로 다른 도메인에서 지식을 전달하는 방법이 Transfer Learning (fine tuning 필요) 이고, 같은 도메인에서 다른 모델 간 지식 전달이 이루어지는 것을 Knowledge Distillation이라고 하면 ’transfer of knowledge across modalities’는 Transfer Learning을 말하는 것으로 보인다.\n\n\ndata modalities and its pros and cons"
  },
  {
    "objectID": "posts/HAR/modalities.html#single-modality",
    "href": "posts/HAR/modalities.html#single-modality",
    "title": "Modalities",
    "section": "2. Single modality",
    "text": "2. Single modality\nRGB, Skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, WiFi will be reviewed"
  },
  {
    "objectID": "posts/HAR/modalities.html#visible-modalities",
    "href": "posts/HAR/modalities.html#visible-modalities",
    "title": "Modalities",
    "section": "visible Modalities",
    "text": "visible Modalities\n\n2.1 RGB\n\n한계와 장점 모두 RGB Data의 특성에서 비롯된다.\n\n\n특성\n\n이미지(들) 로 이루어져있다. (\\(\\because\\) video is sequence of images)\nRGB data를 생성하는 카메라는 사람의 눈으로 보는 장면을 재생산하는 것을 목적으로 한다.\n수집하기 쉽고 상황과 맥락을 반영하고 있는 풍부한 외관 정보가 포함되어있다.\n\n‘rich appearance information’\n\n폭넓은 분야에 사용될 수 있다.\n\nvisual surveillance: 사람이 한 순간도 놓치지 않고 관찰할 수는 없는데 이를 보완할 수 있다.\nautonomous navigation: 자율주행(ANS)의 일부로써 사람의 개입 없이 정확하게 목적지까지 도달하도록 하는 기술이다.\nsport analysis: 눈으로 쫓기 힘든 순간들을 정밀하게 판독해야 하는 분야이므로 이 또한 ’사람의 눈’을 대신한다.\n\n\n한계\n\nvarious of background, viewpoints, scales of humans\n\n학습할 수 있는 데이터는 한정적이고, 이를 활용할 수 있는 변수는 너무 많다.\n\nillumination condition\n\n촬영이라는 개념이 갖는 근본적인 한계로, 광원 상태에 따라 결과가 달라질 가능성이 있다.\n\nhigh computational cost\n\n영상은 이미지의 연속이므로 공간과 시간을 동시에 고려하여 모델링하려면 많은 자원이 요구된다.\n\n\nmodeling methods\n\npre-deep learning : handcrafted feature-based approach, 수작업 특징 기반 접근법\n\nSpace-Time Volume-based methods\nSpace-Time Interest Point (STIP)\n\ndeep learning : currently mainstream\n\nbackbone model을 무엇으로 사용하느냐에 따라 나뉠 수 있다.\n\n\ntwo-stream CNN based method / multi stream architectures (extension of two stream)\n\nbackbone : 2D CNN\n시간정보가 포함될 수 밖에 없기 때문에 temporal information, spatial information 모두 고려하는 two-stream 접근이 제안되었다.\n\nRNN based method\n\nfeature extractor : RNN model with 2D CNNs\nRGB-based model\n\n3D CNN based method\n\n\n\n\n2.1.1 Two Stream 2D CNN-Based Methods\n\ntwo 2D CNN branches taking different input features extracted from RGB video for HAR and the final result is usually obtained through fusion strategy\n\nclassical approach\n고전적으로 two stream network는 각 network를 병렬적으로 학습시킨 후 결과를 융합 fusion 하여 최종 결과를 추론했다. 예를들어, input이 video이면 video에 내재된 정보들을 크게 1) rgb 프레임들은 공간 네트워크의, 2) multi-frame-based optical flow는 시간 네트워크의 학습 정보가 된다. 각 stream은 1) 모양 특성, appearance feature 과 2) 동작 특성 motion feature을 각각 학습한다.\n\nmulti-frame-based optical flow: 움직임을 묘사하는 방법으로, 주로 짧은 시간 간격을 두고 연속된 이미지들로 구성된다. optical flow는 이미지의 velocity(속도) 를 계산하는데 이 속도는 이미지의 특정 지점이 다음의 어디로 이동할지 예측할 수 있게 한다.\n\n주로 video understanding 에서 사용되는 개념으로 보인다.\nacceleration data와 어떻게 다른지 알 필요가 있다. &gt; The optical flow is used to perform a prediction of the frame to code\nnetworks: SpyFlow, PWC-Net; compute pixel-wise motion vectors\n\n\novercome limitation\nRGB 양식 데이터를 사용함에 있어 주된 문제로 지적되는 점은 ’큰 데이터 용량으로 인한 computing resource 부담과 연산 속도 저하’이므로 연산 속도를 높이기 위해 해상도를 낮추거나, 고해상도 데이터에서 center crop을 하는 기법을 적용했다.\nbetter data representation\n모델의 성능은 데이터의 양과 질에 좌우된다. 따라서 더 나은 video representation 에 눈을 돌리게 된다. Wang은 multi-scale video frames, optical flow를 각 CNN stream에 넣어 특성맵 feature map을 추출했고 이에서 trajectories에 중심을 둔 spatio-temporal tubes (action tube)를 샘플링했다. 이렇게 한 결과, Fisher Vector representation \\(^{3)}\\) 과 SVM을 통해 분류했다.\n\n왜 튜브일까? : \\(2)\\) 가 시기상 더 먼저 나온 논문이므로 후자에서 제시된 개념일 것으로 추정됨. 후자 논문의 Abstract에서 tube는 “예측된 동작을 연결함으로써 시간일관적으로 객체를 탐지하는” 개념이다.\n\nsuggest image region : 움직임이 두드러지는 영역을 선택\nCNN을 이용하여 공간적 특징을 추출\nAction tube를 생성\n\n\nWe link our predictions to produce detections consistent in time, which we call action tubes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 1. Discovering Spatio-Temporal Action Tubes; An over view of action detection framework\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 2. Finding action tubes; action tube approach, detect action on (a) and link detected actions in time to produce action\n\n\\(^{1)}\\) Discovering Spatio-Temporal Action Tubes (2018)\n\\(^{2)}\\) Finding action tubes or CVPR open access\n\\(^{3)}\\) Fisher Vector Representation: ref\n\nLong term video level information\n\n정보를 mean pooling하거나 누적하여 단일한 움직임이 아닌 움직임의 연속; 좀 더 복잡한 행동을 인식\n\n각 비디오를 세 개의 segment로 나눈 후 two stream network에 입력한 후, 각 segment의점수를 average pooling 을 이용해 융합한다. 또는 segment 점수를 pooling하지 않고 element-wise multiplication으로 특성의 총계를 구한다. 이 때 two stream framework에 의해 샘플링된 외형과 동작 프레임들은 ’하나의 video-level multiplied’를 위해 aggregate 연산되며 이를 action words 라고 칭한다. \\(^{4)}\\)\n\n\n\n\n\n\n\n\n\n\n\nFig 3. 동작들에서 행동과 관계된 action word를 추출한 후 이를 총 망라하는 하나의 분류를 선택하는 과정\n\n\n\n\n\n\\(^{4)}\\) R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell, “Actionvlad: Learning spatio-temporal aggregation for action classification,” in CVPR, 2017.\n\nEXTENSION of two stream CNN based method\n3 stream 으로 확장하는 등, “움직임” 또는 “프레임간 연속성”을 학습시키기 위해 다양한 방법론을 도입했다. 이후 2 stream siamese network (SNN) 로 확장되었는데 이는 동작 발생 전과 동작 후 프레임에서 특징을 추출하는 one shot learning의 일종으로 연속성이 아닌 동작 시작, 전, 후를 구분하여 학습하는 발상의 전환을 꾀한다.\n\none shot learning : 소량의 데이터로 학습할 수 있게 하는 방법이 few shot learning이라면 one shot은 그 극한으로 이미지 한장을 학습 데이터로 삼는 방법론이다. 사람은 물체간의 유사성을 학습하는데, 이 유사성은 물체를 배우고 물체간의 유사성을 또 다시 배우는 과정으로 나뉠 수 있다. 다시말해, 물체의 특성을 학습하고 이를 일반화할 줄 아는 능력을 학습시키는 방법이 one/few shot learning이다.\n\n\\(\\therefore\\) 이미지 자체의 특성을 학습하는 것이 아닌, 이미지간의 유사성을 파악하고 유사도를 파악할 때 쓰는 기법인 ’거리 함수’를 사용한다.\n\n샴네트워크\n\n\n\n\n\n\n\n\n\n\n\nFig 4. ref: A Guide to One-Shot Learning\n\n\n\n\ntwo stages: verification and generalization 가 포함된다.\n각각 다른 입력을 동일한 네트워크 인스턴스에 학습시키고, 이는 동일한 데이터셋에서 훈련되어 유사도를 반환한다.\n\n\nTackle high computational cost\nKnowledge distillation \\(^{5)}\\) 이 사용된다. “Data 에서 의미를 파악하면 Information 이 되고, Information 에서 문맥을 파악하면 Knowledge 이 되고, Knowledge 를 활용하면 Wisdom 이 된다.” 모델 압축을 위한 절차로, soft label과 hard label을 일치시키는 것이 목적이며 soft label에는 temperature scaling function을 적용하여 확률 분포를 부드럽게 만든다. 예를 들어 feature들의 label이 \\([0, 1, 0]^{T}\\) 이면 Hard label, \\([0.05, 0.75, 0.2]^{T}\\) 이면 soft label이다. 각 feature들은 서로 다른 특성을 가지고 있지만 공통된 특성 또한 가지고 있기 때문에, 이 공통 요소를 포함하는 class score를 날려버리면 (hard label) 정보가 손실되는 셈이다. 이렇게 정보가 손실되지 않게 Teacher network를 구성하고 Student network가 teacher network에 최대한 가까운 정답을 반환하도록 학습시킨다. 위에서 언급한 temperature는 그 값이 낮을 때 입력값의 출력을 크게 만들어주는 등 필요에 따라 값에 가중치를 둠으로써 Soft label의 이점을 최대화 한다. 참고\n\n\n\nteacher network; optical flow data\n\n\n\n\n⬇︎ Knowledge Distillation ⬇︎\n\n\nstudent network; motion vector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Spectrum\nDistillation Architecture\n\n\n\n\n\\(^{5)}\\) Distilling the Knowledge in a Neural Network(2015)\n\nIn Conclusion,\n여러개의 stream으로 CNN architecture들을 확장하거나 더 깊게 레이어를 쌓는 등 여러 시도를 해보았으나 수많은 video의 frame 개수를 고려할 때 깊이는 오히려 HAR에 방해가 될 수 있다. 선행 연구를 통해 ’차별화된 특징’을 예측하는 것이 중요함을 파악하게 되었다. 이 외에도, fusion strategy research의 마지막 conv layer에서 공간과 시간 네트워크를 융합하는 방법이 위에서 지적된 컴퓨팅 자원을 절약하면서 (params를 줄이면서) 정확도를 유지하는 효과적인 방법임을 알아냈다.\n\n\n2.1.2 RNN based\n\nfeature extractor 로 CNN을 사용한 hybrid architecture\n\nLSTM based model\nVanilla Recurrent Neural Network의 gradient vanishing 문제로 인해 RNN based solution은 gate 를 포함하는 RNN Architecture를 채택한다. (e.g. LSTM)\n\n\n\n\n\n\n\n\n\n\n\nFig 5. RGB modality modeling methods (CNN, RNN based)\n\n\n\n물론 ‘이미지’에서 공간적 요소를 빠트릴 수 없기 때문에 특징 추출은 여전히 2D CNN으로 진행하고, 시간요소를 LSTM에서 차용한 구조를 통해 모델링한다. 이를 LRCN (Long Term Recurrent Convolutional Network, Jeff Donahue et al. in 2016) 라고 하며 이는 ’2D CNN; 프레임 단위 RGB feature 추출’ + ’label 생성 LSTM’으로 구성된다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig 6.1 LRCN architecture\nFig 6.2 LRCN model for some tasks\n\n\n\n\nimage reference : Long-term Recurrent Convolution Network(LRCN)\n\nattention mechanism\nmulti layer LSTM model 설계 후 다음 프레임에 가중치를 부여한 Attention map 을 재귀적으로 출력함으로써 공간적 특성에 집중할 수 있게 되었다. - feature map을 중첩해서 뽑아내는 것 처럼? - recap : main idea of attention; decoder에서 출력 단어를 추론하는 매 순간마다 encoder에서의 전체 문장을 참고한다는 점. 단, ‘해당 시점에서 예측에 필요한 부분에 집중(attention)’해서 구한다. \\(Attention (Q, K, V) = Attention \\ Value\\) 로, Query에 대해 모든 Key와의 유사도를 구한 후 (전체 문장을 참고) 이에 관계된 Value에 반영한다. 유사도가 반영된 값, value는 attention value라고도 한다. - Q : t 시점에서 디코더 셀에서의 은닉상태 - K : keys, ’모든 시점에서’ 인코더 셀의 은닉 상태 - V : Values, ‘모든 시점에서’ 인코더 셀의 은닉 상태로 각 인코더의 attention 가중치와 은닉상태가 가중합 된 값이다. (a.k.a. context vector)\n\n\n2.1.3 3D CNN based method\n\nHAR의 공간과 시간을 모두 식별할 수 있다는 강력한 장점이 있으나 많은 양의 훈련 데이터를 요구함\n\n\n한계: Long term Temporal information이 전해지지 않는 문제\n\n지금까지는 모두 2D CNN을 시간과 함께 모델링했다. 그러나 Tran et.al [66]은 raw video data에서 시공간 데이터를 end-to-end 학습하기 위해 3D CNN 모델을 도입한다. 단, 이 경우 클립 수준 (16 frames or so) 에서 사용되는 모델이므로 시간이 길어질수록 temporal 정보가 옅어지는 한계가 있다.\n이에, Diba et al.(github, paper)는 3D 필터 및 pooling kernel로 2D 구조였던 DenseNet을 확장한 T3D (Temporal 3D ConvNet) 과 새로운 시간 계층 TTL (Temporal Transition Layer) 을 제안했다. - 시간에 따라 convolution kernel depth가 달라지도록 모델링 한 것 같다. 3D CNN 모델이 2D 단위에서 학습한 것을 활용하지 않는 것에 착안해 2D와 3D를 함께 쓰는 방식을 채택한 것으로 보임 (논문까지 확인하기 전)\n그 외에, 시간 범위를 늘린 LTC (Long-term Temporal Convolution) 모델, multi scale ‘temporal-only’ convolution; Timeception 모델 등이 제안되었으며 이는 모두 복잡하거나 긴 작업에서 영상의 길이에 구애받지 않고 인식할 수 있는 강건한 모델을 만들기 위함이다.\n\n\n\n2.2 Skeleton\n시점 변화에 민감한 pose estimation에서 motion capture system으로 수집한 데이터셋은 신뢰할 수 있다. (“Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding,” TPAMI, 2020.) 최근의 많은 연구는 Ntu의 depth map, 또는 RGB video를 사용한다 (st-gcn).\nRGB video만 사용할 경우 옷 또는 신체의 부피를 포함해 RGB data의 문제였던 다양한 변수 (e.g. background, illumination environment) 로부터 상당부 자유로울 수 있다. 초기에는 수작업으로 특징을 추출하여 관절 또는 신체 부위 기반의 방법이 제안되었는데 딥러닝의 발전에 따라 RNn, CNN, GNN, GCN을 적용하게 되었다.\n\n\n\n\n\n\n\n\n\n\n\nFig 7. Performance of skeleton-based deep learning HAR methods on NTU RGB+D and NTU RGB+D 120 datasets.\n\n\n\n\n\n2.3 depth\n\nDepth maps refer to images where the pixel values represent the distance information from a given viewpoint to the points in the scene.\n\n색상, 질감 등의 변화에 강건하며 3차원 상의 정보이므로 신뢰할 수 있는 3D 구조 및 기하학적 정보를 제공한다. depth map은 왜 필요한가? 3D 데이터를 2D 이미지로 변환하기 위함이다: depth 정보는 특수한 센서를 필요로 하는데, 이는 active sensors (e.g., Time-of-Flight and structured-light-based cameras) and pas- sive sensors (e.g., stereo cameras) 로 나뉜다.\nactive sensor는 방사선을 물체에 방출하여 반사되는 에너지를 측정하여 깊이정보를 얻는, 말그대로 능동적인 행동에 의해 발생하는 정보를 수집하는 센서다. Kinect, RealSense3D 등의 특수한 장치를 포함하는 센서가 포함된다. passive sensor는 물체가 방출하거나 반사하는 자연적인 에너지 를 말한다. 수동센서의 예인 stereo camera는 인간의 양안을 시뮬레이션 하는 카메라로 are recovered by seek- ing image point correspondences between stereo pairs 한다.\n둘을 비교했을 때, passive depth map generation은 RGB 이미지 사이에서 깊이를 연산해내는 과정이 포함되므로 계산 비용이 많이 들 뿐 아니라 질감이 없거나 반복 패턴이 있는; view point에 따라 크게 달라지지 않는 대상에는 효과를 보이지 않을 수 있다. 따라서 대부분의 연구는 active sensor를 이용한 depth map에 초점을 맞추고 있다. (“only a few works used depth maps captured by stereo cameras”)\ndatasets and methods\n데이터셋으로는 Deep Learning methods가 도래하기 전까지 사용했던 hand-crafted Depth Motion Map (DMM) features가 있다. 딥러닝 프레임워크도 이 DMM을 활용하는데, weighted hierarchial DMMs 이 제안되었다. 그러나 기존의 DMMs가 구체적인 시간정보를 포착하지 못하는 한계를 직면하자 Wang et.al 은 dynamic images at the body, body part, joint level 총 세가지를 짝지은 depth sequences 를 CNNs에 먹이는 방식을 제안했다.\ndepth modality의 성능은 1. depth maps including dynamic (depth images, 2. dynamic depth normal images, 3. dynamic depth motion normal images 의 발견에 힘입어 크게 성장했다. 발전을 위해 제안된 아이디어는 view invarient 을 이용한 방법론들이 다수인데, 다른 시각에서 본 이미지들을 high-level space로 옮김으로써 입체감을 부여하고 (Rahmani et al. [9]) CNN 모델이 human pose model과 Fourier Temporal Pyramids를 학습하게 하여 시점에 따른 행동 변화를 학습하게 하는 방식이 있다.\nestimate without depth sensor\n그러나 depth 정보를 추정해낼 수 있는 방법 또한 있다. depth estimation 기술이 이미 존재하고 Zhu and Newsam [224] 는 depth estimation을 이용해 RGB video에서 depth 을 추출해낸 바 있다.\n\npaperswithcode:depth estimation &gt; Newer methods can directly estimate depth by minimizing the regression loss, or by learning to generate a novel view from a sequence. &gt; The most popular benchmarks are KITTI and NYUv2. Models are typically evaluated according to a RMS metric.\n\nSubtask로는 Monocular Depth Estimation, Stereo Depth Estimation 등이 있다.\n\n2022 큰 주목을 받았던 diffusion model 또한 depth estimation을 사용하고 있다. multi-view 이미지들에서 차이점이 되는 point들을 찾고, 이 차이를 “splatting and diffusion”하여 depth map을 생성한다.\n\ndifferentiable diffusion for dense depth estimation from multi-view images (CVPR, 2021)\n\n\n\n\n\n\n\n\n\n\n\n\ngenerating diffusion map by splatting and diffusion differences of Multi-View images\n\n\n\n\nlimitation\n그러나 일반적으로 depth information은 외형정보가 부족하므로 다른 data modality와 융합하여 사용된다. - section 3 에서 더 살펴볼 수 있으나, 본 포스트에서는 single modality까지만 다루겠다.\n\n\n2.4 infrared (IR)\n\nInfrared radiation is emitted from all surfaces that have a temperature above 0 K (−273.15 °C) and the strength of emitted radiation depends on the surface temperature (higher temperatures have greater radiant energy) (sciencedirect/infrared-radiation)\n\n주변광에 의지하지 않아도 되므로 야간 HAR에 적합하다. depth sensor와 마찬가지로 반사광선을 활용하여 물체를 인식하는데, 적외선을 내보내는 센서가 active sensor라면 대상에서 방출되는 광선 (열 에너지 등) 을 인식하는 방법은 수동 인식이다.\nmethods\nKawashima et.al 의 Action recognition from extremely low-resolution thermal image sequence (2017, AVSS) 에 의하면 극히 낮은 해상도 \\(^{1}\\) \\(^{2}\\) 의 row resolution thermal images에서 먼저 사람의 무게중심을 기반으로 사람 부분만 추출하고, 다음으로 cropped sequences들과 frame간 차이를 LSTM기반의 CNN에 입력해 시공간 정보를 담은 모델을 생성한다. 해당 연구에서 이어진 연구로는 열화상 비디오들에서 학습된 시공간 정보를 동시에 학습하기 위해 3D CNN을 적용한 Shah et al의 연구가 있고, Meglouli et al는 raw thermal images를 사용하는 대신 raw thermal sequences를 3D CNN에 적용하여 optical flow information를 연산해냈다.\n\nthermal images vs thermal sequence?\n\n\n\n\n\n\n\n\n\n\n\nExample of a thermal image sequence (Action Recognition from Extremely Low-Resolution Thermal Image Sequence)\n\n\n\n\n이미지간 차이를 추출해낸 결과로 보인다.\n\\(^{1}\\) 왜 저해상도라는 말이 언급되는가? - 기본적으로 IR(InfraRed) radiation은 전자기파 (electromagnetic waves) 이며 oscillate (진동) 주기는 \\(3 * 10^{11}Hz\\) 에서 \\(4 * 10^{14}Hz\\) 다. (sciencedirect/IR Radiation/Jyrki K. Kauppinen, Jari O. Partanen, in Encyclopedia of Spectroscopy and Spectrometry (Third Edition), 2017) - 고주파면 High-Frequency, 고해상도면 High-Resolution인데 왜 해상도가 언급되는가: 적외선 이미지는 해상도가 rgb 이미지보다 훨씬 낮다. 여기서 말하는 해상도가 낮다는건 픽셀정보인 공간 해상도를 구분하는것\n| ![Example of RGB and InfRared (IR pair images in real maritime dataset)](https://www.researchgate.net/publication/343453594/figure/fig5/AS:928906546802693@1598479987148/Example-of-RGB-and-InfRared-IR-pair-images-in-the-real-maritime-dataset-at-a.png) |\n| :-: |\n| Example of RGB and InfRared (IR pair images in real maritime dataset) |\n\n해상도란 : 신호에서 minimum하게 구분할 수 있는 간격으로 탐지하기 위해 받아들이는게 신호라고 할 때 서로 다른 신호의 간격이 얼마나 가까이까지 구분해낼 수 있는가를 해상도라고 말한다. 즉, 실제로 서로 다른걸 다르다고 말할 수 있는 거리가 해상도다. 해상도가 높으면 같은 이미지도 높은 픽셀로 표현할 수 있다.\n예를들어 High Resolution infrared Radiation Sounder (HIRS) sensor 는 적외선 방사선 검출기의 단점내지 한계인 해상도를 개선한 센서라고 생각할 수 있다.\n\n\\(^{2}\\) 그렇다면 왜 초저해상도 이미지를 굳이 사용하는가? - quotation : Kawashima, Takayuki & Kawanishi, Yasutomo & Ide, Ichiro & Murase, H. & Deguchi, Daisuke & Aizawa, Tomoyoshi & Kawade, Masato. (2017). Action recognition from extremely low-resolution thermal image sequence. 1-6. 10.1109/AVSS.2017.8078497.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3. Example of images captured at night-time\nFigure 4. Example of a thermal image sequence\n\n\n\n\nTherefore, it is difficult to compute feature points and to obtain a clear edge of the human body from it. More-over, the pixel values will be easily affected by factors such as the motion of a person and the distance between the sen-sor and the human body. Therefore, most conventional action recognition methods using a visible-light camera are not suitable for being applied to extremely low-resolution thermal image sequences\n\n이러한 이유로 초저해상도 열화상 이미지를 사용하게 된다.\nlow-resolution thermal image sequences는 아래와 같은 특성을 가진다.\n\nedge of the human body does not appear clearly\n\n위 인용에서 초저해상도 열화상 이미지를 사용하는 이유로 든 사례와 같다. RGB high resolution image 도 human body edge를 명확하게 연산하기는 어려우며 pixel values들은 사람의 움직임, 센서와의 거리 등에 크게 영향을 받는다.\nhigh resolution image가 집중하는 human body edge가 연산해내기 어려운 특성이라면 아예 이를 제외하고 다른 부분에 초점을 맞추겠다는 선언으로 보인다.\n\nThe motion of a person changes the pixel values of both the human body and its surrounding region\n\n1이 low resolution image sequence를 사용해야 하는 이유와 관계가 있다면 이 항목은 edge를 명확하게 찾을 수 없는 대신 얻을 수 있는 정보를 말한다.\n대략적인 범위를 알 수 있으니 결국 얼만큼 움직이는지만 감지해내면 되고, 그 움직임은 pixel 값으로 표현되니 optical image 를 대신할 수 있다.\n\nWhen the distance between the sensor and the human body changes, the pixel values also change\n\n또 다른 RGB image approach의 한계였던 센서와의 거리가 thermal image에도 영향을 미치지만 이것은 pixel value에 영향을 미치므로 연산으로 보완할 수 있을 것으로 보인다.\n\nA pixel value changes depending on the occupancy area ratio of the human body in the observation range of a thermopile infrared sensor\n\n결과적으로 센서와의 거리 (view point) 등의 변수도 pixel에 반영이 되므로 적외선 센서의 특징을 고려할 때 기존 접근법의 한계를 극복할 수 있을 것이다.\n\n\n\n\n2.5 Point Cloud\n\nPoint cloud data is composed of a numerous collection of points that represent the spatial distribution and surface characteristics of the target under a spatial reference system.\n\n시공간보다는 2D와 3D 모두 풍부한 정보를 얻을 수 있는 데이터 양식으로 2차원에서는 실루엣을, 3차원에서는 대상의 기하학적 정보를 포함하기 때문에 3D HAR에서 확고한 입지를 다지고 있다. 해당 정보를 얻기 위한 방법은 두가지가 있는데 하나는 3D sensor (cf. LiDar, Kinect) 를 사용하는 것이고 다른 하나는 이미지를 기반으로 한 3차원 재구성 (image based 3D reconstruction) 을 수행하는 것이다.\n\n\n\n\n\n\n\n\n\n\n\nFig 1. lidar point cloud\n\n\n\nUsing point cloud sequence by Voxel\n또한 딥러닝의 발전으로 딥러닝 방법론이 주목받았고, 일반적으로 더 나은 성능을 보였다. 2020년 CVPR에서 “3dv: 3d dynamic voxel for action recognition in depth video,” 발표되어 raw point cloud sequence를 일반적으로 사용할 수 있는 3D 화소 집합 (voxel sets) 으로 변환한 바 있다.\n\n?: temporal rank pooling이 뭘까? 선형대수의 그 rank 인가? rank를 어떻게 풀링하는가…? 그걸 하는게 어떤 효과가 있는가…?\nvoxel : 입체 화상을 구성하는 3D 화소로 volume element를 말한다. 데이터 포인트로 구성되는데 이는 하나 또는 여러개의 데이터 조각으로 구성된다. (e.g. 불투명도, 색상 …) 따라서 Vector (or Tensor) 데이터로 구성되며 다양한 속성을 표현할 수 있다. (e.g. CT에서 재료의 불투명도를 부여하는 Hounsfield scale: 방사선의 밀도를 표현)\n\npixel : picture + element\n\nraster image를 구성하는 가장 작은 단위 또는 display에서 접근 가능한 모든 점들의 집합을 말하며 대부분 digital display 에서 표현되는 그래픽들의 가장 작은 단위로 사용된다.\n\nvoxel : volume + element\n\n3D Computer graphic에서 3차원 상의 일반 격자(regular grid)를 나타내기 위한 단위로, 고유한 state parameter를 가지며 모델 객체에 종속된다. (wiki)\n\nregular grid? : grid는 regular grid와 irregular grid로 구분되는데 일반 격자는 테셀레이션(tesselation)의 n차원 유클리디안 공간으로 규칙적인 간격을 가진다.\ntesselation? : 테셀레이션은 computer graphic 용어로 장면의 객체를 렌더링하기에 적합하도록 나타내는 다각형 데이터 집합 또는 vertex sets 이다.*\n\n\ntexel : texture + element\n\ntexture map의 기본단위로, 이미지를 픽셀로 표현하는 것 처럼 배열을 texture 공간에 나타내어 질감을 표현한다.\n\nresel : resolution + element\n\n실제 공간 해상도에서 이미지 또는 부피데이터셋이 차지하는 비율을 나타낸다. resels per pixel, resels per voxel 등으로 표현한다.\n\n\n\n이렇듯 voxel에는 다양한 속성을 표현할 수 있으므로 voxel sets 을 3D action information으로 인코딩할 수 있다. 이러한 추상화 과정을 통해 학습한 모델이 PointNet++다.\n물론, point cloud를 voxel로 변환하는 과정에서 다량의 양자화 오차(quantization errors)가 발생하여 효력면에서 충분히 효과적이지 않은데 이를 해결하기 위해 제안된 모델이 MeteorNet이다. 해당 모델은 여러 프레임의 point cloud들을 local 특성으로 합산하는데 이 때 spatio-temporal neighboring point 들을 사용한다. 다시 말해, 모든 point cloud를 voxel로 변환할 때 유실되는 값이 많으므로 국소 범위에서 관계가 있을 것으로 추정되는 주변 값을 변환하고, 또 변환하여 오차를 줄이는 방식을 채용한 셈이다. - 양자화 오차 : ADC (Analog to Digital Converter) 에서 입력 아날로그 신호가 출력 디지털 신호로 변환될 때 유실되는 값이다.\n이와 반대로 점의 공간적 불규칙성이 정보값에 혼란을 줄 것을 우려한 PSTNet은 시공간 정보를 분리하기도 했다.\nModeling\n이렇게 재구성된 Point Cloud로 수행해야하는 바는 다른 modality와 동일하게, 시공간을 동시에 고려고 그 특성을 파악하는 작업이다. 3차원 공간의 정보의 누수를 막고 voxel sets을 구해낸 후의 연구는 시간을 모델링하는데 초점을 맞추는데 이는 여타 방법론과 유사하다. RNN 기반의 모델인 LSTM을 적용한다. 눈여겨 볼 점은 4D CNN이 도입되었다는 점인데, 이는 LSTM 도입의 연장선으로 이미 3차원인 공간 모델링에 시간 차원을 추가하는 방식이다. - self-supervised modeling이 상대적으로 자주 언급되는데 최신 연구이기 때문인지 point cloud 특성 때문인지 확인이 필요하다."
  },
  {
    "objectID": "posts/papers/2310.07820.html",
    "href": "posts/papers/2310.07820.html",
    "title": "LLMs Are Zero-Shot Time Series Forecasters",
    "section": "",
    "text": "Abstract\n시계열을 ’숫자 문자열’로 인코딩하면, 시계열 예측을 자연어 처리에서의 다음 토큰 예측과 같이 구성할 수 있다는 접근으로 시작한다. 따라서 저자들은 GPT-3, LLaMA-2 모델과 같은 LLM 이 zero-shot 시계열 추정(extrapolate)의 비교가능한 수준에서, 또는 시계열 학습을 목적으로 만들어진 모델을 상회하는 능력을 보임을 발견했다. 이러한 성능을 촉진하기 위해서 본 논문에서는 시계열 데이터를 효과적으로 토큰화 하는 방법과 토큰에 대한 이산적 분포를 매우 유연한 밀도의 연속값으로 전환하는 방법을 제안한다. 저자들은 시계열에 대한 LLM의 성공이 단순성 및 반복에 대한 편향과 함께 multi-modal distributions을 자연스럽게 표현하는 LLM의 능력에 있다고 주장하는데 이는 많은 시계열에서 두드러지는 특징이며 특히 반복되는 계절 추세에서 그러하다는 점에서 본 논문의 주장과 상통한다.\n\n\n\n\n\n\n토큰의 이산적 분포를 연속값의 유연한 밀도로 변환하는 방법?\n\n\n\n\n\n\n이산분포(불연속분포)는 가능한 결과가 뚜렷하고 분리되어있는 확률 분포를 의미한다. 예를 들면 특정 토큰이 시퀀스에서 발생할 확률과 같다.\n연속값은 반대로, 어떤 실수에서 온도나 주가와 같은 특정 범위를 취할 수 있는 값을 말한다.\n이산분포를 연속값으로 전환함으로써 저자는 LLM이 고정된 토큰의 집합에 국한되지 않고 가능한 다양한 값을 나타낼 수 있는 예측을 생성할 수 있음에 초점을 맞춘다.\n\n\n\n\n또한 연구진은 본 논문에서 LLM이 누락된 비-수학적 데이터를 자연스럽게 처리하는 방법과 텍스트 정보를 수용하는 방법을 보이며 QA를 예측 결과를 설명하는데 도움이 되도록 사용한다. 모델의 크기와 시계열 성능은 일반적으로 비례하지만 이를 밝혀내는 과정에서 연구진은 GPT-4가 토큰 개수로 인해 GPT-3보다 더 좋지 않은 성능과 나쁜 불확실성 교정 성능을 보임을 알아낸다. 이는 RLHF와 같은 정렬 중재(alignment intervention method)의 결과일 수 있다; 사용자가 GPT-4를 이용하여 정렬하던 과정에서 오류 또는 보정이 제대로 이루어지지 않았을 가능성을 명시한다.\n\n\n\n\n\n\n‘단순성 및 반복에 대한 편향과 함께’ + ’다중 분포를 자연스럽게 표현’하는 능력?\n\n\n\n\n\n\nWe argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trend\n\n\nMultimodal distribution, 여러개의 peak와 mode가 있는 확률분포로 여러개의 그룹 또는 데이터의 패턴이 존재함을 나타낸다.\n\nmodality란 양식, 양상이라는 뜻으로 어떤 형태로 나타나는 현상 또는 ’그것을 받아들이는 방식’을 말한다. LLM은 텍스트로 된 입력을 받아왔는데 텍스트만이 아닌 다른 양식들(사진, 소리 등)을 학습하거나 표현할 수 있게 발전해왔다. 예를 들어, OpenAI의 DALL-E 2는 대표적인 multi-modal AI인데 사용자가 문장을 입력하면 모델이 이해한 바를 그림 또는 사진으로 표현한다. DALL-E 2는 기존 이미지들을 개체별로 나누고 이름을 부여한 다음, 위치와 색상, 어떤 동작을 하고 있는지를 이해하고 이미지를 설명하는데 이용된 텍스트 간의 관계를 학습한다.\n따라서 LLM에서의 Multimodal distribution은 LLM이 다중양식을 학습했을 때 보일 수 있는 확률분포다.\n단순성 및 반복에 대한 편향이 중요한 이유는 다양한 유형의 분포가 내재되어있는 시계열의 특징 때문이다. 시계열에서는 추세를 단순화하고, 반복을 찾아내어 경향을 파악해야 하기 때문에 해당 편향을 유지해야만 한다.\n\n\n\n\n\n\n\n\n\n\nRLHF, alignment method\n\n\n\n\n\n\nReinforcement Learning by Human Feedback 의 약자로 LLM의 alignment intervention method다.\n정렬 (alignment) 이란 LLM을 사용자에게 적합하게 조율하는 과정으로 사용자와의 상호작용 사이에서 이루어진다. 따라서 말 그대로 RLHF: 사람의 피드백에 의한 강화학습은 사용자가 프롬프트로 하는 미세조정이라고 볼 수 있겠다. (확인 필)\n\\(^*\\) 하지만 Meta AI에서 발표한 LIMA (Less Is More for Alignment) 논문은 LLM의 Pre-training이 중요하다고 하는데 주장을 살펴보기 위해 논문을 읽어볼 필요가 있다."
  }
]