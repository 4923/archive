{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Implement hyper-dgstgcn\n",
    "subtitle: DGSTGCN에 hypergraph 구조 도입\n",
    "description: blank\n",
    "categories: HAR/experiments\n",
    "author: YeEun Hong\n",
    "date: 2023-05-03\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyskl-hyper/configs/dgstgcn/ntu60_xsub_3dkp/j_hyper.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deephypergraph.readthedocs.io/en/latest/api/dhg.html#dhg.Hypergraph\n",
    "# class dhg.Hypergraph(num_v, e_list=None, e_weight=None, v_weight=None, merge_op='mean', device=torch.device)\n",
    "# [Source](https://deephypergraph.readthedocs.io/en/latest/_modules/dhg/structure/hypergraphs/hypergraph.html#Hypergraph) 살펴보면 BaseHypergraph 상속해서 클래스 생성함을 알 수 있음\n",
    "\n",
    "# tutorial\n",
    "# https://deephypergraph.readthedocs.io/en/0.9.3/tutorial/structure.html#build-hypergraph\n",
    "\n",
    "# Main ref\n",
    "# https://deephypergraph.readthedocs.io/en/latest/tutorial/structure.html?highlight=masking#prometed-from-low-order-structures\n",
    "\n",
    "import torch\n",
    "import dhg  # Graph, Hypergraph\n",
    "from dhg.models import GCN\n",
    "from dhg.random import set_seed\n",
    "\n",
    "e_list = [(3,4,21), (3,4,5), (3,4,9),\n",
    "          (5,9,21), (3,6,21), (3,10,21), (2,6,21), (2,6,21), (2,10,21),\n",
    "          (9,10,11), (5,6,7), (6,10,21),\n",
    "          (2,11,12), (2,7,8), (7,11,21),\n",
    "          (10,12,24), (9,11,25), (6,8,22), (5,7,22), (2,10,24), (2,6,22),\n",
    "          (1,2,17), (1,2,13),\n",
    "          (1,17,18), (1,13,14), (14,17,18), (13,14,18),\n",
    "          (18,19,20), (14,15,16), (16,19,20), (15,16,20)]\n",
    "\n",
    "def hyper2graph(e_list=e_list):    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        e_list (tuple): hyperedges\n",
    "    Returns:\n",
    "        Adjacency Matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    hg = dhg.Hypergraph(num_v=25, e_list = [(e[0]-1, e[1]-1, e[2]-1) for e in e_list])\n",
    "\n",
    "    # incidence graph : hg.e\n",
    "\n",
    "    # Star Expansion\n",
    "    g, v_mask = dhg.Graph.from_hypergraph_star(hg)\n",
    "\n",
    "    # Clique Expansion\n",
    "    # g = dhg.Graph.from_hypergraph_clique(hg)\n",
    "\n",
    "    # sets = g.e[0]\n",
    "    # A = g.A.to_dense()\n",
    "    return hg, g, v_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyskl-hyper/pyskl/models/gcns/dgstgcn.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Code\n",
    "# bash tools/dist_train.sh configs/dgstgcn/ntu60_xsub_3dkp/j_hyper.py 3 --validate --test-last --test-best\n",
    "\n",
    "\n",
    "import copy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from ...utils import Graph, cache_checkpoint\n",
    "from ..builder import BACKBONES\n",
    "from .utils import dggcn, dgmstcn, unit_tcn\n",
    "\n",
    "EPS = 1e-4\n",
    "\n",
    "# 예시:\n",
    "# modules = [DGBlock(in_channels, base_channels, A.clone(), 1, residual=False, **lw_kwargs[0])]\n",
    "\n",
    "class DGBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True, **kwargs):\n",
    "        super().__init__()\n",
    "        # prepare kwargs for gcn and tcn\n",
    "        common_args = ['act', 'norm', 'g1x1']\n",
    "        for arg in common_args:\n",
    "            if arg in kwargs:\n",
    "                value = kwargs.pop(arg)\n",
    "                kwargs['tcn_' + arg] = value\n",
    "                kwargs['gcn_' + arg] = value\n",
    "\n",
    "        gcn_kwargs = {k[4:]: v for k, v in kwargs.items() if k[:4] == 'gcn_'}\n",
    "        tcn_kwargs = {k[4:]: v for k, v in kwargs.items() if k[:4] == 'tcn_'}\n",
    "        kwargs = {k: v for k, v in kwargs.items() if k[1:4] != 'cn_'}\n",
    "        assert len(kwargs) == 0\n",
    "\n",
    "        self.gcn = dggcn(in_channels, out_channels, A, **gcn_kwargs)\n",
    "        self.tcn = dgmstcn(out_channels, out_channels, stride=stride, **tcn_kwargs)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x, A=None):\n",
    "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
    "        res = self.residual(x)\n",
    "        x = self.tcn(self.gcn(x, A)) + res\n",
    "        return self.relu(x)\n",
    "\n",
    "\n",
    "@BACKBONES.register_module()\n",
    "class DGSTGCN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 graph_cfg,     # ./configs/dgstgcn/ntu~/~.py model:dict\n",
    "                 in_channels=3,\n",
    "                 base_channels=64,\n",
    "                 ch_ratio=2,\n",
    "                 num_stages=10,\n",
    "                 inflate_stages=[5, 8],\n",
    "                 down_stages=[5, 8],\n",
    "                 data_bn_type='VC',\n",
    "                 num_person=2,\n",
    "                 pretrained=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # 원본코드\n",
    "        # self.graph = Graph(**graph_cfg)\n",
    "        # A = torch.tensor(self.graph.A, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        # 수정본\n",
    "        from pyskl.models.gcns.utils._hyper import hyper2graph    \n",
    "        self.hg, self.g, self.v_mask = hyper2graph()\n",
    "        \n",
    "        # example\n",
    "        # Hypergraph(num_v=25, num_e=30),\n",
    "        # Graph(num_v=55, num_e=90),\n",
    "        # tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        #         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        #         True,  True,  True,  True,  True, False, False, False, False, False,\n",
    "        #         False, False, False, False, False, False, False, False, False, False,\n",
    "        #         False, False, False, False, False, False, False, False, False, False,\n",
    "        #         False, False, False, False, False])   # 55개\n",
    "\n",
    "        # size of A = num_v * num_v (== size of v_mask * v_mask)\n",
    "        A = torch.tensor(self.g.A.to_dense(), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        # 원본코드\n",
    "        self.data_bn_type = data_bn_type\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        if data_bn_type == 'MVC':\n",
    "            self.data_bn = nn.BatchNorm1d(num_person * in_channels * A.size(1))\n",
    "        elif data_bn_type == 'VC':      # VC 로 설정돼서 이쪽으로 넘어감\n",
    "            \"\"\"\n",
    "            in channel: 3\n",
    "            A.size : torch.Size([55, 55]) -> A.size()[0] : 55\n",
    "            data_bn: BatchNorm1d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            \"\"\"\n",
    "\n",
    "            # self.data_bn = nn.BatchNorm1d(in_channels * A.size(1))    # original code\n",
    "            # 원래 이 윗줄에 75가 들어가야했는데 165가 들어가는 바람에 문제가 생겼으므로 165/inchannels = 165/3 을 A.size(1) 대신 넣나\n",
    "            self.data_bn = nn.BatchNorm1d(in_channels * A.size()[0])\n",
    "            # print(f\"\\n\\n\\ndata_bn: {self.data_bn}\\n\\n\\n\")\n",
    "\n",
    "        else:\n",
    "            self.data_bn = nn.Identity()\n",
    "\n",
    "        lw_kwargs = [cp.deepcopy(kwargs) for i in range(num_stages)]\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, tuple) and len(v) == num_stages:\n",
    "                for i in range(num_stages):\n",
    "                    lw_kwargs[i][k] = v[i]\n",
    "        lw_kwargs[0].pop('tcn_dropout', None)\n",
    "        lw_kwargs[0].pop('g1x1', None)\n",
    "        lw_kwargs[0].pop('gcn_g1x1', None)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.ch_ratio = ch_ratio\n",
    "        self.inflate_stages = inflate_stages\n",
    "        self.down_stages = down_stages\n",
    "        modules = []\n",
    "        if self.in_channels != self.base_channels:\n",
    "            modules = [DGBlock(in_channels, base_channels, A.clone(), 1, residual=False, **lw_kwargs[0])]\n",
    "\n",
    "        inflate_times = 0\n",
    "        down_times = 0\n",
    "        for i in range(2, num_stages + 1):\n",
    "            stride = 1 + (i in down_stages)\n",
    "            in_channels = base_channels\n",
    "            if i in inflate_stages:\n",
    "                inflate_times += 1\n",
    "            out_channels = int(self.base_channels * self.ch_ratio ** inflate_times + EPS)\n",
    "            base_channels = out_channels\n",
    "            modules.append(DGBlock(in_channels, out_channels, A.clone(), stride, **lw_kwargs[i - 1]))\n",
    "            down_times += (i in down_stages)\n",
    "\n",
    "        if self.in_channels == self.base_channels:\n",
    "            num_stages -= 1\n",
    "\n",
    "        self.num_stages = num_stages\n",
    "        self.gcn = nn.ModuleList(modules)\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "    def init_weights(self):\n",
    "        if isinstance(self.pretrained, str):\n",
    "            self.pretrained = cache_checkpoint(self.pretrained)\n",
    "            load_checkpoint(self, self.pretrained, strict=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, M, T, V, C = x.size()        # st-gcn에서 온 값들. data normalization에 사용됨.\n",
    "        \"\"\"\n",
    "        N : 16, M : 2, T : 100, V : 25, C : 3\n",
    "        \"\"\"\n",
    "\n",
    "        # permute : 모든 차원을 맞교환할 수 있으며 contiguous tensor에서만 사용 가능하고 결과값 또한 contiguous tensor다.\n",
    "        # contiguous tensor인가? : 메모리상에서 인접해있는가?\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        # x after permuted\n",
    "        # N : 16, M : 2, T : 100, V : 25, C : 3\n",
    "\n",
    "        \n",
    "        # RuntimeError: running_mean should contain 75 elements not 165\n",
    "        # -> Conv layer에서 input channel의 개수를 바꾸어 해결 (https://discuss.pytorch.org/t/runtimeerror-running-mean-should-contain-64-elements-not-96/30846)\n",
    "        \n",
    "        print(\"\\n\\n\\n[\\tFORWARD\\t]\\n\\n\\n\")\n",
    "\n",
    "        if self.data_bn_type == 'MVC':\n",
    "            pass        # for testing\n",
    "            print(f\"\\n\\n\\n MVC : \")\n",
    "            x = self.data_bn(x.view(N, M * V * C, T))\n",
    "            print(\"\\t x : \", x)\n",
    "            print(\"\\n\\t (N, M * V * C, T) : \", (N, M * V * C, T))\n",
    "\n",
    "        else:       # vc이므로 이쪽으로 올 것.\n",
    "            print(f\"\\n\\n\\n not MVC : \")     # 여기까지는 되는데\n",
    "            print(\"\\n\\t (N * M, V * C, T) : \", (N * M, V * C, T))   # (6, 75, 100)\n",
    "            print(\"\\n\\t x.view(N * M, V * C, T) : \", x.view(N * M, V * C, T).size())\n",
    "            \n",
    "            # x = self.data_bn(x.view(N * M, V * C, T))    # original code\n",
    "            # view는 기존의 데이터와 같은 메모리 공간을 공유하며 stride 크기만 변경하여 보여주기만 다르게 한다. \n",
    "            # 그래서 contigious해야만 동작하며, 아닌 경우 에러가 발생함\n",
    "            x = self.data_bn(x.view(1, 165, 272))   # RuntimeError: shape '[6, 165, 100]' is invalid for input of size 45000\n",
    "\n",
    "            '''\n",
    "            File \"/home/devin/wdir/pyskl-hyper/pyskl/models/gcns/dgstgcn.py\", line 178, in forward\n",
    "                x = self.data_bn(x.view(N * M, V * C, T))\n",
    "            File \"/home/devin/anaconda3/envs/skl-mmlab/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
    "                return forward_call(*input, **kwargs)\n",
    "            File \"/home/devin/anaconda3/envs/skl-mmlab/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
    "                return F.batch_norm(\n",
    "            File \"/home/devin/anaconda3/envs/skl-mmlab/lib/python3.10/site-packages/torch/nn/functional.py\", line 2450, in batch_norm\n",
    "                return torch.batch_norm(\n",
    "            RuntimeError: running_mean should contain 75 elements not 165\n",
    "            '''\n",
    "            print(\"\\t x : \", x)\n",
    "            print(\"\\n\\t (N, M * V * C, T) : \", (N, M * V * C, T))\n",
    "\n",
    "        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n",
    "\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.gcn[i](x)\n",
    "\n",
    "        x = x.reshape((N, M) + x.shape[1:])\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
