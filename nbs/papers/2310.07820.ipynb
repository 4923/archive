{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: LLM as a Time Series Forecasters\n",
    "subtitle: Large Language Models Are Zero-Shot Time Series Forecasters\n",
    "description: (2310.07820) LLM의 활용방법\n",
    "categories:\n",
    "    - LLM\n",
    "    - Zero Shot\n",
    "    - timeseries-forecast\n",
    "author: YeEun Hong\n",
    "date: 2023.10.18\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시계열을 '숫자 문자열'로 인코딩하면, 시계열 예측을 자연어 처리에서의 다음 토큰 예측과 같이 구성할 수 있다는 접근으로 시작한다. 따라서 저자들은 GPT-3 모델과 LLaMA-2 모델과 같은 LLM 이 zero-shot 시계열 추정(extrapolate) 비교가능한 수준에서, 또는 시계열 학습을 목적으로 만들어진 모델을 상회하는 능력을 보임을 발견했다. 이러한 성능을 촉진하기 위해서 본 논문에서는 *시계열 데이터를 효과적으로 토큰화 하는 방법*과 토큰에 대한 *이산적 분포를 매우 유연한 밀도의 연속값으로 전환*하는 방법을 제안한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\"}\n",
    "#### 토큰의 이산적 분포를 연속값의 유연한 밀도로 변환하는 방법?\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
